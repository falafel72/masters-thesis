#+OPTIONS: toc:nil H:5
#+SETUPFILE: setup.org
#+TITLE: Methods for In-Situ Fish Length Measurement with a Single Laser and Camera to Support Citizen Science
#+AUTHOR: Kyle Sirong Hu
#+LATEX_HEADER: \degree{Electrical Engineering (Intelligent Systems, Robotics and Control)}{Master of Science}
#+LATEX_HEADER: \degreeyear{2024}
#+LATEX_HEADER: \chair{Professor Curt Schurgers}
#+LATEX_HEADER: \cochair{Professor Ryan Kastner}
#+LATEX_HEADER: \committee{Professor Nikolay Atanasov}

* Frontmatter :ignore:
#+LATEX: \frontmatter
#+LATEX: \makecopyright
#+LATEX: \makesignature
#+BEGIN_dedication
To Kat, my friends, and my family.
#+END_dedication
#+BEGIN_epigraph
#+ATTR_LATEX: :latexcode \vskip0.5pt plus.5fil \setsinglespacing
#+BEGIN_VERSE
One fish, Two fish, Red fish, Blue fish,
Black fish, Blue fish, Old fish, New fish.
This one has a little car.
This one has a little star.
Say! What a lot of fish there are.
#+END_VERSE
#+ATTR_LATEX: :latexcode \vskip\baselineskip
/Dr. Seuss/
#+END_epigraph
#+LATEX: \tableofcontents
#+LATEX: \listoffigures
#+LATEX: \listoftables
#+BEGIN_acknowledgements
This project would not have been possible without the many hands available to
help us throughout these past couple of years.

Thank you to the members of my committee: Curt Schurgers, for his invaluable
guidance on writing this document, Ryan Kastner, for guiding us through the
bureaucracies of research funding, and Nikolay Atanasov, for helping me smooth
out some of the math-heavy portions of this document.

Thank you to Christopher Crutchfield, whose mentorship and companionship has
been invaluable throughout this process. Thank you to Nathan Hui, whose stern
engineering and management advice was vital to us making it this far. Thank you
to all undergraduates who have worked with the FishSense project: Vivaswat
Suresh, Raghav Maddukuri, Avik Ghosh, Allen Kan, Ana Perez, Hamish Grant, Harish
Vasanth, Shaurya Raswan, Kyle Tran, Sam Prestrelski, Adrian Zugehar, Claire
Wang, Jennifer Xu, Louis King, and Grace Liu. Thank you also to the REU students
who helped us gather valuable data despite unreasonably short notice: Josie
Dominguez, Jordan Reichhart, Ela Lucas, and Nick Reyes.

Thank you to Professor Brice Semmens, Jack Elstner, Alli Candelmo, Jen Loch and
the members of REEF for testing this system and providing us with the initial
problem we aim to solve. An additional thank you to Patrick Paxson and Peter
Tueller, whose work laid the foundation for our current research.

Most importantly, thank you to Fred the fake fish, who we dearly miss. May he
enjoy his freedom in the Pacific Ocean.

I would also like to thank the people I've met during my time at UCSD, who have
made this journey worth it.

Thank you to Kristina ``Kat'' Diep, for always being encouraging and supportive
during my time in graduate school. Your humor and light-heartedness has kept me
going.

Thank you to Matei Gardus, who taught me how to use emacs org-mode several years
ago, and has helped me tremendously with the formatting aspect of writing this
document.

Thank you to the members of UCSD Quizbowl, in particular to Alistair Gray, who
is a dear friend, and to Praveen Nair, who asked me to include the Dr. Seuss
quote in the epigraph.

Thank you the "ibob" crew, who have fostered my love for technology and with who
I have created a thriving community of people with similar interests through
UCSD's ACM chapter.

Thank you to Daniel Truong, as well as both the San Diego Smash and Fighting
Game communities, for allowing me to grow emotionally that much more.

Finally, thank you to my parents, whose continuous support has allowed the
opportunity to do all of this.

Chapters 2, 3 and 4 contain material that is being prepared for submission.
Christopher Crutchfield, Kyle Hu, Vivaswat Suresh, Ana Perez, Hamish Grant,
Nathan Hui, Jack Elstner, Dylan Heppell, Jen Loch, Alli Candelmo, Brice Semmens,
Curt Schurgers, Ryan Kastner. The thesis author was the primary investigator and
author of this material.

An additional thank you to Nathan Hui and Jack Elstner for collecting some of
the data presented in Chapter 4, which is not included in the aforementioned
upcoming publication.

#+END_acknowledgements

* Abstract                                                           :ignore:
#+BEGIN_dissertationabstract
Ecologists are interested in studying fish length distributions as a metric for
the health of fish populations, both for fishery management and to gauge the
effect of policies and worsening ocean conditions in a small area. Current
methods of gathering these data are time intensive, require expensive equipment
and/or training. Approaching this issue with citizen science -- utilizing
efforts from the general public -- allows us to gather much more data about fish
populations that would currently be costly to acquire. We are working towards
developing a device and software platform that allows recreational divers to
take pictures of fish, and have the images processed such that length and
species information are determined automatically.

This work focuses on the mechanism under which fish length is calculated. The
device incorporates a standard laser pointer with a consumer waterproof camera,
and uses the location of the laser dot in the object to determine the distance
between the object and the camera. We also discuss methods to ensure that the
parameters of the laser with respect to the camera are known, as well as how we
can ensure that these procedures can be performed in the field. We test these by
conducting trial calibrations and length measurements in a controlled setting,
and evaluating where sources of error come from. From our testing we are able to
verify that our technology is capable of obtaining measurements within 20% of of
true length, similar to the error margins of visual estimation.

#+END_dissertationabstract
#+LATEX: \mainmatter
* Introduction
** Preamble :ignore:
Marine ecosystems provide humankind many resources which we depend on. Marine
agriculture is an important source of food worldwide, and approximately 3
billion people depend on seafood as a significant source of protein\nbsp\cite{WWF}.
Data that can be used to evaluate the health of our oceans is therefore
essential for us to understand the potential impacts of overfishing and climate
change.

Fish in particular are a significant part of this -- fish populations are known
to be in decline as a result of overfishing and climate change, which has dire
consequences for both food stocks and marine ecosystems as a
whole\nbsp\cite{Hutchings2004,Myers2003}. In order to be able to quantify the
effects of climate change on fish, we require some metric with which we can
study how healthy fish are on a population level.

One such way of doing this is to obtain a fish length distribution. By examining
such distributions over time, scientists can identify whether a population is
being overfished, or is recovering. There are two primary important applications
for these data. The first is fish population conservation -- if policies are put
into place to assist with the recovery of a fish population, it is within the
interest of scientists to document the success of these protective measures. Our
collaborators have performed a study of a particular species of Nassau Grouper,
and have done this fairly successfully by looking at annual spawning
aggregations\nbsp\cite{Heppell2012}. The second is fishery management. Fisheries
want to ensure that they extract as much catch as possible without irreparably
damaging their stock, meaning that detailed, real-time information about
population health is vital, especially where other data are
unavailable\nbsp\cite{Balde2019}.

** Current Methods for Fish Length Studies
Current global fish length data often come from catch and release programs. In
California, the California Collaborative Fisheries Research Program (CCFRP) is
one such program, which as of 2024, consists of over 1800 volunteer anglers.
These anglers use hook-and-line procedures to catch the fish, from which length
is measured directly with a tape measure before the fish is released. This
length information is combined with metrics like catch per unit effort to
estimate the biomass of a population. However, fishing takes time -- in their
most recent study, each examined location took 3 days to gather data
for\nbsp\cite{Ziegler2024}. In addition, stress caused by barotrauma and air
exposure can cause fish to be less able to find food or evade predators, which
reduces their life expectancy\nbsp\cite{Campbell2010}. In summary, these expeditions
are logistically difficult to set up, may be dangerous for the
anglers\nbsp\cite{Rohner2011}, and also the fish being
studied\nbsp\cite{Ramsay2009,Campbell2010}. Thus a low effort, non-invasive approach
is desirable.

This approach also poses an issue for marine protected areas (MPAs), within
which fishing may be restricted to certain species, or completely illegal
outright. To study populations within these areas, adjacent fishing activities
are sometimes used to infer the abundance of a particular
species\nbsp\cite{Ziegler2024}.

A non-invasive alternative is the roving diver survey, where a team of divers
conducts a visual census of fish species and their lengths within the MPA.
Sometimes divers may use a T-bar as a length reference. However, measurements
obtained purely from visual estimation are naturally imprecise -- humans are
only capable of estimating to within 20% of a fish's true length
\cite{Harvey2001}. This is also a skill that must be honed in order to be
improved, and requires retraining as frequent as once every 6 months
\cite{Bell1985}.

The effort described within this work relates to our solution to the above
problem -- a device that we call FishSense Lite. FishSense Lite relies on a
single laser as depth reference in order to measure the length of a fish from a
captured image, and due to the simplicity and accessibility of the hardware
construction, it can be utilized by recreational divers to get much larger data
coverage.

** Related Work
<<sec:existing-technologies>>

A technological approach to measuring an objects size involves first capturing
an image of the scene, then using distances to objects to scale their apparent
sizes appropriately. Herein we discuss existing techniques using lasers, stereo
video measurements, and acoustic methods.

A summary of these underwater ranging methods can be seen in
Table\nbsp\ref{tab:comparison}, including FishSense Lite.

#+ATTR_LATEX: :align |c||c|c|c|c| :width 70%
#+CAPTION: Comparison of different underwater ranging techniques.
#+LABEL: tab:comparison
|--------------------+-------------------------+---------------------------------+--------------+--------------------------|
| Technique          | Est. Cost (USD)         | Max. Relative Error             | Ease of Use  | Range                    |
|--------------------+-------------------------+---------------------------------+--------------+--------------------------|
|--------------------+-------------------------+---------------------------------+--------------+--------------------------|
| Stereo Video       | 4600\nbsp\cite{SeaGIS}      | 2.5%\nbsp\cite{Harvey2001}          | Intermediate | 2-10m\nbsp\cite{Mallet2014}  |
|--------------------+-------------------------+---------------------------------+--------------+--------------------------|
| Laser Caliper      | 600\nbsp\cite{BERGERON2007} | 12%\nbsp\cite{Stock2021}            | Intermediate | 2-5m\nbsp\cite{Stock2021}    |
|--------------------+-------------------------+---------------------------------+--------------+--------------------------|
| Acoustic Methods   | 20k\nbsp\cite{Mueller2006}  | 1.1% - 35.2%\nbsp\cite{Mueller2006} | Hard         | 1-16m\nbsp\cite{Mueller2006} |
|--------------------+-------------------------+---------------------------------+--------------+--------------------------|
| Laser Rangefinding | 1200                    | 15%                             | Easy         | 2-5m                     |
|--------------------+-------------------------+---------------------------------+--------------+--------------------------|

*** Stereo Camera Systems
A standard method for collecting fish length measurements uses stereo video
technology\nbsp\cite{Mallet2014}. To measure the actual size of objects in an image,
an additional camera can be added to create a stereo camera setup - provided the
relationship between the two cameras is known, apparent size can be converted to
actual size by determining the distance of the fish from the cameras. These are
typically diver-operated (known as stereo diver-operated video or
stereo-DOV)\nbsp\cite{Goetze2019} or placed in baited remote underwater video
systems\nbsp\cite{Mallet2014}. While stereo-DOV is a more cost-effective solution
than deploying a remote system, the current state of the art still requires
purchasing proprietary hardware and software\nbsp\cite{Goetze2019}, which can be
prohibitively expensive for a citizen scientist at a minimum of
$4,600USD\nbsp\cite{SeaGIS} for a scientist grade stereo video system. In addition,
stereo video generates a large amount of data that requires significant effort
to store and process\nbsp\cite{Tueller2021}.

Commercial stereo video solutions include the AQ1 AM100\nbsp\cite{Shafait2017} and
the AKVA Vicass HD\nbsp\cite{Churnside2012}, intended for use in aquaculture. Such
systems are also costly and require a tether to a surface-side computer with
proprietary software that must be used to manage the system. This limits the
regions of the world where the data can be collected as it requires scientists
to interact with the system. The tether also limits the depths at which the data
can be collected.

A previous FishSense "Pro" device has been attempted using proprietary stereo
camera units\nbsp\cite{Paxson2022, Tueller2021}. These attempts involved using an
Intel Realsense D455 -- a depth camera that used both stereo cameras and
structured light in order to infer depth information. This camera was paired
with a NVIDIA Jetson TX2 for onboard video processing, and was fully contained
in a waterproof enclosure with onboard power and storage. Since the Realsense
was up against a flat acrylic port, distortions from Snell's law caused both the
incoming images and the structured infrared light to refract, which yielded
erroneous depth map information. On top of this, some of the infrared light
reflected back into the lens, adding additional color artefacts in the images
produced, rendering species identification impossible. Traditional camera
calibration procedures are also difficult to do underwater, if not
impossible\nbsp\cite{Wong2022}. We concluded that until we are able to model Snell's
law distortions, an approach using proprietary stereo camera approach would be
infeasible.

*** Laser Calipers
Another solution for ranging uses laser calipers -- two parallel lasers placed a
known distance away from each other. When calibrated correctly, the distance
between the two laser dots can be used as a reference length to measure the
entire fish\nbsp\cite{Rohner2011, Heppell2012}. For these measurements to be
accurate, both lasers must be perfectly parallel with each other and the camera
axis. Depending on manufacturing tolerances, such a requirement may mean that
lasers must be carefully selected. These systems are calibrated by measuring the
distance between the two laser dots at a large distance before a
dive\nbsp\cite{Heppell2012}. Lengths are then calculated using the known distance
between two points and the projection of the fish onto the camera. While length
estimation becomes simple using this method, the cost of two lasers and time for
the minute readjustments waste valuable resources for researchers. The dual
laser mechanism also requires that the object is larger than the distance of the
two lasers, meaning fish smaller than the offset cannot be measured.

This system has been used to document the recovery of a population of
Nassau Grouper over 7 years, in order to quantify the effectiveness of recovery
efforts\nbsp\cite{Heppell2012}. While such a system was also demonstrated to be more
effective than visual estimation, it also requires manufacturing a
custom-machined aluminum mount, which is not easily accessible for most divers.
Calibration of the system also involves verifying that the beams are the same
distance apart up to 15m from the source, which can be challenging to do in a
field setting.

*** Acoustic Methods
The primary related technology in this category is sonar, which involves
actively sending sound waves and using the signal as it echoes back to construct
an image of the scene. As we are interested in studying particular fish
populations, it is also important that we are able to identify the species of
fish being imaged, a relatively recent development when it comes to acoustic
methods. Technologies like Dual-frequency Identification Sonar
(DIDSON)\nbsp\cite{Belcher2002} and Adaptive Resolution Imaging Sonar
(ARIS)\nbsp\cite{Jones2021} both take advantage of many individual sonic beams and
high frequencies in order to construct a high-fidelity sonic image of the scene.
Both DIDSON and ARIS technologies have been used to measure fish
lengths\nbsp\cite{Burwen2010,Cook2019} and identify fish
species\nbsp\cite{Langkau2012,Jones2021}. Sonar is far more effective than
camera-based techniques when visibility conditions are poor. However, they are
by far the most expensive option listed here. A system in 2006 was said to have
cost around 20,000USD\nbsp\cite{Mueller2006}. We were unable to find specific
prices for modern imaging sonar units, though prices have ranged from tens of
thousands to even hundreds of thousands of US dollars.

Passive techniques -- monitoring sounds that come from the fish themselves --
have also more recently been used as a way to study fish
populations\nbsp\cite{Fornshell2013}. These have shown to be an effective way to
study species distribution\nbsp\cite{VanHoeck2021}, or for recording sightings of a
targeted species\nbsp\cite{Bolgan2023}, though we are unaware of any existing
literature that attempts to estimate size using this technology.

*** Laser Rangefinding
Our system uses only a single laser to measure distance, which removes the need
to calibrate two lasers simultaneously and keep them in parallel. This technique
is similar to a light projection-based triangulation rangefinder
system\nbsp\cite{Parthasarathy1982}, as it uses spatial information about the laser
dot to determine the depth of the subject. This method can be extremely accurate
with the right combination of laser and image sensor - up to 10
micrometers\nbsp\cite{Ebrahim2015, Cavedo2016}. Such sensors have been experimented
with as a low-cost solution for robot localization\nbsp\cite{Nguyen1995}, quality
assurance in manufacturing\nbsp\cite{Cavedo2016}, and 3D scanning\nbsp\cite{Baba2001}.

Single laser range finding also has precedence for use in animal size
studies\nbsp\cite{Jaquet2006,Monkman2019,Breuer2007}. The primary benefit of this
approach is that it is more inexpensive than other solutions and requires less
training to operate\nbsp\cite{Monkman2019}. Jaquet and Breuer et al. utilize a range
finder as a separate module from a regular digital camera\nbsp\cite{Jaquet2006,Breuer2007}.
Data from both modules must be combined and processed manually to
obtain lengths\nbsp\cite{Jaquet2006, Monkman2019}, which our platform improves upon.

** Outline
The rest of this work will be focused on the algorithms behind the length
extraction for FishSense Lite, specifically how the laser parameters are
determined, how these are use to calculate the coordinates of the laser dot in
physical space, and how that in turn is used to get the fish length.
Chapter\nbsp[[sec:system-overview]] will give a brief overview of FishSense Lite as a
whole. Chapter [[sec:algorithms]] will provide an in-depth description of the laser
algorithms. Chapter [[sec:testing]] will show how we have tested our system and
where sources of error come from. Finally, Chapter [[sec:conclusion]] will summarize
the content of this work and future work that needs to be done on the system.

* System Overview
<<sec:system-overview>>
** Preamble :ignore:
This chapter contains an overview of FishSense Lite. Our primary goals with this
system are to reduce the cost to gather fish length data, and to make the
practice more accessible to recreational divers. To accomplish this, we attach
an underwater camera (in our case, an Olympus TG6) to an off-the-shelf
underwater laser pointer. Both of these tools are commonly owned by recreational
divers, which is why we target this low cost hardware setup.
Figure\nbsp\ref{fig:fsl} shows an example of one such camera system.

#+ATTR_LATEX: :scale 0.2  :options angle=-90
#+ATTR_ORG: :width 200px
#+CAPTION: A fully assembled FishSense Lite hardware unit.
#+label: fig:fsl
[[file:images/fishsense-lite-system.jpg]]

Our system relies on the diver taking images with the laser beam pointed on the
fish. With just the image, as well as a priori knowledge of the laser beam
parameters, we can then use this to obtain a length of the fish within the
image. A long-term goal we have is that this data can be mapped to region, a
specific species and even a specific individual fish, which would allow us to
create a map of global fish population data.
** Hardware Setup
*** Overview :ignore:
This is the physical device that divers would use to take measurements. It is
intended to be composed of materials that many divers already own, which is why
we use off the shelf parts. The only new component that we include is a 3D
printed mount.
*** Camera
For our own testing, we use an Olympus TG6, a common camera for divers to own
for underwater photography. This camera itself is waterproof, though we use
another protective housing around it.

This is just one example of a camera that can be used for this system --
theoretically any camera would work.
*** Wide Angle Lens
<<sec:backscatter>>

With just the waterproof housing, Snell's law distorts images in such a way that
a relationship between lengths in the image and lengths in the real world are
difficult to obtain\nbsp\cite{Agrawal2012}. To mitigate the effects of this, we use
a corrective optic. Developed by Backscatter, this corrective optic is designed
to attach to the Olympus's underwater housing, and is typically used to widen
the field of view for underwater photographers. Future work is required for us
to understand how to model these Snell's law distortions better so we can
ultimately remove it. This allows us to use a simple camera model described in
Section\nbsp[[sec:pinhole]].

In order to verify that this model was sufficient for this
camera system, one metric we used was the mean squared error given by OpenCV's
=calibrateCamera= function. This value was significantly lower when calibrating
with the dome port on, and the images taken without the dome port showed a
significantly higher amount of distortion.

**** Notes :noexport:
include ana's charts here? that might be sufficient justification

*** Laser and Mount
The laser pointer is another off the shelf component. Many divers own laser
pointers in order to gesture to others, which further reduces the cost of the
overall hardware. This laser is mounted to the camera's housing with a
3D-printed polylactic acid (PLA) mount. Due to manufacturing defects and small
perturbations, that may occur in transit, the laser moves enough to cause
significant error in the measurements. See Section\nbsp[[sec:laser-calibration]] for how
this issue is solved.

The color of the laser also has an effect, discussed in Section\nbsp[[sec:laser-comparison]].
** Operation
*** Overview :ignore:
This section details how the device would be used in the field.
*** Calibration Procedures
**** Camera Calibration
<<sec:camera-calibration>>

A procedure is necessary to calibrate the camera, for reasons which will be
explained in Section\nbsp[[sec:pinhole]]. This is done using Zhang's
procedure\nbsp\cite{Zhang2000}, where many images of a checkerboard are taken, and
parameters are found such that known dimensions and regularity of the
checkerboard are satisfied. This procedure need only be performed once during
the device's lifetime.

Currently, this procedure must be done underwater. We do the described procedure
in a swimming pool, with the corrective optic attached. OpenCV's
=cameraCalibrate= function provides a straightforward way to obtain camera
intrinsics and lens distortion parameters. We have experimentally observed that
the camera parameters in salt water differ from the parameters in fresh water,
which is discussed in Section\nbsp[[sec:fw-vs-sw]].
**** Laser Calibration
Since our laser mount is not stable enough to guarantee that laser parameters
stay consistent between dives, we require a laser calibration at the beginning
of every dive. More detail about this procedure can be found in
Section\nbsp[[sec:slate-calibration]].

*** Diver Operation
In the field, after calibrating the laser, operating the camera is fairly
straightforward. The diver will take a picture of nearby fish as normal, with
the restriction that the laser dot must be trained on the fish. Note that we
expect the diver to ensure that two conditions are met:
1. The head and tail points of the fish are visible in the image.
2. The laser is both present and visible on the fish.
Excerpts from the field manual are provided in Appendix [[sec:user-manual]].
*** Data Offloading
After the data has been collected, the operator must offload the data to be
processed externally. We are currently in the process of developing software in
order to perform this processing, though details are out of scope of this
document.

** Software
Once the data has been collected, it must be processed to extract the lengths of
the captured fishes. A summary of the process is as follows: locations of both
the fish and the laser dot in the image are identified using machine learning
techniques. The laser's location in 3D space is determined using the location of
the laser dot in the image, along with prior knowledge of the laser's location
with respect to the camera. The fish segmentation mask is used to create a
polygon of the fish's outline, after which PCA is performed to identify the axis
of symmetry of the fish. The intersection of this line is used to find the head
and tail of the fish, and the depth obtained from the laser dot is used to
determine the locations of the head/tail points in 3D space. The distance
between these points is what is used to calculate the length of the fish. We
currently have plans to create a web service that handles all of this
processing, and intent for citizen scientists to upload their data.

A flowchart of the processing pipeline is included in
Figure\nbsp\ref{fig:software-flowchart}.

#+ATTR_LATEX: :scale 0.6
#+ATTR_ORG: :width 200px
#+CAPTION: Flowchart of software processing pipeline.
#+label: fig:software-flowchart
[[file:images/software_flowchart.pdf]]

** Accuracy Bounds
The primary method that our system aims to improve upon is human-made visual
estimates. While humans can collectively estimate the mean length of a
population quite accurately, the precision with which this is done can vary
wildly between divers\nbsp\cite{Harvey2002}. Within the study conducted by Harvey
et. al., we see that divers can be incorrect about the length of a fish by up to
20%. As our device is meant to be a supplement for a similar experiment setup,
we aim to perform as well or even better than this.

** Conclusion :ignore:
\\
Thus far we have described the system setup. The next chapter will describe the
main contribution of this work -- the algorithms underlying distane estimation
that utilize the laser.
* Algorithm Details
<<sec:algorithms>>
** Preamble :ignore:
The specific contribution detailed in this work are the algorithms relating to
the laser pointer -- both how the parameters of the laser with respect to the
camera are determined, and, given those parameters are known, how the location
of the laser dot can be found in the image.

** Pinhole Camera Model
<<sec:pinhole>>
All of the calculations shown assume that our camera follows the pinhole camera
assumption. In air, this assumption is extremely common and is used in stereo
cameras for object triangulation.

The pinhole camera model assumes that all light that comes into the camera
passes through a single point known as the /focal point/, or /optical center/,
of the camera. These quantities must be calibrated for, and we follow the
procedure described in Section\nbsp[[sec:camera-calibration]]. All incoming rays fall
onto the image sensor, which is a plane perpendicular to the camera's axis, at a
fixed distance from the focal point known as the /focal length/. The image
projected onto the sensor is a flipped version of the real life scene, as the
light rays are inverted after passing through the optical center. Instead of
looking at the image on the image sensor, it is typically the convention to
mirror the image sensor plane in front of the optical sensor, such that the
distance from the focal point to this new plane is also the focal length. This
plane is designated as the /image plane/. This convention allows us to more
easily translate from pixel coordinates to coordinates in the image plane, as we
need only know the spacing between individual pixels on the sensor is to map to
physical units, without needing to flip the image. A pictorial representation of
this is show in Figure\nbsp\ref{fig:pinhole-general}.

#+ATTR_LATEX: :scale 0.4
#+ATTR_ORG: :width 200px
#+CAPTION: Illustration of the pinhole camera model. Adapted from\nbsp\cite{Dima2018}.
#+label: fig:pinhole-general
[[file:images/pinhole-explanation.png]]

Underwater, it is more difficult to make the assumption that imaging systems
follow the pinhole camera model, especially with our current hardware setup. Our
camera housing's transparent port separating the camera lens from the water is
flat, meaning that Snell's law causes incoming light to refract. We have
described how we get around this issue in Section\nbsp[[sec:backscatter]].
** Quantities and Conventions
Our axis conventions in this chapter are as follows: the $x$ axis points to the
right in the image, the $y$ axis points downward in the image, and the $z$ axis
points forward, away from the camera. This system has the added bonus that the
$x$ and $y$ axes coincide with the directions of pixel coordinates. The origin
will be defined at the camera's optical center. This is commonly known as the
``optical frame''. We also designate a point $O$ to be the origin of this
coordinate system.

We will make use of the following quantities: $x$ represents the size of the
fish in pixels, $w$ represents the ``pixel pitch'' (distance between two pixels
on the image sensor), and $f$ represents the focal length of the camera. A
diagram of the image sensor is shown in Figure\nbsp\ref{fig:image-sensor}. Note that
the product $xw$ can be interpreted as the size of the fish on the image plane.
Figure\nbsp\ref{fig:pinhole-fish} shows a diagram of both the live fish and its
projection onto the image plane.

#+ATTR_LATEX: :scale 0.4
#+ATTR_ORG: :width 200px
#+CAPTION: Diagram of the image plane, demonstrating pixel pitch $w$ and apparent object size $x$.
#+label: fig:image-sensor
[[file:images/image-sensor.png]]

#+ATTR_LATEX: :scale 0.4
#+ATTR_ORG: :width 200px
#+CAPTION: Pinhole diagram showing a lionfish with it's image projection.
#+label: fig:pinhole-fish
[[file:images/pinhole-fish.png]]

If the apparent size of the fish is known, using similar triangles we are able
to obtain the following relationship:
\begin{equation}
L = \frac{Dxw}{f} \label{eq:length-from-depth}
\end{equation}
Our only unknown on the right hand side is $D$, the distance of the fish from
the camera. The key part of our system is the mechanism with which we obtain
this $D$.

Note that the above equation makes the following assumptions:
1. The fish is parallel with the image plane.
2. The laser dot is at the same depth as head and tail points on the fish.

In practice, neither of these are always, if ever, true, and the implications
of this are discussed more in Chapter [[sec:testing]].

We assume that we can describe precisely how the laser is positioned relative to
the camera. More precisely, we define parameters $\ell$ and $\alpha$ , where $\ell$ is the
3D vector from the origin to the laser beam in the $xy$ plane, and $\alpha$ is a 3D
vector of unit length that points in the direction of the laser beam. The laser
beam can therefore be described by a ray with scale factor $\lambda$:
\[
\vec{\ell p} = \ell + \lambda\alpha
\]

Note that our restriction that $\ell$ lies purely in the $xy$ plane means that the
$z$ component of $\ell$ is always zero.

We also assume that we are able to accurately find the laser dot in the image.
In practice, these are hand-labeled, but automatic algorithms are outside the
scope of this document.

Figure\nbsp\ref{fig:known-quantities} illustrates all the aforementioned quantities in one place.

#+ATTR_LATEX: :scale 0.5
#+ATTR_ORG: :width 200px
#+CAPTION: Diagram demonstrating known quantities. Laser shown in solid green, imaged laser dot in dashed green.
#+label: fig:known-quantities
[[file:images/known-quantities.png]]

** Finding the Distance to the Fish
<<sec:distance-finding>>

We assume that this laser beam intersects the fish at an unknown point $p$. As
imaged by the camera, this laser dot will have known pixel coordinates
$\mathfrak{p}$. This corresponds to a point $p_\text{image}$ on the image plane,
defined as

\[
p_\text{image} = \begin{bmatrix}
\mathfrak{p}_x \\
\mathfrak{p}_y \\
f
\end{bmatrix}
\]

We arbitrarily decide to scale this vector to be of unit
length, defining
\[
v = \frac{p_\text{image}}{\lVert p_\text{image}\rVert}
\]

This creates two rays that converge toward $p$: the laser beam, and the ray from
the camera that passes through $\mathfrak{p}$ on the image plane. By projecting
both $\vec{\ell p}$ and $\vec{v}$ out until they intersect, we can identify where
$p$ is.

More specifically, we can identify parameters $\lambda_1$ and $\lambda_2$ such that
\[
\ell + \lambda_1\alpha = \lambda_2v
\]

We can refactor the above relationship to be the form

\begin{equation}
\begin{bmatrix}
\alpha & -v
\end{bmatrix}
\begin{bmatrix}
\lambda_1 \\
\lambda_2
\end{bmatrix}
= -\ell
\label{eq:find-laser}
\end{equation}

In practice, however, this relationship does not necessarily hold, because the
laser dot as observed in the image may be in a particular pixel that does not
match up perfectly with where $\lambda_2 \vec{v}$ intersects with the image plane.
This causes the rays to come very close to each other, but not touch, meaning
Equation\nbsp\ref{eq:find-laser} has no solution.


We can, however, obtain $\lambda_1$ and $\lambda_2$ values that minimize the difference
between the left and right hand sides -- this is the least squares solution,
i.e. the solution to the minimization problem
\[
\text{argmin}_{x} \lVert Ax - b \rVert^2,
\]

where $A \in \mathbb{R}^{m\times n}$, $m > n$, and $x,b\in \mathbb{R}^{n}$. In our case, we
define the following:
\begin{align*}
A &= \begin{bmatrix}
\alpha \ -v
\end{bmatrix} \\
x &= \begin{bmatrix}
\lambda_1 \\
\lambda_2
\end{bmatrix} \\
b &= -\ell
\end{align*}

In general, we can solve a least squares problem with the following formula:
\[
x = (A^TA)^{-1}A^T b
\]

Using this, we can obtain a closed form solution for both $\lambda_1$ and $\lambda_2$. We
only require one of them, so we use $\lambda_2$:
\[
 \lambda_2 = \frac{-\alpha^T \ell \alpha^T v + v^T \ell}{1 - (\alpha^T v)^2}
\]

Obtaining $\lambda_1$ or $\lambda_2$ allows us to obtain $p$, and the z-component of $p$,
along with Equation\nbsp\ref{eq:length-from-depth}, gives us the corresponding fish
length.

** Finding Laser Parameters
<<sec:laser-calibration>>
So far, we have assumed that the parameters of the laser $\alpha$ and $\ell$ are known.
These cannot be measured directly for two reasons:
1. Precise measurements relative to the optical center of the camera are hard to
   obtain, since in reality the location of the optical center of the camera is
   not known.
2. Since the laser mount is not perfectly stable, and can change in between
   dives and over time, the parameters need to be recalculated.
Thus, we must use a calibration procedure.

We have $n$ images from which we obtain laser points $p_i$. Here we describe two
possible algorithms to leverage this information to determine $\alpha$ and $\ell$. A
comparison of the two methods is presented in Section\nbsp[[sec:math-testing]].
*** Method 1: Gauss Newton Optimization
We leverage the fact that the parameterized laser beam must intersect with
the laser dot point to give us the following series of equations:
\begin{equation}
p_i = \lVert p_i - \ell \rVert\alpha + \ell
\label{eq:problem}
\end{equation}
The above equation states that the point $p_i$ should be able to be retrieved
from scaling the laser ray out by a factor determined by the difference between
$p_i$ and $\ell$.

We stack these points into a single vector, defining the following;
\[
\mathbf{p} = \begin{bmatrix} p_1 \\ p_2 \\ \vdots \\ p_n \end{bmatrix}
\]




First we define a parameter vector $x$ that contains all of our parameters:
\[
x = \begin{bmatrix}
\alpha \\
\ell_x \\
\ell_y
\end{bmatrix}
\]

We also define a function in terms of our parameters for the right hand side:
\begin{align*}
        g_i(x) &= \lVert p_i - \ell \rVert \alpha + \ell \\
        \mathbf{g}(x) &= \begin{bmatrix}
        g_1(x) \\
        g_2(x) \\
        \vdots \\
        g_n(x)
        \end{bmatrix}
\end{align*}

We then formulate this in terms of the following optimization problem:
\begin{align}
\text{argmin}_{x}\lVert \mathbf{r}(x)\rVert, \nonumber \\
\mathbf{r}(x) = \mathbf{p} - \mathbf{g}(x) \label{eq:residual}
\end{align}

We have a relationship that relates our known quantities $p_i$ and unknown
quantities $\alpha$ and $\ell$, though in this case the relationship is non-linear. We
must therefore choose a non-linear optimization method to find the best
candidates for $\alpha$ and $\ell$ that satisfy this.

The method we currently choose is the Gauss-Newton method, which
involves the following steps:
1. Find the Jacobian $J_r$ of the minimizing function w.r.t to $x$.
2. Take iterative steps of the following form:
\[
x^{(k+1)} = x^{(k)} - (J_r^TJ_r)^{-1}J_r \mathbf{r}(x^{(k)})
\]

According to Equation \ref{eq:residual}, only the second term depends on our
parameters, so we can rework our iterative step into
\[
x^{(k+1)} = x^{(k)} + (J_g^{T}J_g)^{-1}J_{g}\mathbf{g}(x^{(k)})
\]

Here we set $x^{(0)}$ to a rough estimate of where the laser is with respect to the
camera, taken with a ruler. For the particular test system mentioned in this
work, the laser's starting point is assumed to be -4cm in the x direction and
-11cm in the $y$ direction, with the laser parallel to the camera axis.

There are many other methods of this kind that we could have used, such as the
Levenberg-Marquardt algorithm, though from our experiments this calibration
method has been sufficient.

The Jacobian of $\mathbf{g}(x)$ is given by the following, and the full
derivation is detailed in Appendix\nbsp[[sec:jacobian-derivation]]:

\begin{align}
J_g &= \begin{bmatrix}
J_{g\alpha}^1 & J_{g\ell}^1 \\
J_{g\alpha}^2 & J_{g\ell}^2 \\
\vdots & \vdots \\
J_{g\alpha}^n & J_{g\ell}^n
\end{bmatrix} \in \mathbb{R}^{3n\times5}\nonumber \\
J_{g\alpha}^i &= \lVert p_i - \ell\rVert I \in \mathbb{R}^{3\times3}\\
J_{g\ell}^i &= \begin{bmatrix}
1 & 0 \\
0 & 1 \\
0 & 0
\end{bmatrix}
\left(I_{3\times3} - \alpha\frac{(p_i - \ell)^T}{\lVert p_i - \ell\rVert }\right) \in \mathbb{R}^{3\times2}
\end{align}

*** Method 2: Averaging
Once again assuming we have $n$ images with image $i$ corresponding with laser
dot point $p_i$, a straightforward method to get our laser trajectory $\alpha$ can be
obtained from a normed average over the differences between all points:
\[
        \alpha = \frac{1}{n(n-1)}\sum_{i=1}^n\sum_{j\neq i}\frac{p_i - p_j}{\lVert p_i - p_j \rVert}
\]

This can be flipped as necessary to face the direction in which the laser beam
travels. Once $\alpha$ is obtained, we take the centroid of all of our points, which
we define as $\mu$, and set the intersection of the ray defined by $\mu$ and $\alpha$
with the $xy$ plane to be $\ell$. Formally, this is defined as
\[
\ell = \mu - \frac{\mu_z}{\alpha_z}\alpha
\]

This method can be performed much faster than the former method, though while
developing our device we have stuck to using the Gauss Newton method since the
constraints and minimizing function are more explicit.
** Calibration Procedure
<<sec:slate-calibration>>

As mentioned in Section\nbsp[[sec:laser-calibration]], to calibrate the laser, we assume
that we know the optical frame coordinates of each laser dot $p_i$. While the
laser parameters $\alpha$ and $\ell$ are still unknown, the vector $v$ is always known,
since this only depends on $\mathfrak{p}$, determined solely from the camera
image. By intersecting $v$ with a known plane, we can precisely determine the
location of each $p_i$.

The same checkerboard pattern mentioned in Section\nbsp[[sec:camera-calibration]] is one
example plane that is ideal for this. The main reasoning for this method was
that there are two parts to this method for which OpenCV provides functionality
for - detecting corners of specifically the checkerboard pattern, and
calculating a 3D transformation given object points and corresponding points in
an image. However, in the field, requiring that divers carry a large and heavy
checkerboard for every dive is a large ask.

We have developed a procedure that does not utilize a checkerboard, and can
instead be done with a dive slate -- something that most divers will carry. An
example of one of these is shown in Figure\nbsp\ref{fig:slate-img}. We add
pieces of duct tape in an arbitrary pattern on one side to make the slate more
featureful. We also make the assumption that the slate remains close to parallel
with the image plane.

#+ATTR_LATEX: :scale 0.4
#+ATTR_ORG: :width 200px
#+CAPTION: An example of a slate calibration image.
#+label: fig:slate-img
[[file:images/slate-calibration.JPG]]

We assume that we have a scanned copy of the dive slate, and hence the physical
measurements of the duct tape pattern. Assuming that a correspondence can be
drawn between the corners of the scan representation and the corners within the
image, a transformation between the two can be found. This is known as a
Perspective-N-Point (PnP) problem, and OpenCV has a =solvePnP= function that
makes this relatively straightforward. The same problem exists using the
checkerboard, but OpenCV's =findChessboardCorners= function abstracts this away
from the developer by taking advantage of the specific structure of the
checkerboard (number of squares per side, size of a square).

Results demonstrating this method compared to the checkerboard are shown in
Section\nbsp[[sec:field-calibration-testing]].
** Conclusion :ignore:
\\
In this chapter, we have discussed the assumptions we make for this system to
work, and outlined the algorithm for obtaining laser parameters, and using these
to obtain the length of a fish. In the next chapter, we will discuss
experiements done to test the device.
* Experiments
<<sec:testing>>
In this section we describe the testing that we have done so far to quantify how
well the system works. A total of 24 pool tests and 2 field deployments were
conducted in the process of developing FishSense Lite.
** Pool Experiments
The vast majority of our experiements have been performed in swimming pools, due
to our ability to control as many variables as possible, and for convenience. A
full list of pool tests is included in Appendix\nbsp[[sec:pool-test-list]].

In order to test the system, we follow variations on the following procedure:
1. If calibration parameters do not exist or we do not trust the current parameters, calibrate the camera.
1. Calibrate the laser by taking pictures of one or both the planar objects mentioned in Section\nbsp[[sec:slate-calibration]].
2. Take images of objects with a known length.
3. Analyze how the estimated lengths compare with the true length of the object.

The majority of these experiments were performed in a pool setting, as this was
the most accessible for us.

The most common way in which we will demonstrate results in this work will be a
plot of distance versus estimated length.

*** Reference Objects
The experiments described in this section involve one of three measurement objects, described below.
**** Checkerboard
The checkerboard itself is useful for the same reason that makes it a good
object to calibrate the laser with -- it is relatively straightforward to
identify the corners of a checkerboard in an image. In our case, we use the
checkerboard as a convenient way to measure the orientation of the object when
we measure lengths from it, while also using it to get many different
measurements from a single image.

**** Box
Historically we have also used an aluminum box, with a 15cm section of tape
being used as the reference length that we measure. The box is shown in
Figure\nbsp\ref{fig:box}.
#+ATTR_LATEX: :scale 0.05
#+ATTR_ORG: :width 200px
#+CAPTION: The aluminum box. The top edge of the black tape across the surface is what we typically measure.
#+LABEL: fig:box
[[file:images/box.jpg]]

**** Fake Fish
In order to fully simulate the purpose for which the device was intended, we use
a dummy rainbow trout as a reference object. Each dummy is made from the same
model with the same dimensions, though different techniques have been used to
make them slightly negatively buoyant. Three generations of dummy fish, named
Fred, George, and Ginny, have been used for these tests\footnote{If you would like to purchase a fish of your own, go to https://www.loftus.com/items/LF-0167.}.
Fred is shown in Figure \ref{fig:fred}.
Henceforth in this section, we will refer to all three of them as just "the fake
fish".
#+ATTR_LATEX: :scale 0.05
#+ATTR_ORG: :width 200px
#+CAPTION: Fred, our first fake fish. He, and all the others, are 31cm long.
#+LABEL: fig:fred
[[file:images/fred.jpg]]

*** Laser Calibration Algorithm
<<sec:math-testing>>
We test the accuracy of the calibration by comparing reference length
measurements with different laser calibration parameters.
Figure\nbsp\ref{fig:laser-calibration-comparison} shows the results from obtained
from both calibrations -- as can be seen, they are functionally identical.
Future work still needs to be done to formally verify that the two methods are
equivalent.

#+ATTR_LATEX: :scale 0.8
#+ATTR_ORG: :width 200px
#+CAPTION: Comparison of two calibration methods, graphing estimated distance against estimated length.
#+LABEL: fig:laser-calibration-comparison
[[file:images/laser-calibration-comparison.png]]

*** Distance Measurements
<<sec:distance-test>>
To evaluate purely the distance estimates we can obtain from the system, we take
images of objects of known size (in this case, a checkerboard square), at known
distances, and comparing this known distance with the distance we get from the
algorithm described in Section\nbsp[[sec:distance-finding]]. These data were collected by taking
pictures at fixed increments -- in our case, we taped 20cm marks to a long
fishing rod, and took images at each mark. There were originally 19 marks along
the rod, though many of the pieces of tape came off underwater. Results are
shown in Figure\nbsp\ref{fig:length-reference}, which demonstrates that the actual
distance and the measured distance are well correlated.

#+ATTR_LATEX: :scale 0.8
#+ATTR_ORG: :width 200px
#+CAPTION: Distance estimates compared to a distance reference, measured with two different laser parameter sets.
#+LABEL: fig:length-reference
[[file:images/length-reference.png]]

Note that this is heavily dependent on the laser calibration being accurate --
the data in Figure\nbsp\ref{fig:length-reference} follow two different trends, each
of which comes from a different set of laser parameters. Both of these
parameters were obtained from images in the same test -- one calibration run was
performed before and after the length measurements. In this particular case, the
incorrect calibration parameters were obtained before all measurements were
taken, and the correct parameters were obtained afterward. This implies that the
mount or the laser itself was moved slightly before measurements began.

In practice, there is no way that the diver would be able to tell if the mount
has been moved or not, which makes verifying the measurements difficult.
Section\nbsp[[sec:length-measurements]] will contain further discussion on how
calibration parameters affect our measurements.

*** Length Measurements
<<sec:length-measurements>>
Figure\nbsp\ref{fig:measurements} demonstrates measurements of our fake fish and the
box during a pool test. We can see that with varying distances away, the
estimate remains relatively consistent, and we always underestimate the lengths
of both objects.

#+ATTR_LATEX: :scale 0.8
#+ATTR_ORG: :width 200px
#+CAPTION: Measurements of the fish and the box, plotted over estimated distances away from the camera.
#+LABEL: fig:measurements
[[file:images/measurements.png]]

Based on Equation\nbsp\ref{eq:length-from-depth}, there are several factors that
cause error here, listed below.
**** Object Thickness
If the object is not flat, as is the case with most fish, then the laser dot may
be at a different distance from the camera than the head and tail points. This
causes closeup measurements of the object to be shorter than in images taken
further away, due to the fact that the difference in distance between the laser
dot and the head/tail points makes up a smaller percentage of the total distance
as the camera becomes further away. This effect can be seen in
Figure\nbsp\ref{fig:measurements}, as the fish's trend upward until around
1m away. It is likely that in field use, there will not be many measurements
taken that close to the fish anyway, as that would likely scare the fish away.
**** Object Tilt
If the object is tilted in the image toward or away from the camera, this causes
the length estimates to be lower than their true length. An experiment on this
is discussed in Section [[sec:tilt-experiment]].

**** Deviations in Laser Calibration
Section\nbsp[[sec:distance-test]] showed that an error in the laser calibration
parameters can cause deviation in measured distance. This creates a deviation in
measured length that increases the further away the object is from the camera.
Figure\nbsp\ref{fig:incorrect-calibration} shows the effect that these incorrect
parameters have on object length measurements.

#+ATTR_LATEX: :scale 0.8
#+ATTR_ORG: :width 200px
#+CAPTION: A demonstration of data where the laser parameters calculated during calibration do not match the parameters during measurement.
#+LABEL: fig:incorrect-calibration
[[file:images/length-with-incorrect-calibration.png]]

Notably, a change in the laser orientation $\alpha$ results in much more error than a
change in the laser location $\ell$, so error increases as the laser dot gets
further away from the camera.

**** Labelling Errors
All the data in this chapter were hand labeled, and this is especially
noticeable at longer distances from the camera.
Figure\nbsp\ref{fig:incorrect-calibration} has particularly obvious instances of
large amounts of noise being introduced at longer distances.

**** Deviations in Camera Parameters
An incorrect calibration of the camera's internal parameters can also cause a
deviation in length. Specifically, if the focal length $f$ is off by a certain
percentage, then the measured length of an object will be off by the inverse of
that percentage. This effect can be seen in both
Figures\nbsp\ref{fig:laser-calibration-comparison} and \ref{fig:measurements}. So
far, we have not yet encountered a situation where the contributions from this
exceed our 20% error margin, but more thorough experiments in this area are
currently underway.

*** Contributions of Tilt to Error
<<sec:tilt-experiment>>
To evaluate the system's accuracy with respect to different tilt angles, we had
the following experiment setup. We captured a total of 183 images of a
checkerboard pattern with the laser dot on it, with varying tilt angles and
depths. A subset of these images were used to obtain the laser position and
orientation. Using these data, from each image we extracted 13 lengths, as shown
in Figure\nbsp\ref{fig:checkerboard-experiment-setup}.
Figures\nbsp\ref{fig:checkerboard-errors-by-length} and
\ref{fig:checkerboard-errors-by-distance} shows the results of this experiment.

#+ATTR_LATEX: :scale 0.5
#+ATTR_ORG: :width 200px
#+CAPTION: Illustration of lengths measured on checkerboard. Each square pictured here is 41.3mm wide.
#+LABEL: fig:checkerboard-experiment-setup
[[file:images/checkerboard-experiment-setup.png]]

#+ATTR_LATEX: :scale 0.8
#+ATTR_ORG: :width 200px
#+CAPTION: Percent error of length measurements over all distances and angles, grouped by true object length.
#+LABEL: fig:checkerboard-errors-by-length
[[file:images/checkerboard_errors_by_length.png]]

#+ATTR_LATEX: :scale 0.8
#+ATTR_ORG: :width 200px
#+CAPTION: Percent error of length measurements over all distances and angles, plotted over the distance from the camera each image was taken.
#+LABEL: fig:checkerboard-errors-by-distance
[[file:images/checkerboard_errors_by_distance.png]]

Both figures show that the relative error in measurement decreases dramatically
when the angle becomes lower than 15 degrees, which supports previous studies
\cite{Heppell2012}. Since our system is diver operated, we expect the vast
majority of images taken will fit this criteria.

Figure\nbsp\ref{fig:variances} shows the variances of the percentage measurement
errors in all images, where each data point represents a single image. We see
that the variance in measurements is much higher when the tilt angle is higher
and the camera is close to the image. My current hypothesis for this is that
when the calibration board is closer to the camera, the distance covered by the
tilting checkerboard is a more significant percentage of the overall distance,
which causes the error of the distance measured to be larger.

#+ATTR_LATEX: :scale 0.8
#+ATTR_ORG: :width 200px
#+CAPTION: Variances of percentage length error, with each data point being a single image.
#+LABEL: fig:variances
[[file:images/checkerboard_error_variances.png]]

*** Laser Attenuation
<<sec:laser-comparison>>
The bulk of our experiments have been with two main laser pointer models: the
Innovative Scuba Concepts laser
pointer\footnote{https://dealer.innovativescuba.com/tc-101-aluminum-underwater-laser-pointer.html},
and the Shark
Laser\footnote{https://waterprooflaser.com/contents/en-us/d22\_TO-ORDER.html}.
Both lasers are rated for less than 5mW of power. The former model emits light
at roughly 700nm, and the latter at roughly 532nm.

Figure\nbsp\ref{fig:laser-test} shows an example of a comparative range test with
the Shark laser and three different red lasers, of which the innovative Scuba
laser is second from the left. Our goal with this test was to qualitatively
understand how attenuation of different laser colors would affect how it would
be used. In pool water, we were able to observe the green laser from almost the
entire width of the pool (roughly 15m), while the red laser was effectively
unusable at around 5m. This did not take into account the fact that the the
diver would sometimes not able to keep the camera steady enough to perceive the
red laser dot at distances of roughly 4m.

#+ATTR_LATEX: :scale 0.3
#+ATTR_ORG: :width 200px
#+CAPTION: Lasers from left to right: Shark laser (green), Innovation Scuba, Orca, Gold
#+LABEL: fig:laser-test
[[file:images/lasers-close.JPG]]

A more detailed discussion of light attenuation in water is included in
Appendix\nbsp[[sec:light-attenuation]].
** Field Experiments
There are two main locations in which Fishsense Lite modules have been field
tested. The first was conducted off the La Jolla coast. Water conditions in La
Jolla are turbid, so this was an opportunity to test how our system would fare
in low visibility conditions. The main goal of this test was to evaluate the red
and green lasers, looking both at attenuation and how fish would react.

Secondly, in August 2023, we deployed six of our FishSense Lite units (FSL-01 to
FSL-06) to be tested by REEF staff in the Florida Keys. The water conditions in
this region were much clearer than in the La Jolla Kelp forests, so despite the
shorter attenuation of red lasers, the data from this test are predominantly
generated using red lasers. These deployments allowed us to look at how the
system would fare during more long term use, and in the hands of real
recreational divers.

*** Salt Water Measurements
<<sec:fw-vs-sw>>

Much like in our pool tests, the laser was first calibrated, then a fake fish
was measured and lengths were calculated in post-processing.

Length measurements obtained in salt water were generally higher than length
measurements obtained in pool water, as shown in
Figure\nbsp\ref{fig:freshwater-vs-saltwater}. Since the cameras themselves are
calibrated in fresh water, the change in refractive index in salt water is
likely what causes the discrepancy in measurements. However, this is still
within our accepted error margin of 20%, as we defined earlier.

#+ATTR_LATEX: :scale 0.8
#+ATTR_ORG: :width 200px
#+CAPTION: A freshwater vs saltwater comparison of measured fake fish lengths.
#+LABEL: fig:freshwater-vs-saltwater
[[file:images/freshwater-vs-saltwater.png]]

*** Laser Calibration With A Slate
<<sec:field-calibration-testing>>

We have been using the checkerboard method as the standard of calibration, as
this can be easily automated. However, we want to assess how effective slate
calibration is in theory, as it would be a huge asset to people using the device
in the field. Luckily for us, the divers at REEF were able to test the slate
calibration with a slate of their own construction. The methodology was as
follows: each of their seven camera units were calibrated using the same slate,
and the length of the slate, show in
Figure\nbsp\ref{fig:slate-calibration-measurement}, was measured using the newly
obtained laser parameters. Several measurements were taken per image to create a
box and whisker plot, shown in Figure\nbsp\ref{fig:slate-calibration}. We can see
that the measurements are fairly consistent, which demonstrates that the
calibration method is indeed viable.

#+ATTR_LATEX: :scale 0.8
#+ATTR_ORG: :width 200px
#+CAPTION: The length measured in the slate calibration experiment.
#+LABEL: fig:slate-calibration-measurement
[[file:images/slate-calibration-measurement.png]]


#+ATTR_LATEX: :scale 0.8
#+ATTR_ORG: :width 200px
#+CAPTION: Comparisons of slate measurements across different devices.
#+LABEL: fig:slate-calibration
[[file:images/slate-calibration.png]]

*** Fish Behavior Study
<<sec:fish-behavior>>
From our testing in the La Jolla Kelp Forest, divers observed that fish tended
to avoid the green laser. Our main hypothesis for why this occurs is that fish
are able to see green light far better than red. In especially turbid water,
added scattering meant that the beam was visible even when the beam was not
pointed directly at the fish's eyes. An example of this is shown in
Figure\nbsp\ref{fig:sheephead}. This meant that in La Jolla, fish were especially
skittish around the green laser. We realized this because after the divers made
a safety stop further off the bottom, when pointing the green laser at the fish
they seemed less skittish.

#+ATTR_LATEX: :scale 0.08
#+ATTR_ORG: :width 200px
#+CAPTION: A male California Sheephead from the La Jolla Kelp Beds. Note the ``lightsaber'' effect caused by particulates in the water.
#+LABEL: fig:sheephead
[[file:images/sheephead.JPG]]

*** Points of Mechanical Failure
<<sec:mechanical-failure>>

During field testing, we identified two main points where the device would tend
to mechanically fail, listed below.
**** Laser Mount
<<sec:mount-failure>>

The laser mounts are made out of PLA, and therefore absorb water. This causes
the laser mounts to become brittle, and they have been shown to break after a
month or two of operation. This could be due to a combination of absorption of
water, degradation from exposure to ultraviolet light, or the large amount of
stress being put on certain parts by the screws. Current work is being done to
create these mounts out of a different material which does not absorb water and
resists corrosion.

Figure\nbsp\ref{fig:mount-crack} shows the most common stress point which causes the
mount to fail. This cracking is caused by the two screws used to keep the laser
secured to the mount.

#+ATTR_LATEX: :scale 0.5
#+ATTR_ORG: :width 200px
#+CAPTION: Image of one example of mount failure.
#+LABEL: fig:mount-crack
[[file:images/mount-failure.png]]

**** Laser Pointer
The laser pointers themselves have also been known to fail. There are two main
reasons for this:
1. The O-rings fail due to lack of proper maintenance or debris getting in the way, flooding the battery enclosure
2. A mechanical failure from the mechanism which activates the laser. Wear from turning them on so many times causes them to chip.

Discussions with Backscatter are in the works to create a custom solution that
does not need to be unscrewed, and also includes the possibility of having
multiple lasers. Multiple lasers also allow us to estimate the angle of the fish
relative to the camera, which further increases the accuracy of our estimations.

** Discussion of Results
Thus far, we have shown that in ideal scenarios, the measurements of our device
can be quite accurate. However, the "ideal" scenario seems to be rare -- in
practice, it seems to be quite easy for the mount to go out of calibration,
whether it be due to weakness in the mount material, twisting or moving the
laser out of alignment during operation, etc. There is currently no way to know
for sure when this happens, which makes the results from this device more
unreliable than they should be. Our next priority is to create a mount of
sturdier construction such that the mount is less likely to move out of
calibration during use, and that less calibration runs are required.

While this system still has many issues with durability and reliability, we have
still shown that it is capable of obtaining fish lengths to within our desired
error margins.
* Conclusion
<<sec:conclusion>>
** Summary
In this work we have discussed the construction and inner workings of FishSense
Lite. We described the main assumption -- the pinhole camera assumption, which
allows us to draw simple relationships between apparent and actual size. We
established that distance to the fish is necessary to calculate it's actual
length. We discussed both procedures to obtain the laser parameters, and to
obtain the distance to the laser dot from the camera once those parameters are
known. We presented results that verify the accuracy of this system is within
20% of a fish's true length.

** Future Work
There is much work to be done to improve the system. We are working towards
removing some of the current requirements of the subject being imaged. In order
to remove the requirement that the fish be parallel to the image plane, we are
looking into either using multiple lasers, or relying on a machine learning
method to estimate the fish's orientation. We also need to refine some of the
automatic detection algorithms for both the fish and the calibration object, and
continue to build the infrastructure to support fully automatic data processing.

One area which we are especially interested in advancing is removing the
corrective optic, which will be possible once we are able to properly model the
distortions caused by the port of the underwater housing. This will reduce the
cost of our current device significantly, and will also be a significant step in
allowing us to use stereo cameras with Snell's law distortions.

#+LATEX: \appendix

* Derivation of Jacobian
<<sec:jacobian-derivation>>
Recall the function $\mathbf{g}(x)$:
\begin{equation*}
    \mathbf{g}(x) = \begin{bmatrix}
        \lVert p_1 - \ell\rVert\alpha + \ell \\
        \lVert p_2 - \ell\rVert\alpha + \ell \\
        \vdots \\
        \lVert p_n - \ell\rVert\alpha + \ell
   \end{bmatrix},
\end{equation*}
where $p_i$ is the laser dot in optical coordinates from calibration image $i$.

We can split $J_g$ into two block columns representing the Jacobians with respect to different vectors:
\begin{equation*}
    J_g = \begin{bmatrix}
        J_{g\alpha} & J_{g\ell}
    \end{bmatrix}
\end{equation*}
Calculating $J_{g\alpha}$ is fairly trivial, as the function is linear in $\alpha$:
\begin{equation*}
    J_{g\alpha} = \begin{bmatrix}
        \lVert p_1 - \ell\rVert I_{3\times3} \\
        \lVert p_2 - \ell\rVert I_{3\times3}\\
        \vdots \\
        \lVert p_n - \ell\rVert I_{3\times3}
    \end{bmatrix}
\end{equation*}

We can take the Jacobian with respect to $\ell$ by taking the Jacobians of each row individually. For a particular image $i$ we have:
\begin{align*}
    \mathbf{g}_i(\alpha,\ell) &= \lVert p_i - \ell \rVert \alpha + \ell \\
    \frac{\partial}{\partial\ell}\mathbf{g}_i(\alpha,\ell) &= -\frac{1}{\lVert p_i - \ell\rVert}\left[\begin{smallmatrix}
        (p_x - \ell_x)\alpha_x+1 & (p_y - \ell_y)\alpha_x & (p_z - \ell_z)\alpha_x \\(p_x - \ell_x)\alpha_y & (p_y - \ell_y)\alpha_y + 1 & (p_z - \ell_z)\alpha_y \\p_x - \ell_x)\alpha_z & (p_y - \ell_y)\alpha_z & (p_z - \ell_z)\alpha_z + 1
    \end{smallmatrix}\right] \\
        &= I - \frac{1}{\lVert p_i - \ell \rVert}\alpha(p_i - \ell)^T
\end{align*}

We only want the first two columns of this, so we get
\begin{equation*}
    J^i_{g\ell} =  \begin{bmatrix} I_{2\times 2} \\ 0\end{bmatrix}(I_{3\times3} - \frac{1}{\lVert p_i - \ell \rVert}\alpha(p_i - \ell)^T) \in \mathbb{R}^{3\times 2}
\end{equation*}

* Pool Test List
<<sec:pool-test-list>>
Table\nbsp\ref{tab:test-list} shows the full list of pool tests conducted while testing FishSense Lite.
#+ATTR_LATEX:  :align |c|p{0.8\linewidth}|
#+CAPTION: List of pool tests conducted, with their purpose
#+LABEL: tab:test-list
|------------+----------------------------------------------------------------------------------------------------|
|       Date | Purpose                                                                                            |
|------------+----------------------------------------------------------------------------------------------------|
| 2023-02-24 | Lens calibration of first TG6                                                                      |
| 2023-03-03 | Evaluate accuracy of OpenCV's calibration  (Olympus vs. Go Pro, housing vs. no housing)            |
| 2023-03-10 | Test attenuation of red vs. green laser                                                            |
| 2023-03-20 | First attempt of laser calibration                                                                 |
| 2023-04-05 | Repeat of both camera calibration and laser calibration                                            |
| 2023-04-12 | Quantify errors of length measurement from both object tilt and thickness                          |
| 2023-04-19 | Collect length-referenced data for ground-truth comparison                                         |
| 2023-05-03 | Evaluate deviation in laser calibration parameters after travel                                    |
| 2023-05-08 | Obtain laser parameters with new mount                                                             |
| 2023-07-13 | Calibrate FSL-01 camera and laser                                                                  |
| 2023-07-29 | Determine whether FSL-01 has maintained calibration after ocean test                               |
| 2023-08-07 | Camera calibrations for FSL-02, 03, 04, 05, 06 and 07                                              |
| 2023-08-14 | Laser calibrations for FSL-02, 03, 04, 05, 06 and 07                                               |
| 2023-08-18 | Recalibration of laser mounts due to faulty mechanical design. First testing of slate calibration  |
| 2023-10-28 | Flat port data                                                                                     |
| 2023-11-10 | Trial for obtaining more fine-grain length data for different tilt angles                          |
| 2023-11-17 | Second trial for obtaining more fine-grain length data for different tilt angles                   |
| 2024-02-12 | Testing if rougher edges and corners affect the quality of slate calibration                       |
| 2024-02-26 | Laser calibration and length measurements with flat port                                           |
| 2024-03-04 | Data for testing automatic slate calibration                                                       |
| 2024-04-25 | Determine accuracy of range measurements                                                           |
| 2024-05-02 | Better determine accuracy of range measurements with more data points                              |
| 2024-05-09 | Redo the range accuracy test, attempt new camera calibration                                       |
|------------+----------------------------------------------------------------------------------------------------|

* FishSense Lite User Manual
<<sec:user-manual>>
** Setup
1. Take the camera underwater and burp the lens. When reattaching the lens, ensure that it is tightly attached.  The lens needs to remain in the same position it originally calibrated.
2. Turn on the laser by turning the back end clockwise.
3. Take a validation photo using the method specified in Section

** Use
1. When the laser is in use, aim it towards the center of the body mass of the fish. Ensure that the laser is not pointed directly toward a person.
2. In order to image fish, it is not necessary to look through the camera's viewport.  If the laser is visible on the fish, it should also be visible in the camera.
3. Refer to the appropriate camera manual for photo and video collection procedures. Take photos with fish as close to flat in the image as possible; otherwise, the length measurement may be inaccurate.

** Teardown
1. When data collection is complete, turn off the laser.
2. Offload data to a hard disk if necessary.

** Guidelines
1. Do not use the zoom function on the camera. This changes the focal length, rendering our calibration useless.
2. Avoid touching or moving the laser directly. Do not use the laser as a handle. The laser must be rigidly attached and unmoved for calibration values to hold true.
3. The laser must be pointed directly at the fish, as close to its center of body mass as possible.
4. The camera's optical axis should establish approximately a right angle with the vertical axis of the fish.
5. Ensure the fish is located within the effective range of 0.5 to 5 meters from the camera. With 3 meters being the ideal distance.
6. The fish must be illuminated.

** Camera Settings
The following settings are the ones for which the camera was tested, and to keep the data consistent, we recommend that you use them as well. Unless specified, all these settings can be found by pressing the "OK" button.
- Program auto (set the wheel to P)
- Flash off
- ISO-A 1600
- Exposure bias 0
- Burst mode
- Underwater shallow white balance
- ESP metering
- Face priority off
- Accessory off
- JPEG mode underwater
- Autofocus
- Aspect ratio 4000 x 3000
- Output format LF + raw
- Still image stabilization on

* Light Attenuation In Water
<<sec:light-attenuation>>

The optical properties of water create some unique challenges in underwater
optical imaging. Attenuation of light underwater is influenced by two main
factors: absorption, the property of water itself to absorb light, and
scattering, where water can re-emit incident light in other directions.
Formally, the /attenuation coefficient/ $c$ of a medium is defined as a sum of
the /absorption coefficient/ $a$ and /scattering coefficient/ $b$, all of which
are functions of the wavelength of light $\lambda$.
\begin{equation}
c(\lambda) = a(\lambda) + b(\lambda) \label{eq:attenuation}
\end{equation}
All of the above quantities are in units of $m^{}^{}^{-1}$, and represent the fraction of
incident power dissipated per meter.

Figure\nbsp\ref{fig:water-absorption} shows the absorption coefficients of pure
water corresponding to different wavelengths of light -- the least absorbed
portion of the spectra falls within the visible region.

#+ATTR_LATEX: :scale 0.4
#+ATTR_ORG: :width 200px
#+CAPTION: Absorption coefficients of pure water over the electromagnetic spectrum. From Wikimedia Commons\nbsp\cite{Absorption}.
#+LABEL: fig:water-absorption
[[file:images/water-absorption.png]]

#+ATTR_LATEX: :scale 0.2
#+ATTR_ORG: :width 200px
#+CAPTION: Absorption coefficients of pure water in just the visible part of the spectrum. Adapted from Wikimedia Commons\nbsp\cite{Absorption2}.
#+LABEL: fig:water-absorption-visible
[[file:images/water-absorption-visible.png]]


In particular, water permits green and blue light to pass up to an order of
magnitude more easily than red light. Divers experience this the deeper they go
into the ocean -- as sunlight gets attenuated by ocean water, objects that are
red begin to appear black, while blue and green objects remain the same.
Underwater photographers must either bring filters or lights with them
underwater to ensure that the scene they photograph is reflective of its true
colors. Figure\nbsp\ref{fig:underwater-colors} demonstrates the difference in color
as sunlight is absorbed. Also note that ultraviolet and near IR frequencies are
absorbed significantly more than within the visual spectrum -- this is why
underwater imaging is almost never done outside the visual spectrum.

Referring back to the laser comparison in Section\nbsp[[sec:laser-comparison]], the red
laser sits with the 650-700nm wavelength range, and the green laser sits within
the 510-540nm range. As can be seen from
Figure\nbsp\ref{fig:water-absorption-visible}, the green laser is absorbed around 10
times less than the red laser.

#+ATTR_LATEX: :scale 0.2
#+ATTR_ORG: :width 200px
#+CAPTION: Colored rods photographed on the surface and at depth. From PBS Learning Media\nbsp\cite{PBS}.
#+LABEL: fig:underwater-colors
[[file:images/underwater-colors.png]]

The coefficients in Equation\nbsp\ref{eq:attenuation} also change with dissolved or
particulate matter in the water, and will generally increase both $a$ and $b$.

For a detailed guide on the optical properties of water and how they are
studied, the Ocean Optics Web Book\footnote{https://www.oceanopticsbook.info/}
is a great resource.

*** Notes :noexport:
- define attenuation in terms of absorption and scattering? maybe even mention inherent optical properties?
-

* Backmatter                                                         :ignore:
\bibliographystyle{plain}
\bibliography{fishsense}
