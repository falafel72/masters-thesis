#+OPTIONS: toc:nil H:5
#+SETUPFILE: setup.org
#+TITLE: Quantity over Quality: Introducing Fishsense Lite, a Inaccurate Length Measurement Tool for Citizen Science
#+AUTHOR: Kyle Sirong Hu
#+LATEX_HEADER: \degree{Intelligent Systems, Robotics and Control}{M.S}
#+LATEX_HEADER: \degreeyear{2024}
#+LATEX_HEADER: \chair{Professor Curt Schurgers}
#+LATEX_HEADER: \cochair{Professor Ryan Kastner}
#+LATEX_HEADER: \committee{Professor Nikolay Atanasov}

* Frontmatter :ignore:
#+LATEX: \frontmatter
#+LATEX: \makecopyright
#+LATEX: \makesignature
#+BEGIN_dedication
To Kat, my friends, and my family.
#+END_dedication
#+BEGIN_epigraph
#+ATTR_LATEX: :latexcode \vskip0.5pt plus.5fil \setsinglespacing
#+BEGIN_VERSE
One fish, Two fish, Red fish, Blue fish,
Black fish, Blue fish, Old fish, New fish.
This one has a little car.
This one has a little star.
Say! What a lot of fish there are.
#+END_VERSE
#+ATTR_LATEX: :latexcode \vskip\baselineskip
/Dr. Seuss/
#+END_epigraph
#+LATEX: \tableofcontents
#+LATEX: \listoffigures
#+LATEX: \listoftables
#+BEGIN_acknowledgements
This project would not have been possible without the many hands available to
help us throughout these past couple of years.

Thank you to the members of my committee: Curt Schurgers, for his invaluable
guidance on writing this document, Ryan Kastner, for guiding us through the
bureaucracies of research funding, and Nikolay Atanasov, for helping me smooth
out some of the math-heavy portions of this document.

Thank you to Christopher Crutchfield, who has dealt with far more logistical
and managerial issues than a PhD student should ever have to deal with. Thank
you to Nathan Hui, whose stern engineering and management advice was vital to us
making it this far. Thank you to all undergraduates who have worked with the
Fishsense project: Vivaswat Suresh, Raghav Maddukuri, Avik Ghosh, Allen Kan, Ana
Perez, Hamish Grant, Harish Vasanth, Shaurya Raswan, Kyle Tran, Sam Prestrelski,
Adrian Zugehar, and Jennifer Xu. Thank you also to the REU students who helped us gather
valuable data despite unreasonably short notice: Josie Dominguez, Jordan
Reichhart, Ela Lucas, and Nick Reyes.

Thank you to Professor Brice Semmens, Jack Elstner, Alli Candelmo, Jen Loch and
the members of REEF for testing this system and providing us with the initial
problem we aim to solve. An additional thank you to Patrick Paxson and Peter
Tueller, whose work laid the foundation for our current research.

Most importantly, thank you to Fred the fake fish, who we dearly miss. May he
enjoy his freedom in the Pacific Ocean.

I would also like to thank the people I've met during my time at UCSD, who have
made this journey worth it.

Thank you to Kristina ``Kat'' Diep, for always being encouraging and supportive
during my time in graduate school. Your humor and light-heartedness has kept me
going.

Thank you to Matei Gardus, who taught me how to use emacs org-mode several years
ago, and is what this document is written with.

Thank you to the members of UCSD Quizbowl, in particular to Alistair Gray, who
is a dear friend, and to Praveen Nair, who asked me to include the Dr. Seuss
quote in the epigraph.

Thank you the "ibob" crew, who have fostered my love for technology and with who
I have created a thriving community of people with similar interests through
UCSD's ACM chapter.

Thank you to Daniel Truong, as well as both the San Diego Smash and Fighting
Game communities, for allowing me to grow emotionally that much more.

Finally, thank you to my parents, whose continuous support has allowed the
opportunity to do all of this.

#+END_acknowledgements
* Abstract                                                           :ignore:
#+BEGIN_dissertationabstract
Ecologists are interested in studying fish length distributions as a metric for
the health of fish populations, both for fishery management and to gauge the
effect of policies and worsening ocean conditions in a small area. Current
methods of gathering these data involve catch and release programs, which often
kill the fish being measured, is expensive to coordinate, and only obtains a
limited sample of the fish population. These are also not possible in marine
protected areas (MPAs), where fishing is illegal. The state of the art method in
MPAs is to conduct a roving diver survey, where divers must visually estimate
and record individual fish lengths. Measurements are generally low-fidelity, and
divers must be trained and retrained before their estimates are considered to be
sufficiently accurate.

We are developing a different method that can be used by recreational divers,
and generates these data using citizen science efforts. We mount a standard
waterproof laser on a consumer waterproof camera, and obtain fish length purely
from an image of the fish with the laser dot on it. This work focuses primarily
on the extraction of the fish's length from the image, using a priori knowledge
of laser parameters. We show that the accuracy of this solution is better than
visual estimation, to within 15% of the fish's true length.

#+END_dissertationabstract
#+LATEX: \mainmatter
** Annotation                                                      :noexport:
Verify the requirements for including material from your own paper
* Introduction
** Preamble :ignore:
Marine ecosystems provide humankind many resources which we depend on.
Phytoplankton account for 70% of all photosynthesis on Earth, and marine
agriculture is an important source of food worldwide. Data that can be used to
evaluate the health of our oceans is therefore essential for us to understand
the potential impact of climate change. Currently, however, methods of getting
data only a global scale are low-resolution.

This work examines one particular metric of interest: fish length. In context of
the typical length of a species and the typical length within a population, fish
length serves as a useful quantitative metric for determining the overall health
of a fish population. Studies of fish length have also been used to quantify the
impact of specific fishing policies\nbsp\cite{Heppell2012}.

Current global fish length data typically comes from catch and release programs.
In California, the California Collaborative Fisheries Research Program (CCFRP)
is one such program. However, these expeditions incur a lot of overhead for
relatively little gain, may be dangerous for the anglers\nbsp\cite{Rohner2011}, and
also the fish being studied\nbsp\cite{Ramsay2009}. Thus a low effort, non-invasive
approach is desirable.

Roving diver surveys, where a team of divers conducts a visual census of fish
species and their lengths, are one such approach, though humans are only capable
of estimating to within 20% of a fish's true length \cite{Harvey2001}. This
skill also requires retraining as frequent as once every 6 months
\cite{Bell1985}.

This work presents a collaboration with the Semmens Lab and the Reef
Environmental Education Foundation (REEF): a device that can be used by any
recreational diver made from off-the-shelf components, that produces estimations
more accurately than a human being can. Dubbed Fishsense Lite, this device
incorporates laser triangulation techniques to allow divers to take images of a
fish and use them to estimate fish length.

*** Notes from Brice :noexport:
Tracking length distributions can give information about frequency without
having to count all the fish in the population

Frequency is important since we want to be able to fish sustainably

** Problem Specification
This work will target the Fishsense Lite - the device that consists of the laser
triangulation technique as well as the software to process images taken by the
physical device.

The length measurement algorithm makes the following assumptions about the
object of interest:
1. The object is planar.
2. The object is parallel with the image plane.
3. The laser dot is visible and lies in the same plane as the object.

Our desired accuracy goal with this system is only within 20% of the true fish
length -- for this type of data, quantity is desired over quality, especially if
the error is consistent, as methods exist to compensate for those types of
errors. In addition, the current primary method of collecting these data is to
visually estimate the length of a fish, which humans are capable of doing to
within 20% of true length, but requires retraining as frequent as once every 6
months.

Another important assumption that we make is that the image is rectified. We
have found that Snell's law is a significant enough problem for image distortion
that can cause major inaccuracies in depth
estimation\nbsp\cite{Agrawal2012,Luczynski2017}, though work on investigating
solutions to these problems is ongoing and is outside of the scope of this work.

Since our solution is constrained to citizen scientists, we will only be
comparing against other handheld systems and manual visual estimation, and will
not be considering passive data collection methods.
** Related Work
*** Underwater Ranging
#+ATTR_LATEX: :float sideways :align |c||c|c|c|c| :width 70%
#+CAPTION: Comparison of different underwater ranging techniques.
#+LABEL: tab:comparison
|----------------------------+------------------------------+---------------------------------+--------------+--------------------------|
| Technique                  | Minimum Estimated Cost (USD) | Accuracy (relative error)       | Ease of Use  | Range                    |
|----------------------------+------------------------------+---------------------------------+--------------+--------------------------|
|----------------------------+------------------------------+---------------------------------+--------------+--------------------------|
| Stereo Video               | 4600\nbsp\cite{SeaGIS}           | 2.5%\nbsp\cite{Harvey2001}          | Intermediate | 2-10m\nbsp\cite{Mallet2014}  |
|----------------------------+------------------------------+---------------------------------+--------------+--------------------------|
| Laser Caliper              | 600\nbsp\cite{BERGERON2007}      | 12%\nbsp\cite{Stock2021}            | Intermediate | 2-5m\nbsp\cite{Stock2021}    |
|----------------------------+------------------------------+---------------------------------+--------------+--------------------------|
| Acoustic Methods           | 20k\nbsp\cite{Mueller2006}       | 1.1% - 35.2%\nbsp\cite{Mueller2006} | Hard         | 1-16m\nbsp\cite{Mueller2006} |
|----------------------------+------------------------------+---------------------------------+--------------+--------------------------|
| Triangulation Rangefinding | 1200                         | 15%                             | Easy         | 2-5m                     |
|----------------------------+------------------------------+---------------------------------+--------------+--------------------------|

A standard method for collecting fish length measurements uses stereo video
technology\nbsp\cite{Mallet2014}. To measure the actual size of objects in an image,
an additional camera can be added to create a stereo camera setup - provided the
relationship between the two cameras is known, apparent size can be converted to
actual size by determining the distance of the fish from the cameras. These are
typically diver-operated (known as stereo diver-operated video or
stereo-DOV)\nbsp\cite{Goetze2019} or placed in baited remote underwater video
systems\nbsp\cite{Mallet2014}. While stereo-DOV is a more cost-effective solution
than deploying a remote system, the current state of the art still requires
purchasing proprietary hardware and software\nbsp\cite{Goetze2019}, which can be
prohibitively expensive for a citizen scientist at a minimum of
$4,600USD\nbsp\cite{SeaGIS} for a scientist grade stereo video system. In addition,
stereo video generates a large amount of data that requires significant effort
to store and process\nbsp\cite{Tueller2021}.

Commercial stereo video solutions include the AQ1 AM100\nbsp\cite{Shafait2017} and
the AKVA Vicass HD\nbsp\cite{Churnside2012}, typically used in aquaculture. Such
systems are also costly and require a tether to a surface-side computer with
proprietary software that must be used to manage the system. This limits the
regions of the world where the data can be collected as it requires scientists
to interact with the system. The tether also limits the depths at which the data
can be collected.

Another solution for ranging uses laser calipers -- two parallel lasers placed a
known distance away from each other. When calibrated correctly, the distance
between the two laser dots can be used as a reference length to measure the
entire fish\nbsp\cite{Rohner2011, Heppell2012}. For these measurements to be
accurate, both lasers must be perfectly parallel with each other and the camera
axis. Depending on manufacturing tolerances, such a requirement may mean that
lasers must be carefully selected. Typically, this system is calibrated by
measuring the distance between the two laser dots at a large distance before a
dive\nbsp\cite{Heppell2012}. Lengths are then calculated using the known distance
between two points and the projection of the fish onto the camera. While length
estimation becomes simple using this method, the cost of two lasers and time for
the minute readjustments waste valuable resources for researchers. The dual
laser mechanism also requires that the object is larger than the distance of the
two lasers, meaning fish smaller than the offset cannot be measured.

Our system uses only a single laser to measure distance, which removes the need
to calibrate two lasers simultaneously and keep them in parallel. This technique
is similar to a light projection-based triangulation rangefinder
system\nbsp\cite{Parthasarathy1982}, as it uses spatial information about the laser
dot to determine the depth of the subject. This method can be extremely accurate
with the right combination of laser and image sensor - up to 10
micrometers\nbsp\cite{Ebrahim2015, Cavedo2016}. Such sensors have been experimented
with as a low-cost solution for robot localization\nbsp\cite{Nguyen1995}, quality
assurance in manufacturing\nbsp\cite{Cavedo2016}, and 3D scanning\nbsp\cite{Baba2001}.

Single laser range finding also has precedence for use in animal size
studies\nbsp\cite{Jaquet2006, Monkman2019,Breuer2007}. The primary benefit of this
approach is that it is more inexpensive than other solutions and requires less
training to operate\nbsp\cite{Monkman2019}. Jaquet and Breuer et al. utilize a range
finder as a separate module from a regular digital camera\nbsp\cite{Jaquet2006, Breuer2007}.
Data from both modules must be combined and processed manually to
obtain lengths\nbsp\cite{Jaquet2006, Monkman2019}.

A summary of these underwater ranging methods can be seen in
Table\nbsp\ref{tab:comparison}, including Fishsense Lite.

*** Underwater Length Measurement
Previous work has been conducted with Engineers for Exploration under the
Fishsense umbrella\nbsp\cite{Paxson2022, Tueller2021}. These attempts involved using
an Intel Realsense D455 -- a depth camera that used both stereo cameras and
structured light in order to infer depth information. This camera was paired
with a NVIDIA Jetson TX2 for onboard video processing, and was fully contained
in a waterproof enclosure with onboard power and storage. Since the Realsense
was up against a flat acrylic port, distortions from Snell's law caused both the
incoming images and the structured infrared light to refract, which yielded
erroneous depth map information. On top of this, some of the infrared light
reflected back into the lens, adding additional color artefacts in the images
produced, rendering species identification impossible. Traditional camera
calibration procedures are also difficult to do underwater, if not
impossible\nbsp\cite{Wong2022}. We concluded that until we are able to model Snell's
law distortions, a stereo camera approach would be impossible.

Our collaborators have also tried using a laser caliper system to measure fish
length. This system has been used to document the recovery of a population of
Nassau Grouper over 7 years, in order to quantify the effectiveness of recovery
efforts\nbsp\cite{Heppel2012}. While such a system was also demonstrated to be more
effective than visual estimation, it also requires manufacturing a
custom-machined aluminum mount, which is not easily accessible for most diverse.
Calibration of the system also involves verifying that the beams are the same
distance apart up to 15m from the source, which can be challenging to do in a
field setting.

*** Laser Rangefinding
While the technology we use here is novel to fish length measurement, it has
been used for a wide variety of other purposes.
* Algorithm Details
** Preamble :ignore:
Our primary goals with this system are to reduce the cost to gather fish length
data, and to make the practice more accessible to recreational divers. To
accomplish this, we attach an underwater camera (in our case, an Olympus TG6) to
an off-the-shelf underwater laser pointer. Both of these tools are commonly
owned by recreational divers, which is why we target this low cost hardware
setup. Figure\nbsp\ref{fig:fsl} shows an example of one such camera system.

#+ATTR_LATEX: :scale 0.2  :options angle=-90
#+ATTR_ORG: :width 200px
#+CAPTION: A fully assembled Fishsense Lite unit.
#+label: fig:fsl
[[file:images/fishsense-lite-system.jpg]]

Our system relies on the diver taking images with the laser beam pointed on the
fish. Purely from the image, as well as a priori knowledge of the laser beam
parameters, we can then use this to obtain a length of the fish within the
image.

** Pinhole Camera Model
All of the calculations shown assume that our camera follows the pinhole camera
assumption. In air, this assumption is extremely common and is used in stereo
cameras for object triangulation.

The pinhole camera model assumes that all light that comes into the camera
passes through a single point known as the /focal point/, or /optical center/, of
the camera. All incoming rays fall onto the image sensor, which is a plane
perpendicular to the camera's axis, at a fixed distance from the focal point
known as the /focal length/. The image projected onto the sensor is a flipped
version of the real life scene, as the light rays are inverted after passing
through the optical center.

Instead of looking at the image on the image sensor, it is typically the
convention to mirror the image sensor plane in front of the optical sensor, such
that the distance from the focal point to this new plane is also the focal
length. This plane is designated as the /image plane/. This convention allows us
to more easily translate from pixel coordinates to coordinates in the image
plane, as we need only know how large each pixel is to map to physical units
without needing to invert.

Underwater it is more difficult to make the assumption that imaging systems
follow the pinhole camera model, especially with our current hardware setup. Our
camera's transparent port separating the camera lens from the water is flat,
meaning that Snell's law causes incoming light to refract. Measures to mitigate
this problem are detailed in Section\nbsp[[sec:backscatter]].
** Hardware Setup
*** Overview :ignore:

*** Camera
For our own testing, we use an Olympus TG6, a common camera for divers to own
for underwater photography. This camera itself is waterproof, though we use
another protective housing around it.

This is just one example of a camera that can be used for this system --
theoretically any camera would work.
*** Wide Angle Lens
<<sec:backscatter>>

To fit our pinhole camera assumption, we must have some way to rectify images
taken by the camera. Distortions from Snell's law make this impossible using the
standard camera model\nbsp\ref{Agrawal2012}.

We add a corrective optic in order to correct these distortions. Developed by
Backscatter, this corrective optic is designed to attach to the Olympus's
underwater housing, and is typically used to widen the field of view for
underwater photographers. Future work is required for us to understand how to
model these Snell's law distortions better so we can ultimately remove it.

** Quantites and Conventions
Our axis conventions in this chapter are as follows: the $x$ axis points to
the right in the image, the $y$ axis points downward in the image, and the $z$
axis points forward, away from the camera. This system has the added bonus that
the $x$ and $y$ axes coincide with the directions of pixel coordinates. The
origin will be defined at the camera's optical center.

Given a particular fish image, we assume that we are able to trivially determine
the size of the fish in pixels. Figure\nbsp\ref{} shows a diagram of both the live
fish and its projection onto the image plane. If the apparent size of the fish
is known, using similar triangles we are able to obtain the following
relationship:
\begin{equation}
L = \frac{dxw}{f} \label{eq:length-from-depth}
\end{equation}

Here $x$ represents the size of the fish in pixels, $w$ represents the ``pixel
pitch'' (distance between two pixels on the image sensor), and $f$ represents
the focal length of the camera. Note that the product $xw$ can be interpreted as
the size of the fish on the image plane. Our only unknown on the right hand side
is $d$, the distance of the fish from the camera. Most of the challenge in this
system comes from obtaining this measurement.

We assume that we can describe precisely how the laser is positioned relative to
the camera. More precisely, we define parameters $\ell$ and $\alpha$ , where $\ell$ is the
3D vector from the origin to the laser beam in the $xy$ plane, and $\alpha$ is a 3D
vector of unit length that points in the direction of the laser beam. The laser
beam can therefore be described by a ray with scale factor $\lambda$:
\[
\text{laser} = \ell + \lambda\alpha
\]

Note that our restriction that $\ell$ lies purely in the $xy$ plane means that the
$z$ component of $\ell$ is always zero.

We also assume that we are able to accurately find the laser dot in the image.
In practice, these are hand-labeled, but automatic algorithms are outside the
scope of this document.

Figure\nbsp\ref{fig:diag} illustrates all the aforementioned quantities in one place.

#+CAPTION: Diagram demonstrating known quantities. Laser shown in green, fish plane shown in red, image sensor shown in gray.
#+LABEL: fig:diag
#+begin_figure
    \centering
    \begin{tikzpicture}
    [cube/.style={very thick,black},
			grid/.style={very thin,gray},
			axis/.style={->,black,thick}];

   \filldraw[
        draw=red,%
        fill=red!20,%
    ]          (1,1,-5)
            -- (1,-1,-5)
            -- (-1,-1,-5)
            -- (-1,1,-5)
            -- cycle;
    \filldraw[
        draw=black,%
        fill=black!20,%
    ]          (0.2,0.15,-1)
            -- (0.2,-0.15,-1)
            -- (-0.2,-0.15,-1)
            -- (-0.2,0.15,-1)
            -- cycle;

    \draw[green,thick]  (1.5,0.5,0) node[anchor=west,black]{$\alpha \mid \lVert\alpha\rVert = 1$} --(0.3,0.1,-5) node[anchor=west,black]{$p$};

    \draw[->, black,thin] (1.5,0.5,0)--(1.26732998, 0.42244333, -0.96945842) node[anchor=east]{$\ell$};
    \draw[black,dashed] (0.3,0.1,-5)--(0,0,0);
    \draw[axis] (0,0,0)--(1,0,0) node[anchor=west]{$x$};
    \draw[axis] (0,0,0)--(0,-1,0) node[anchor=west]{$y$};
    \draw[axis] (0,0,0)--(0,0,-1) node[anchor=west]{$z$};
    \end{tikzpicture}
#+end_figure

** Finding the Distance to the Fish
We assume that this laser beam intersects the fish at an unknown point $p$. As
imaged by the camera, we know that this laser dot will have known pixel
coordinates $\mathfrak{p}$. This corresponds to a vector $p_\text{image}$ on the
image plane, defined as
\[
p_\text{image} = \begin{bmatrix}
\mathfrak{p}_x \\
\mathfrak{p}_y \\
f
\end{bmatrix}
\]

We arbitrarily decide to scale this vector to be of unit
length, defining
\[
v = \frac{p_\text{image}}{\lVert p_\text{image}\rVert}
\]

This creates two rays that converge toward $p$: the laser beam, and the ray from
the camera that passes through $\mathfrak{p}$ on the image plane. By projecting
both of these rays outward until they intersect, we can identify where $p$ is.

More formally, we define scale factors $\lambda_1$ and $\lambda_2$ such that
\[
\ell + \lambda_1\alpha = \lambda_2v
\]

We can refactor the above relationship to be the form

\begin{equation*}
\begin{bmatrix}
-\alpha & -v
\end{bmatrix}
\begin{bmatrix}
\lambda_1 \\
\lambda_2
\end{bmatrix}
+ \ell = 0
\end{equation*}

Due to pixel quantization, the two rays may not line up perfectly, so we can
obtain the least-squares solution, i.e. the solution to the minimization problem
\[
\text{argmin}_{x} \lVert Ax - b \rVert^2
\]

where $A \in \mathbb{R}^{m\times n}$, $m > n$, and $x,b\in \mathbb{R}^{n}$. In our case, we
define the following:
\begin{align*}
A &= \begin{bmatrix}
\alpha \ -v
\end{bmatrix} \\
x &= \begin{bmatrix}
\lambda_1 \\
\lambda_2
\end{bmatrix} \\
b &= -\ell
\end{align*}

In general, we can solve a least squares problem with the following formula:
\[
x = (A^TA)^{-1}A^T b
\]

Using this, we can obtain a closed form solution for both $\lambda_1$ and $\lambda_2$. We
only require one of them, so we use $\lambda_2$:
\[
 \lambda_2 = \frac{-\alpha^T \ell \alpha^T v + v^T \ell}{1 - (\alpha^T v)^2}
\]

Experiments from using $\lambda_1$ or $\lambda_2$ are show in Section\nbsp\ref{}.

Obtaining $\lambda_1$ or $\lambda_2$ allows us to obtain $p$, and the z-component of $p$,
along with Equation\nbsp\ref{eq:length-from-depth}, gives us the corresponding fish
length.

** Finding Laser Parameters
<<sec:laser-calibration>>
So far, we have assumed that the parameters of the laser $\alpha$ and $\ell$ are known.
These cannot be measured directly for two reasons:
1. Precise measurements relative to the optical center of the camera are hard to
   obtain, since in reality the location of the optical center of the camera is
   not known.
2. Since the laser mount is not perfectly stable, and can change in between
   dives and over time, the parameters need to be recalculated.
Thus, we must use a calibration procedure.

We have $n$ images from which we obtain laser points $p_i$. Here I describe two
possible algorithms to solve this. A comparison of the two methods is presented
in Section\nbsp[[sec:math-testing]].
**** Hu
We leverage the fact that the parameterized laser beam must intersect with
the laser dot point to give us the following series of equations:
\begin{equation}
p_i = \lVert p_i - \ell \rVert\alpha + \ell
\label{eq:problem}
\end{equation}

We stack these points into a single vector, defining the following; \[
\mathbf{p} = \begin{bmatrix} p_1 \\ p_2 \\ \vdots \\ p_n \end{bmatrix} \] We
have a relationship that relates our known quantities $p_i$ and unknown
quantities $\alpha$ and $\ell$, though in this case the relationship is
non-linear. We must therefore choose a non-linear optimization method to find
the best candidates for $\alpha$ and $\ell$ that satisfy this.

First we define a parameter vector $x$ that contains all of our parameters:
\[
x = \begin{bmatrix}
\alpha \\
\ell_x \\
\ell_y
\end{bmatrix}
\]

We must first define a function in terms of our parameters which we want to
minimize:

\[
g_i(x) = \lVert p_i - \ell \rVert \alpha + \ell
\]

We then define the vector function $F(\alpha,\ell)$ as follows:
\[
\mathbf{g}(x) = \begin{bmatrix}
g_1(x) \\
g_2(x) \\
\vdots \\
g_n(x)
\end{bmatrix}
\]

We then formulate this in terms of the following optimization problem:
\begin{align}
\text{argmin}_{x}\lVert \mathbf{r}(x)\rVert, \nonumber \\
\mathbf{r}(x) = \mathbf{p} - \mathbf{g}(x) \label{eq:residual}
\end{align}

The method we currently choose to solve this is the Gauss-Newton method, which involves the following steps:
1. Find the Jacobian $J_r$ of the minimizing function w.r.t to $x$.
2. Take iterative steps of the following form:
\[
x^{(k+1)} = x^{(k)} - (J_r^TJ_r)^{-1}J_r \mathbf{r}(x^{(k)})
\]

According to Equation \ref{eq:residual}, only the second term depends on our parameters, so we can rework our iterative step into
\[
x^{(k+1)} = x^{(k)} + (J_g^{T}J_g)^{-1}J_{g}\mathbf{g}(\alpha,\ell)
\]


There are many other methods of this kind that we could have used, such as the
Levenberg-Marquardt algorithm, though from our experiments this calibration
method has been sufficient.

The Jacobian of $\mathbf{g}(x)$ is given by the following, and the full
derivation is detailed in Section\nbsp[[sec:jacobian-derivation]]:

\begin{align}
J_g &= \begin{bmatrix}
J_{g\alpha}^1 & J_{g\ell}^1 \\
J_{g\alpha}^2 & J_{g\ell}^2 \\
\vdots & \vdots \\
J_{g\alpha}^n & J_{g\ell}^n
\end{bmatrix} \in \mathbb{R}^{3n\times5}\nonumber \\
J_{g\alpha}^i &= \lVert p_i - \ell\rVert I \in \mathbb{R}^{3\times3}\\
J_{g\ell}^i &= \begin{bmatrix}
1 & 0 \\
0 & 1 \\
0 & 0
\end{bmatrix}
\left(I_{3\times3} - \alpha\frac{(p_i - \ell)^T}{\lVert p_i - \ell\rVert }\right) \in \mathbb{R}^{3\times2}
\end{align}

***** Derivation of Jacobian
<<sec:jacobian-derivation>>
Recall the function $\vb{g}(\alpha,\ell)$:
\begin{equation*}
    \vb{g}(\alpha,\ell) = \begin{bmatrix}
        \lVert p_1 - \ell\rVert\alpha + \ell \\
        \lVert p_2 - \ell\rVert\alpha + \ell \\
        \vdots \\
        \lVert p_n - \ell\rVert\alpha + \ell
   \end{bmatrix},
\end{equation*}
where $p_i$ is the laser dot in optical coordinates from calibration image $i$.

We can split $J_g$ into two block columns representing the Jacobians with respect to different vectors:
\begin{equation*}
    J_g = \begin{bmatrix}
        J_{g\alpha} & J_{g\ell}
    \end{bmatrix}
\end{equation*}
Calculating $J_{g\alpha}$ is fairly trivial, as the function is linear in $\alpha$:
\begin{equation*}
    J_{g\alpha} = \begin{bmatrix}
        \lVert p_1 - \ell\rVert I \\
        \lVert p_2 - \ell\rVert I \\
        \vdots \\
        \lVert p_n - \ell\rVert I
    \end{bmatrix}
\end{equation*}

We can take the Jacobian with respect to $\ell$ by taking the Jacobians of each row individually. For a particular image $i$ we have:
\begin{align*}
    \vb{g}_i(\alpha,\ell) &= \lVert p_i - \ell \rVert \alpha + \ell \\
    \frac{\partial}{\partial\ell}\vb{g}_i(\alpha,\ell) &= -\frac{1}{\lVert p_i - \ell\rVert}\left[\begin{smallmatrix}
        (p_x - \ell_x)\alpha_x+1 & (p_y - \ell_y)\alpha_x & (p_z - \ell_z)\alpha_x \\(p_x - \ell_x)\alpha_y & (p_y - \ell_y)\alpha_y + 1 & (p_z - \ell_z)\alpha_y \\p_x - \ell_x)\alpha_z & (p_y - \ell_y)\alpha_z & (p_z - \ell_z)\alpha_z + 1
    \end{smallmatrix}\right] \\
        &= I - \frac{1}{\lVert p_i - \ell \rVert}\alpha(p_i - \ell)^T
\end{align*}

We only want the first two columns of this, so we get
\begin{equation*}
    J^i_{g\ell} =  \begin{bmatrix} I_{2\times 2} \\ 0\end{bmatrix}(I_{3\times3} - \frac{1}{\lVert p_i - \ell \rVert}\alpha(p_i - \ell)^T) \in \mathbb{R}^{3\times 2}
\end{equation*}

**** Atanasov
Once again assuming we have $n$ images with image $i$ corresponding with laser
dot point $p_i$, a straightforward method to get our laser trajectory $\alpha$ can be
obtained from a normed average over the differences between all points:
\[
        \alpha = \frac{1}{n(n-1)}\sum_{i=1}^n\sum_{j\neq i}\frac{p_i - p_j}{\lVert p_i - p_j \rVert}
\]

This can be flipped as necessary to face the direction in which the laser beam
travels. Once $\alpha$ is obtained, we can choose a point $p_k$ such that $p_k$ is on
our ray, and pick the intersection of the ray defined by $p_k$ and $\alpha$ with the
$xy$ plane to be $\ell$.

** Missing pieces :noexport:
- [ ] the laser hardware setup rationale
- [X] pinhole camera model ("optical center")
- [X] the image plane
- [ ] camera calibration
- [X] laser field calibration
** Conclusion :ignore:
* Procedures
** Calibration Procedures
*** Camera Calibration
*** Laser Calibration
Since our laser mount is not stable enough to guarantee that laser parameters
stay consistent between dives, we require a laser calibration at the beginning
of every dive. To do this, we assume that we know the optical frame coordinates
of each laser dot $p_i$. This requires that we have some kind of depth reference.

The first method that we tested was to use the same checkerboard pattern used in
camera calibration. The main reasoning for this method was that there are two
parts to this method for which OpenCV provides functionality for - detecting
corners of specifically the checkerboard pattern, and calculating a 3D
transformation given object points and corresponding points in an image.
However, in the field, requiring that divers carry a large and heavy
checkerboard for every dive is a large ask.

We have developed a procedure that does not utilize a checkerboard, and can
instead be done with a dive slate -- something that most divers will carry. An
example of one of these is shown in Figure\nbsp\ref{fig:slate-calibration}. We add
pieces of duct tape in an arbitrary pattern on one side to make the slate more
featureful. We also make the assumption that the slate remains close to parallel
with the image plane.

#+ATTR_LATEX: :scale 0.4
#+ATTR_ORG: :width 200px
#+CAPTION: An example of a slate calibration image.
#+label: fig:slate-calibration
[[file:images/slate-calibration.JPG]]

We assume that we have a scanned copy of the dive slate, and hence the physical
measurements of the duct tape pattern. We perform the following steps:
1. Get the largest contour on the slate scan
2. Get the most similar shape on the calibration image
3. Fit an ellipse to the slate contour, match this with the calibration contour to find rotation and scale
4. Find the center of masses of both contours and use the difference to obtain translation
5. Combine scale, rotation and translation to get a 2D transform
6. Take contour, use transform to get corresponding points in calibration image
7. Use these points correspondences to obtain a 3D transformation using solvePnP
8. Define the slate plane using this 3D transformation
9. Use the detected laser point and project a ray out to the plane. The intersection is our calculated laser dot coordinate.

Results demonstrating this method compared to the checkerboard are shown in
Section\nbsp[[sec:field-calibration-testing]].

** Conclusion :ignore:
* Testing
** Locations
The data in this section were gathered from four locations, listed below.
*** Canyonview Pool
The vast majority of our data comes from Canyonview pool.
**** Note :noexport:
list of tests should go in an appendix
*** La Jolla Kelp Forests
In order to gain some insight into how this system would behave in real world
conditions, we used this system on fish off the La Jolla coast. The author would
like to acknowledge and thank Nathan Hui and Jack Elstner for collecting these
data.

Water conditions in La Jolla are turbid, so this was an opportunity to test how
our system would fare in low visibility conditions. The main goal of this test
was to evaluate the red and green lasers, looking both at attenuation and how
fish would react.

Two devices, FSL-N and FSL-01, were used during this test. FSL-N was fitted with
the green laser, and FSL-01 with the red.

We used Fred as a reference object and took measurement photos from very close
(about 1m) to as far as visibility allowed. This was done with both FSL-N and
FSL-01. After this, divers conducted a roving survey, taking fish measurement
images in the same area. Divers did not take images of the same individuals.

This was Fred's final test. According to our recorded logs, we know the following:
#+BEGIN_QUOTE
``Anchor stuck, Fred was secured to anchor before ascent. Fred lost when anchor was retrieved.''
#+END_QUOTE
We thank Fred for his service.

*** REEF Florida Keys Deployment
The Reef Environmental Education Foundation (REEF) is an organization affiliated
with the Semmens Lab. In August 2023, we deployed six of our Fishsense Lite
units (FSL-01 to FSL-06) to be tested by REEF staff in the Florida Keys.

The water conditions in this region were much clearer than in the La Jolla Kelp
forests, so despite the shorter attenuation of red lasers, the data from this
test are predominantly generated using red lasers.

Standard roving diver surveys were conducted in ??? different areas. On all
dives, a Fishsense Lite camera unit, along with a stereo Go Pro pair, were used
to measure fish lengths.
** Tests
*** Overall Accuracy
<<sec:accuracy>>

We have attempted to measure the device's rangefinding accuracy by comparing the
calibrated test measurements with a tape measure. Figure\nbsp\ref{} demonstrates the
results. As can be seen, the range from the tape measure sags below the range
found from the tape measure. This may be because in our experiment setup, the
tape measure is suspended in the water, while we attempted to keep it as taut as
possible. We hypothesize that the tape measure sagging may be the cause of the
discrepancy in measured distances.

To measure the accuracy of this system, we rely on the accuracy of length
measurements obtained, since we are unable to rely on the true position and
orientation of the laser. In particular, we use relative length, as our sensor
is image-based, and errors would therefore occur in relation to the object's
apparent size, which is depth dependent. The key assumption which we rely on is
that the size of the object is irrelevant, as we only care about the apparent
size in the image.

In order to fully simulate the purpose for which the device was intended, we use
a dummy rainbow trout as a reference object. Each dummy is made from the same
model, though different techniques have been used to make them negatively
buoyant. Three generations of dummy fish, named Fred, George, and Ginny, have
been used for these tests. Fred is shown in Figure \ref{fig:fred}.

#+ATTR_LATEX: :scale 0.05
#+ATTR_ORG: :width 200px
#+CAPTION: Fred, our first fake fish. He, and all the others, are 31cm long.
#+LABEL: fig:fred
[[file:images/fred.jpg]]

Figure\nbsp\ref{} demonstrates measurements of our fake fish, in this case, Fred,
during a pool test. We can see that with varying distances away from Fred, the
estimate remains relatively consistent, and we always underestimate Fred's
length. There are several potential sources of error that are discussed below.

**** Object Thickness
Fred, and other fishes we measure, have some thickness. Therefore, if the laser
dot is in the middle of the fish, the laser dot point is at a different depth
than the snout and fork points. We know that Fred is approximately 8cm thick at
his thickest point, so we can adjust for this. Figure\nbsp\ref{} shows the adjusted
length measurements, and we can see that they line up almost perfectly.

**** Laser Detection Errors
Small errors in the location of the detected laser point can contribute to errors in the
*** Comparison with Stereo Video
During the REEF deployment, the Fishsense Lite camera modules were tested
simultaneously with a stereo Go Pro rig with a baseline of 0.5m. Figure\nbsp\ref{}
shows examples of fish length distributions from specific individual fish. To
test if the data collected with the two different systems could be considered to
be significantly different, we used a Kruskal-Wallis test. This was done using
R's built in =kruskal.test= function, which assumes a null hypothesis that the
distributions from the two groups are the same. From this, we get a p-value of
0.453, which indicates that given the two distributions are the same, the
current data is fairly likely.

The author would like to acknowledge and thank Jen Loch for analyzing the stereo
video data, and for providing the statistical background needed to write this
section.
*** Field Calibration Accuracy
<<sec:field-calibration-testing>>

We compare our checkerboard laser calibration procedure to our automated field
laser calibration procedure. Figure\nbsp\ref{} demonstrates that the measurement
accuracy obtained between these two methods is similar, though the field
calibration procedure performs slightly worse. My current hypothesis is that the
largest source of error mainly comes from having significantly fewer data points
with the dive slate as compared to the checkerboard, and that if as many images
were taken with the dive slate as with the checkerboard, the measurement error
would decrease.

*** Different Calibration Procedures
<<sec:math-testing>>

We again test the accuracy of the calibration by comparing reference length
measurements with different laser calibrations.
Figure\nbsp\ref{fig:laser-calibration-comparison} shows the results from obtained
from both calibrations -- as can be seen, they are virtually identical.

#+ATTR_LATEX: :scale 0.8
#+ATTR_ORG: :width 200px
#+CAPTION: Comparison of two calibration methods, graphing estimated distance against estimated length.
#+LABEL: fig:laser-calibration-comparison
[[file:images/laser-calibration-comparison.png]]

This led me to investigate whether the two methods may lead to equivalent
solutions. It is possible that the Atanasov method may happen to yield the laser
parameter solution that minimizes the function defined in the Hu method.

*** Effects of Freshwater Versus Saltwater
Unsurprisingly, we found that the red laser attenuated much more than the green
laser. Particles in the water also affected this, as the light would scatter off
and render the laser beam visible, as show in Figure\nbsp\ref{fig:sheephead}.

One other interesting result was related to the change in salinity between pool
water and sea water. Length measurements obtained in salt water were generally
higher than length measurements obtained in pool water.

#+ATTR_LATEX: :scale 0.1
#+ATTR_ORG: :width 200px
#+CAPTION: A male California Sheephead from the La Jolla Kelp Beds. Note the ``lightsaber'' effect caused by particulates in the water.
#+LABEL: fig:sheephead
[[file:images/sheephead.JPG]]

*** Different Lasers
<<sec:laser-comparison>>
We discovered two significant factors that influenced our decision to use one
laser over the other.

The first was attenuation - within the visible spectrum, water attenuates light
with longer wavelengths significantly more than shorter wavelengths, and so red
light is more difficult to see at greater distances. From our testing,
we found that the red laser became incredibly difficult to spot at around 5m.
Figure\nbsp\ref{fig:laser-test} shows the two laser dots side by side in clear water at a distance
of 2m. The red laser is barely visible, while the green laser is still very bright.

#+ATTR_LATEX: :scale 0.3
#+ATTR_ORG: :width 200px
#+CAPTION: Lasers from left to right: Shark laser (green), Innovation Scuba, Orca, Gold
#+LABEL: fig:laser-test

The second was fish behavior - from tests with real fish, divers have observed
that fish tended to avoid the green laser. Our main hypothesis for why this
occurs is that fish are able to see green light far better than red. In
especially turbid water, added scattering meant that the beam was visible even
when the beam was not pointed directly at the fish's eyes. This meant that in La
Jolla, fish were especially skittish around the green laser. We realized this
because after the divers made a safety stop further off the bottom, when
pointing the green laser at the fish they seemed less afraid.

*** Robustness to Tilt
To evaluate the system's accuracy with respect to different tilt angles, we had
the following experiment setup. We captured a total of 183 images of a
checkerboard pattern with the laser dot on it, with varying tilt angles and
depths. A subset of these images were used to obtain the laser position and
orientation. Using these data, from each image we extracted 13 lengths, as shown
in Figure\nbsp\ref{fig:checkerboard-experiment-setup}.
Figures\nbsp\ref{fig:checkerboard-errors-by-length} and
\ref{fig:checkerboard-errors-by-distance} shows the results of this experiment.

#+ATTR_LATEX: :scale 0.5
#+ATTR_ORG: :width 200px
#+CAPTION: Illustration of lengths measured on checkerboard.
#+LABEL: fig:checkerboard-experiment-setup
[[file:images/checkerboard-experiment-setup.png]]

#+ATTR_LATEX: :scale 0.8
#+ATTR_ORG: :width 200px
#+CAPTION: Percent error of length measurements over all distances and angles, grouped by true object length.
#+LABEL: fig:checkerboard-errors-by-length
[[file:images/checkerboard_errors_by_length.png]]

#+ATTR_LATEX: :scale 0.8
#+ATTR_ORG: :width 200px
#+CAPTION: Percent error of length measurements over all distances and angles, plotted over the distance from the camera each image was taken.
#+LABEL: fig:checkerboard-errors-by-distance
[[file:images/checkerboard_errors_by_distance.png]]

Both figures show that the relative error in measurement decreases dramatically
when the angle becomes lower than 15 degrees, which supports previous studies
\cite{Heppell2012}. Since our system is diver operated, we expect the vast
majority of images taken will fit this criteria.

Figure\nbsp\ref{} shows the variances of the percentage measurement errors in all
images, where each data point represents a single image. We see that the
variance in measurements is much higher when the tilt angle is higher. We
hypothesize that this is due to small deviations from the true laser dot
center location.

*** Points of Mechanical Failure
<<sec:mechanical-failure>>
There are several areas in which the mechanical system can fail.
**** Laser Mount
<<sec:mount-failure>>

The laser mounts are made out of PLA, and therefore absorb water. This causes
the laser mounts to become brittle, and they have been shown to break after a
month or two of operation. Current work is being done to create these mounts out
of a different material which does not absorb water and resists corrosion.

**** Laser Pointer
The laser pointers themselves have also been known to fail. There are two main
reasons for this:
1. The O-rings fail due to lack of proper maintenance or debris getting in the way, flooding the battery enclosure
2. A mechanical failure from the mechanism which activates the laser. Wear from turning them on so many times causes them to chip.
*** User Experience
Divers have said the following about using the Fishsense Lite modules:
- Generally the devices are ``fun and easy to use''
- They take 1-2 dives to get used to

Our biggest issues with the hardware are those of durability -- with prolonged
use, we found that laser points would leak, and laser mounts would crack.

One other concern we had was how difficult it would be to get an image such that
the fish is close to parallel with the image plane -- this turned out to not be
a big issue if such an angle was easily accessible. However, if the fish
happened to be next to a rock at an angle, getting a good picture proved
difficult.
** Conclusion :ignore:
* Conclusion
In this work we have discussed the construction and inner workings of Fishsense
Lite. We have laid out the conditions for which it works, described the
procedures required to operate it, and the underlying principle behind the
distance measurement mechanism. We have also demonstrated that the current
iteration of the device is capable of measuring fish lengths to within 15% of
the true length. Field testing has demonstrated that this device has the
potential to be adopted by recreational divers, as it is indeed relatively easy
to use and manufacture.

There is much work to be done to improve the system. We are working towards
removing some of the current requirements of the subject being imaged -- no
longer needing the fish to be perfectly parallel to the image plane, refining
some of the automatic detection algorithms, and continuing to build the
infrastructure to support fully automatic data processing.

One area which we are especially interested in advancing is removing the
corrective optic, which will be possible once we are able to properly model the
distortions caused by the port of the underwater housing.



#+LATEX: \appendix
* Appendix
** Test Schedule

#+ATTR_LATEX:  :align |c|p{0.8\linewidth}|
#+CAPTION: List of pool tests conducted, with their purpose
#+LABEL: tab:comparison
|       Date | Purpose                                                                                                                    |
|------------+----------------------------------------------------------------------------------------------------------------------------|
| 2023-02-24 | Lens calibration of first TG6                                                                                              |
| 2023-03-03 | Evaluate accuracy of OpenCV's calibration for different camera configurations (Olympus vs. Go Pro, housing vs. no housing) |
| 2023-03-10 | Test attenuation of red vs. green laser                                                                                    |
| 2023-03-20 | First attempt of laser calibration                                                                                         |
| 2023-04-05 | Repeat of both camera calibration and laser calibration                                                                    |
| 2023-04-12 | Quantify errors of length measurement from both object tilt and thickness                                                  |
| 2023-04-19 |                                                                                                                            |
| 2023-05-03 | Evaluate deviation in laser calibration parameters after travel (nathan's house)                                           |
| 2023-05-08 | Obtain laser parameters with new mount (nathan's house)                                                                    |
| 2023-07-13 | Calibrate FSL-01 camera and laser  (nathan's house)                                                                        |
| 2023-07-29 | Recalibrate and determine whether FSL-01 has maintained calibration after ocean test (nathan's house)                      |
| 2023-08-07 | Camera calibrations for FSL-02, 03, 04, 05, 06 and 07 (nathan's house)                                                     |
| 2023-08-14 | Laser calibrations for FSL-02, 03, 04, 05, 06 and 07 (nathan's house)                                                      |
| 2023-08-23 | Recalibration of                                                                                                           |
| 2023-08-29 |                                                                                                                            |
| 2023-10-28 | Flat port data                                                                                                             |
| 2023-11-10 | Trial for obtaining more fine-grain length data for different tilt angles                                                  |
| 2023-11-17 | Second trial for obtaining more fine-grain length data for different tilt angles                                           |
| 2024-02-12 | Test different dive slate                                                                                                  |
| 2024-02-22 |                                                                                                                            |
| 2024-02-26 |                                                                                                                            |
| 2024-03-04 |                                                                                                                            |

** Fishsense Lite User Manual
* Backmatter                                                         :ignore:
\bibliographystyle{plain}
\bibliography{fishsense}
* Problem Motivation                                               :noexport:
Scrapping this, it's too wordy 

I will begin describing the problem with some background on the work that
Engineers for Exploration (E4E for short) does, as I think it provides important
context for where the work came from.

** Who are E4E?
Engineers for Exploration are a research lab that specializes in developing
instrumentation for studying wildlife conservation and archaeology. Founded by
Professors Ryan Kastner and Curt Schurgers, along with Albert Lin, it combines
engineering with conservation efforts to create a completely unique research
area that is highly interdisciplinary. We primarily collaborate with scientists
with the goal of aiding them in their research, by designing systems that suit
their specific needs. Some examples of projects that we currently do include:
Acoustic Species ID, a collaboration with the San Diego Zoo with the goal of
studying bird populations in the Amazon Rainforests by classifying bird calls
from audio moths.

** Our Collaborators and Their Goals
Fishsense's primary collaborators are the Semmens Lab in the Scripps Institute
of Oceanography, and the Reef Environmental Education Foundation (REEF). Much of
my current understanding of how fish lengths are recorded is based on work done
by Professor Brice Semmens\nbsp\cite{Heppell2012}.

** My Role
I began attending UCSD as an undergraduate in September of 2018, and had been
intermittently involved with several of E4E's projects. In September of 2022, I
was moved from the Radio Telemetry Tracking project onto Fishsense to
investigate issues with a previous prototype. The device consisted of a
waterproof enclosure containing an Intel Realsense D455 and an NVIDIA Jetson
TX2, which was designed to process depth maps and convert these to fish lengths
of all fishes in a particular scene in real time. The issues with this device
were two-fold: the Realsense lacked sufficient resolution for species
identification, and distortions stemming from Snell's law prevented us from
obtaining accurate depth maps.

After several unsuccessful attempts at underwater calibration of the Realsense,
we decided to pivot to a much more rudimentary solution - rigidly attach a laser
beam to a normal underwater camera, have a diver take images of fish with this
laser beam on the fish, and perform post-processing to extract fish length. This
work will primarily be focused on this device. rigidly attach a laser beam to a
normal underwater camera, have a diver take images of fish with this laser beam
on the fish, and perform post-processing to extract fish length. This work will
primarily be focused on this device.

** Applications of the System
The problem that this system aims to solve is slightly different from the
original intent of the project - since our system is much more accessible than
our previous prototype, it can be easily deployed in the hands of recreational
divers, who can collect and upload data that will become publicly available for
worldwide usage. We henceforth refer to these recreational divers as "citizen
scientists". Citizen science efforts allow for data collection over a much wider
area than is possible with specialized research deployments.

Our desired accuracy goal with this system is only within 20% of the true fish
length - for this type of data, quantity is desired over quality, especially if
the error is consistent, as methods exist to compensate for those types of
errors. In addition, the current primary method of collecting these data is to
visually estimate the length of a fish, which humans are capable of doing to
within 20% of true length, but requires retraining as frequent as once every 6
months.
* Implementation Details :noexport:
** Preamble :ignore:
The following chapter contains material from a previously published work.
** System Description
The precise components that constitute the overall ``Fishsense'' system have
changed drastically over the past few years. The contention comes from the fact
that there are two main components to the system. The first component is all the
hardware -- the physical camera, corrective optic, laser mount, and laser
pointer. The second is the software pipeline that is used to process the data.
Both are required to accomplish the goal of gathering fish length data.
*** Usage Guide
Prior to deploying this device in the field, calibration procedures must be
performed to maximize the accuracy of the obtained measurements.
**** Offline Procedures
***** Camera Calibration
The internal parameters of the camera must be calibrated in water. These
parameters include the camera's calibration matrix $K$, as well as lens
distortion coefficients $d$. This is currently achieved using Zhang's method
\nbsp\cite{Zhang2000}, which is to take many images of a checkerboard pattern, and
optimize the aforementioned parameters such that the checkerboard corners
reflect the structure of the checkerboard (lines remain straight).

This is performed in a swimming pool prior to any field deployments, and only
needs to be performed once in the device's lifetime.

***** Laser Calibration
The precise position and orientation of the laser beam must also be estimated.
Currently we do this by using a flat object with known features -- a dive slate
with duct tape on it, in our case. Multiple images of this flat object must be
taken with the laser dot on it in order to estimate the laser beam's parameters.

The procedure to obtain these parameters is described in [[sec:laser-calibration]].
**** Online Procedures
When in use, the diver points the laser pointer towards a fish and takes a
picture. The laser dot is generally in parallel with the camera, so it acts a
guide for the diver to ensure the image will meet our requirements, even in
cases where the viewfinder is too dim to see the image being taken.
**** Post Processing
After a dive or a series of dives, the diver can then upload the data to an
automated software pipeline that calculates the fish length. The mechanism of
transfer is still a work in progress -- currently it exists as a series of
Python scripts that must be run sequentially, though current work is being done
to both transfer it to a Rust codebase for speed, and construct a web
application in order for this software pipeline to become more easily
accessible.
** Hardware
*** Overview :ignore:
The device used to take the fish images must fit the following assumptions:
1. The system must have an imager that images can be extracted from.
2. The imager must be able to be modeled using the pinhole camera model.
3. The laser beam must be mounted rigidly to the imager.
4. The laser dot must be visible within the image for all desired ranges.

One example of our implementation of the Fishsense Lite unit is shown in Figure\nbsp\ref{fig:fsl}.

We currently have seven Fishsense Lite camera units, codenamed FSL-01 through
FSL-07. These are all comprised of exactly the same hardware components listed
below.
**** Annotations :noexport:
resolution requirements
rectified image assumption
currently experiements with backscatter demonstrate a decrease in error.
you may want to rerun the checkerboard multiple measurement experiment
*** Laser Mount
To secure the laser rigidly to the camera, we have designed a 3D-printed mount
made of polylactic acid. Figure\nbsp\ref{fig:laser-mount} shows a render of this
mount. Currently, the mount contains three screws: two to secure the laser, and
one to secure the base to the cold shoe on the camera's outer casing.

#+ATTR_LATEX: :scale 0.08
#+ATTR_ORG: :width 200px
#+CAPTION: A CAD render of an early version of the Fishsense laser mount.
#+label: fig:laser-mount
[[file:images/laser_mount_render.png]]

These mounts are not water resistant, and break with prolonged use. More
information is included in Section\nbsp[[sec:mount-failure]]. Current work is being done
to design and manufacture a mount made out of other materials that are able to
better withstand saltwater and remain rigid.
**** Annotations :noexport:
Ask Adrian if it's ok to include his work here.
*** Laser Pointer
The bulk of our experiments have been with two main laser pointer models: the
Innovative Scuba Concepts laser
pointer\footnote{https://dealer.innovativescuba.com/tc-101-aluminum-underwater-laser-pointer.html},
and the Shark
Laser\footnote{https://waterprooflaser.com/contents/en-us/d22\_TO-ORDER.html}.
Both lasers are rated for less than 5mW of power. The former model emits light
at 700nm, and the latter at 532nm. Section\nbsp[[sec:laser-comparison]] discusses
factors where one model may perform better than another.

** Laser Triangulation
*** Overview :ignore:
Knowing the laser's exact position and orientation is paramount to keeping
measured lengths as accurate as possible. However, due to manufacturing defects
or perturbation during transit, the position of the laser will shift by small
amounts both after the first initial measurement, as well as between dives.

Here we cover the forward problem - calculating laser dot locations using a
known laser position and orientation, and the inverse problem - calculating the
laser position and orientation based on known laser dot locations.
*** Quantity Definitions
We define the following quantities:
 - $\ell \in \mathbb{R}^3$: the location of the laser origin. To further constrain the problem ,we fix $\ell_z = 0$.
 - $\alpha \in \mathbb{R}^3$: a unit vector that determines the direction of the laser from the laser origin.
 - $p \in \mathbb{R}^3$: the 3D point at which the laser beam intersects with the fish.
 - $\mathfrak{p} \in \mathbb{R}^2$: the image coordinates of the laser in pixels, assuming the center of the image is $(0,0)$.
 - $f \in \mathbb{R}$: the focal length of the camera.
 - $w \in \mathbb{R}$: the distance between the center of two pixels on the camera sensor, otherwise known as pixel pitch.
 - $\mathfrak{p}_p \in \mathbb{R}^3$: the principal point. This is the true ``center'' of the imager found using camera calibration.
 - $o \in \mathbb{R}^3$: the origin, which we define using the principal point. Specifically, the origin will be located at $\mathfrak{p}_p + \begin{bmatrix}0 & 0 & f\end{bmatrix}^T$.

Figures\nbsp\ref{fig:diag} and \ref{fig:image_sensor} are used to illustrate these.

#+CAPTION: Close-up view of the image sensor. The distance between the plane and the origin is the focal length. $\mathfrak{p}$ is the image coordinate of the laser dot.
#+LABEL: fig:image_sensor
#+begin_figure
    \centering
    \begin{tikzpicture}
    [cube/.style={very thick,black},
			grid/.style={very thin,gray},
			axis/.style={->,black,thick}];

    \filldraw[
        draw=black,%
        fill=black!20,%
    ]          (1,0.75,5)
            -- (1,-0.75,5)
            -- (-1,-0.75,5)
            -- (-1,0.75,5)
            -- cycle;


    \draw[axis] (0,0,0)--(0.5,0,0) node[anchor=west]{$x$};
    \draw[axis] (0,0,0)--(0,-0.5,0) node[anchor=west]{$y$};
    \draw[axis] (0,0,0)--(0,0,-0.5) node[anchor=west]{$z$};

    \draw[black,thin] (0,0,0)-- node[above]{$f$} ++(0,0,5) ;
    \draw[black,dashed] (0,0,0) -- (0.25,0.1*5/6, 5) node[anchor=west]{$\mathfrak{p}$};

    \end{tikzpicture}
#+end_figure

*** Forward Problem (Point Triangulation)
Here we assume that laser beam origin $\alpha$ and laser beam orientation $\ell$ are
known. $\mathfrak{p}$ is also known from the captured image. Our goal is to use
this information to calculate $p$.

Firstly, we can calculate the vector in the camera's coordinate frame of the
location of the laser dot on the image sensor, which we call $p_s$:

\[
p_s = \begin{pmatrix}-\mathfrak{p}_xw \\ -\mathfrak{p}_yw \\ -f\end{pmatrix}
\]

We can then get the unit vector that points from the origin toward the laser
dot, which we call $v$:

\[
v = -\frac{p_s}{\lVert p_s \rVert}
\]

An observation that we leverage is that the laser beam, parameterized by $\ell +
\lambda_1\alpha$, and $\lambda_2v$, must intersect at $p$. This gives us the equation

\begin{equation}
\ell + \lambda_1\alpha = \lambda_2v
\end{equation}

We can restructure this into a matrix multiplication where $\lambda_1$ and $\lambda_2$ are
being transformed:

\begin{equation}
\begin{bmatrix}
\alpha_x & -v_x \\
\alpha_y & -v_y \\
\alpha_z & -v_z
\end{bmatrix}
\begin{bmatrix} \lambda_1 \\ \lambda_2 \end{bmatrix} =
\begin{bmatrix} -\ell_x \\ -\ell_y \\ 0 \end{bmatrix}
\end{equation}

This is an overconstrained problem for which, in general, no solution for $\lambda_1$
and $\lambda_2$ exists. We can, however, pose this as a least squares problem where we
minimize the norm of the difference between the left and right hand sides:

\begin{equation}
\text{argmin}_{\lambda_1,\lambda_2} \left\lVert\begin{bmatrix}
        \alpha_x & -v_x \\
        \alpha_y & -v_y \\
        \alpha_z & -v_z
    \end{bmatrix}\begin{bmatrix}
        \lambda_1 \\ \lambda_2
    \end{bmatrix} - \begin{bmatrix}
        -\ell_x \\ -\ell_y \\ 0
    \end{bmatrix}\right\rVert_2,
\end{equation}

Note that only one of the two scale parameters needs to be known in order for us
to get the laser point. We choose $\lambda_2$, for which the following closed-form
solution exists:

\begin{equation}
\lambda_2 = \frac{-\alpha^T \ell \alpha^T v + v^T \ell}{1 - (\alpha^T v)^2}
\end{equation}

We then trivially obtain $p$ from $p = \lambda_2v$.
*** Inverse Problem (Calibration)
Here we assume that $\alpha$ and $\ell$ are unknown, but
** Software
*** Overview :ignore:
In order to obtain length measurements from the captured images, we use a custom
software pipeline to extract the necessary features to measure length. Algorithm
\ref{alg:pipeline} shows a summary of the pipeline.

This algorithm is currently written in Python and run manually, though in future
we plan to move this to a Rust codebase.

\begin{algorithm}
\caption{Image processing pipeline.}
\label{alg:pipeline}
\begin{algorithmic}
\Require Image $I$, camera matrix $K$, laser parameters $\alpha$ and $\ell$
\State $I_R \gets \text{rectify}(I, K)$
\If{$\text{not laserInImage}(I_R)$} \\
    Reject image.
\EndIf
\State $_{}P_{\text{laser} }\gets \text{getLaserDot}(I_R)$
\If{$\text{not fishInImage}(I_R)$} \\
    Reject image.
\EndIf
\State $P_{\text{head}},P_{\text{tail}} \gets \text{getHeadTail}(I_R)$
\State $L \gets \text{getFishLength}(I_R, P_{\text{head}}, P_{\text{tail}}, P_{\text{laser}})$ \\
\Return $L$
\end{algorithmic}
\end{algorithm}
*** Image Rectification
All calculations relating to image projection rely on pinhole camera model
assumptions. In order to ensure that images fit this assumption, we rely on the
camera matrix $K$ obtained in the calibration process, as well as lens
distortion parameters. This is currently accomplished using OpenCV's standard
camera model, which contains a calibration camera matrix and a six-degree
polynomial to account for lens distortions.
*** Raw Processing
Figure\nbsp\ref{fig:jpg-vs-png} shows a raw image compared with a JPEG from the same
camera -- we can observe that the bottom of the pool in the raw image is slightly
curved whereas it is straight in the JPEG, implying that the camera does its own
distortion correction internally. To ensure that we have full control over the
entire image processing pipeline, we begin our operation on purely the raw
images. This also gives us control over color correction, which is advantageous
for us, as we can color correct the image in such a way that makes the laser
easier to detect.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\textwidth]{images/camera-jpg}
  \caption{Example JPEG image processed by the camera's internal software. }
  \label{fig:jpg-example}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\textwidth]{images/camera-png-unrectified}
  \caption{The same image }
  \label{fig:png-example}
\end{subfigure}
\caption{Comparison of the same image with onboard camera processing versus our custom processing.}
\label{fig:jpg-vs-png}
\end{figure}
**** Annotations :noexport:
Ask Hamish about whether it's ok to include his work in here and credit him
*** Laser Calibration

*** Automatic Laser Detection
<<sec:laser-detection>>
We chose to use a neural network to find the location of the laser, as over the
course of many pool tests and deployments, we gathered a large enough number of
hand-labeled laser dot images to be able to train such a model.

We chose a binary classification model which classified 20x20 tiles in a given
image as either containing the laser or not. Training data was generated on 326
images and testing data on 24 unique images. 225 tiles with lasers at varying
points within each tile were created per image, resulting in 73350 tiles. 439300
tiles without the laser were also created by extracting tiles from images in
steps of 85 pixels horizontally and vertically, while avoiding areas of said
images that contained a laser dot. Our network consists of a single
convolutional layer, a single average pooling layer, and two linear layers,
trained for 469 epochs. We are able to achieve 99\% accuracy on test data.

To reduce the number of potential tiles, we looked for all the local maximums
within an image, then select a 20x20 region around it. Due to the limited amount
of training images given to the model, many non-laser tiles were classified as
laser. However, we are able to constrain possible laser locations to a line in
the image using a mask derived from the laser parameters. The point closest to
the vanishing point was determined to be the laser.

Performance metrics of the model are shown in
Table\nbsp\ref{tab:laser-model-metrics}. Because most training data was taken from
pool images, the model trained on pool data performed very well. The one false
positive detected in the pool data was another point on the fish, so length
detection would work just fine. The ocean data had many false negatives, which
was deliberate - it is much better for our model to have high precision than it
is to have high recall because a falsely detected laser generates a wildly
incorrect fish length. On the other hand, because we are taking multiple images
of each fish, if a few of those images have lasers that are not detected, these
can simply be discarded.

#+ATTR_LATEX:  :align |c||c|c|c| :width 70%
#+CAPTION: Performance metrics for our laser detection model.
#+LABEL: tab:laser-model-metrics
|---------------+-------------+----------+-------|
| *Data Source* | *Precision* | *Recall* |  *F1* |
|---------------+-------------+----------+-------|
|---------------+-------------+----------+-------|
| Pool          |       0.996 |    0.981 | 0.989 |
|---------------+-------------+----------+-------|
| Ocean         |       0.986 |    0.861 | 0.919 |
|---------------+-------------+----------+-------|


**** Annotations :noexport:
Ask Viva for permission to include his work in here and make sure to credit him
*** Fish Segmentation
To locate the fish in the image, we use an existing technology called
Fishial\nbsp\cite{Fishial2023}, which uses a convolutional neural network to perform
segmentation of fishes within the frame. From this segmentation we get the
outline of the fish. We then employ the "line of best symmetry" to find the head
and tail points.

To find the line of symmetry of the fish, we employ Principal Component Analysis
(PCA) on the extracted contours. PCA is a dimensionality reduction technique
that works by identifying the directions in which the data exhibits the most
variance. This is particularly valuable for our purposes, as in most cases, the
axis of greatest variance corresponds closely to the symmetry axis of the fish,
which, in turn, aligns with the axis along which we intend to measure the fish
length. An illustration of this process is depicted in
Figure\nbsp\ref{fig:fishsymmetry}, showcasing the contours detected by Fishial on a
fish image.

\begin{figure}
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{images/fish_symmetryline}
        \caption{Fish with more typical body plan.}
        \label{fig:fishsymmetry}
    \end{subfigure}%
    \qquad
    \begin{subfigure}{.5\textwidth}
        \includegraphics[width=0.9\textwidth]{images/asymmetricalfish}
        \caption{Fish with large offset features (\textit{Mola mola}).}
        \label{fig:asymmetrical_fish}
    \end{subfigure}
    \caption{Example fish with detected lines of symmetry.}
\end{figure}

By applying PCA to these contours, we discern the principal axis that captures
the maximal variance. This axis, which aligns closely with the desired
measurement axis, provides a robust reference for accurately measuring the
fish's length. Following the identification of the symmetry axis, we intersect
this axis with the contours of the fish. and label the intersection of the two
points with two dots to show the head and tail location of the fish.
**** Annotations :noexport:
Ask Kyle Tran about whether it's ok to include his work in here and credit him

** Acknowledgements
The author would like to thank Christopher Crutchfield, Ana Perez, Hamish Grant,
Viva Suresh, and Kyle Tran for contributions described in this section.
