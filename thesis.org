#+OPTIONS: toc:nil H:5
#+SETUPFILE: setup.org
#+TITLE: Methods for In-Situ Fish Length Measurement with a Single Laser and Camera to Support Citizen Science
#+AUTHOR: Kyle Sirong Hu
#+LATEX_HEADER: \degree{Electrical Engineering (Intelligent Systems, Robotics and Control)}{Master of Science}
#+LATEX_HEADER: \degreeyear{2024}
#+LATEX_HEADER: \chair{Professor Curt Schurgers}
#+LATEX_HEADER: \cochair{Professor Ryan Kastner}
#+LATEX_HEADER: \committee{Professor Nikolay Atanasov}

* Frontmatter :ignore:
#+LATEX: \frontmatter
#+LATEX: \makecopyright
#+LATEX: \makesignature
#+BEGIN_dedication
To Kat, my friends, and my family.
#+END_dedication
#+BEGIN_epigraph
#+ATTR_LATEX: :latexcode \vskip0.5pt plus.5fil \setsinglespacing
#+BEGIN_VERSE
One fish, Two fish, Red fish, Blue fish,
Black fish, Blue fish, Old fish, New fish.
This one has a little car.
This one has a little star.
Say! What a lot of fish there are.
#+END_VERSE
#+ATTR_LATEX: :latexcode \vskip\baselineskip
/Dr. Seuss/
#+END_epigraph
#+LATEX: \tableofcontents
#+LATEX: \listoffigures
#+LATEX: \listoftables
#+BEGIN_acknowledgements
This project would not have been possible without the many hands available to
help us throughout these past couple of years.

Thank you to the members of my committee: Curt Schurgers, for his invaluable
guidance on writing this document, Ryan Kastner, for guiding us through the
bureaucracies of research funding, and Nikolay Atanasov, for helping me smooth
out some of the math-heavy portions of this document.

Thank you to Christopher Crutchfield, who has dealt with far more logistical
and managerial issues than a PhD student should ever have to deal with. Thank
you to Nathan Hui, whose stern engineering and management advice was vital to us
making it this far. Thank you to all undergraduates who have worked with the
FishSense project: Vivaswat Suresh, Raghav Maddukuri, Avik Ghosh, Allen Kan, Ana
Perez, Hamish Grant, Harish Vasanth, Shaurya Raswan, Kyle Tran, Sam Prestrelski,
Adrian Zugehar, and Jennifer Xu. Thank you also to the REU students who helped us gather
valuable data despite unreasonably short notice: Josie Dominguez, Jordan
Reichhart, Ela Lucas, and Nick Reyes.

Thank you to Professor Brice Semmens, Jack Elstner, Alli Candelmo, Jen Loch and
the members of REEF for testing this system and providing us with the initial
problem we aim to solve. An additional thank you to Patrick Paxson and Peter
Tueller, whose work laid the foundation for our current research.

Most importantly, thank you to Fred the fake fish, who we dearly miss. May he
enjoy his freedom in the Pacific Ocean.

I would also like to thank the people I've met during my time at UCSD, who have
made this journey worth it.

Thank you to Kristina ``Kat'' Diep, for always being encouraging and supportive
during my time in graduate school. Your humor and light-heartedness has kept me
going.

Thank you to Matei Gardus, who taught me how to use emacs org-mode several years
ago, and is what this document is written with.

Thank you to the members of UCSD Quizbowl, in particular to Alistair Gray, who
is a dear friend, and to Praveen Nair, who asked me to include the Dr. Seuss
quote in the epigraph.

Thank you the "ibob" crew, who have fostered my love for technology and with who
I have created a thriving community of people with similar interests through
UCSD's ACM chapter.

Thank you to Daniel Truong, as well as both the San Diego Smash and Fighting
Game communities, for allowing me to grow emotionally that much more.

Finally, thank you to my parents, whose continuous support has allowed the
opportunity to do all of this.
#+END_acknowledgements

* Abstract                                                           :ignore:
#+BEGIN_dissertationabstract
Ecologists are interested in studying fish length distributions as a metric for
the health of fish populations, both for fishery management and to gauge the
effect of policies and worsening ocean conditions in a small area. Current
methods of gathering these data are time intensive, require expensive equipment
and/or training. Approaching this issue with citizen science -- utilizing
efforts from the general public -- allows us to gather much more data about fish
populations that would currently be costly to acquire. We are working towards
developing a device and software platform that allows recreational divers to
take pictures of fish, and have the images processed such that length and
species information are determined automatically.

This work focuses on the mechanism under which fish length is calculated. The
device incorporates a standard laser pointer with a consumer waterproof camera,
and uses the location of the laser dot in the object to determine the distance
between the object and the camera. We also discuss methods to ensure that the
parameters of the laser with respect to the camera are known, as well as how we
can ensure that these procedures can be performed in the field. We test these by
conducting trial calibrations and length measurements in a controlled setting,
and evaluating where sources of error come from. From our testing we are able to
verify that our technology is capable of obtaining measurements within 20% of of
true length, similar to the error margins of visual estimation.

#+END_dissertationabstract
#+LATEX: \mainmatter
** Annotation                                                      :noexport:
Verify the requirements for including material from your own paper
* Introduction
** Preamble :ignore:
Marine ecosystems provide humankind many resources which we depend on. Marine
agriculture is an important source of food worldwide, and approximately 20% of
the global population primarily depend on it. Data that can be used to evaluate
the health of our oceans is therefore essential for us to understand the
potential impacts of overfishing and climate change. In order to be able to
quantify the effects of climate change on fish, we require some metric with
which we can study how healthy fish are on a population level.

One such way of doing this is to obtain a fish length distribution. By examining
such distributions over time, scientists can identify whether a population is
being overfished, or is recovering. A 2012 study looked at the effects of
fishing policies on a particular population of grouper, and used an annual fish
length distribution as the primary method of studying their
recovery\nbsp\cite{Heppell2012}. Fish length can also be used to convert to mass,
from which the biomass of a population can be calculated.

** Current Methods for Fish Length Studies
Current global fish length data often come from catch and release programs. In
California, the California Collaborative Fisheries Research Program (CCFRP) is
one such program, which as of 2024, consists of over 1800 volunteer anglers.
These anglers use hook-and-line procedures to catch the fish, from which length
is measured directly with a tape measure before the fish is released. This
length information is combined with metrics like catch per unit effort to
estimate the biomass of a population. However, fishing takes time - in their
most recent study, each examined location took 3 days to gather data
for\nbsp\cite{Ziegler2024}. In addition, stress caused by barotrauma and air
exposure can cause fish to be less able to find food or evade predators, which
reduces their life expectancy\nbsp\cite{Campbell2010}. In summary, these expeditions
incur a lot of overhead for relatively little gain, may be dangerous for the
anglers\nbsp\cite{Rohner2011}, and also the fish being
studied\nbsp\cite{Ramsay2009,Campbell2010}. Thus a low effort, non-invasive approach
is desirable.

This approach also poses an issue for marine protected areas (MPAs), within
which fishing may be restricted to certain species, or completely illegal
outright. To study populations within these areas, adjacent fishing activities
are sometimes used to infer the abundance of a particular
species\nbsp\cite{Ziegler2024}.

A non-invasive alternative is the roving diver survey, where a team of divers
conducts a visual census of fish species and their lengths within the MPA.
Sometimes divers may use a T-bar as a length reference. However, measurements
obtained purely from visual estimation are naturally imprecise -- humans are
only capable of estimating to within 20% of a fish's true length
\cite{Harvey2001}. This is also a skill that must be honed in order to be
improved, and requires retraining as frequent as once every 6 months
\cite{Bell1985}.

The effort described within this work relates to our solution to the above
problem -- a device that we call FishSense Lite. FishSense Lite relies on a
single laser as depth reference in order to measure the length of a fish from a
captured image, and due to the simplicity and accessibility of the hardware
construction, it can be utilized by recreational divers to get much larger data
coverage.

Images taken by this device will consist of a fish with a laser dot, as shown in
Figure\nbsp\ref{fig:example-fish-pic}. One can imagine that as the fish gets further
away from the camera, the apparent size of the fish decreases, and the position
of the laser dot on the image changes. We leverage both of these observations to
obtain the length of the fish, as described in Chapter\nbsp[[sec:algorithms]].

** Related Work
<<sec:existing-technologies>>

A technological approach to measuring an objects size involves first capturing
an image of the scene, then using distances to objects to scale their apparent
sizes appropriately. Herein we discuss existing techniques using lasers, stereo
video measurements, and acoustic methods.

A summary of these underwater ranging methods can be seen in
Table\nbsp\ref{tab:comparison}, including FishSense Lite.

#+ATTR_LATEX: :align |c||c|c|c|c| :width 70%
#+CAPTION: Comparison of different underwater ranging techniques.
#+LABEL: tab:comparison
|--------------------+-------------------------+---------------------------------+--------------+--------------------------|
| Technique          | Est. Cost (USD)         | Max. Relative Error             | Ease of Use  | Range                    |
|--------------------+-------------------------+---------------------------------+--------------+--------------------------|
|--------------------+-------------------------+---------------------------------+--------------+--------------------------|
| Stereo Video       | 4600\nbsp\cite{SeaGIS}      | 2.5%\nbsp\cite{Harvey2001}          | Intermediate | 2-10m\nbsp\cite{Mallet2014}  |
|--------------------+-------------------------+---------------------------------+--------------+--------------------------|
| Laser Caliper      | 600\nbsp\cite{BERGERON2007} | 12%\nbsp\cite{Stock2021}            | Intermediate | 2-5m\nbsp\cite{Stock2021}    |
|--------------------+-------------------------+---------------------------------+--------------+--------------------------|
| Acoustic Methods   | 20k\nbsp\cite{Mueller2006}  | 1.1% - 35.2%\nbsp\cite{Mueller2006} | Hard         | 1-16m\nbsp\cite{Mueller2006} |
|--------------------+-------------------------+---------------------------------+--------------+--------------------------|
| Laser Rangefinding | 1200                    | 15%                             | Easy         | 2-5m                     |
|--------------------+-------------------------+---------------------------------+--------------+--------------------------|

*** Stereo Camera Systems
A standard method for collecting fish length measurements uses stereo video
technology\nbsp\cite{Mallet2014}. To measure the actual size of objects in an image,
an additional camera can be added to create a stereo camera setup - provided the
relationship between the two cameras is known, apparent size can be converted to
actual size by determining the distance of the fish from the cameras. These are
typically diver-operated (known as stereo diver-operated video or
stereo-DOV)\nbsp\cite{Goetze2019} or placed in baited remote underwater video
systems\nbsp\cite{Mallet2014}. While stereo-DOV is a more cost-effective solution
than deploying a remote system, the current state of the art still requires
purchasing proprietary hardware and software\nbsp\cite{Goetze2019}, which can be
prohibitively expensive for a citizen scientist at a minimum of
$4,600USD\nbsp\cite{SeaGIS} for a scientist grade stereo video system. In addition,
stereo video generates a large amount of data that requires significant effort
to store and process\nbsp\cite{Tueller2021}.

Commercial stereo video solutions include the AQ1 AM100\nbsp\cite{Shafait2017} and
the AKVA Vicass HD\nbsp\cite{Churnside2012}, intended for use in aquaculture. Such
systems are also costly and require a tether to a surface-side computer with
proprietary software that must be used to manage the system. This limits the
regions of the world where the data can be collected as it requires scientists
to interact with the system. The tether also limits the depths at which the data
can be collected.

A previous FishSense "Pro" device has been attempted using proprietary stereo
camera units\nbsp\cite{Paxson2022, Tueller2021}. These attempts involved using an
Intel Realsense D455 -- a depth camera that used both stereo cameras and
structured light in order to infer depth information. This camera was paired
with a NVIDIA Jetson TX2 for onboard video processing, and was fully contained
in a waterproof enclosure with onboard power and storage. Since the Realsense
was up against a flat acrylic port, distortions from Snell's law caused both the
incoming images and the structured infrared light to refract, which yielded
erroneous depth map information. On top of this, some of the infrared light
reflected back into the lens, adding additional color artefacts in the images
produced, rendering species identification impossible. Traditional camera
calibration procedures are also difficult to do underwater, if not
impossible\nbsp\cite{Wong2022}. We concluded that until we are able to model Snell's
law distortions, an approach using proprietary stereo camera approach would be
infeasible.

*** Laser Calipers
Another solution for ranging uses laser calipers -- two parallel lasers placed a
known distance away from each other. When calibrated correctly, the distance
between the two laser dots can be used as a reference length to measure the
entire fish\nbsp\cite{Rohner2011, Heppell2012}. For these measurements to be
accurate, both lasers must be perfectly parallel with each other and the camera
axis. Depending on manufacturing tolerances, such a requirement may mean that
lasers must be carefully selected. These systems are calibrated by measuring the
distance between the two laser dots at a large distance before a
dive\nbsp\cite{Heppell2012}. Lengths are then calculated using the known distance
between two points and the projection of the fish onto the camera. While length
estimation becomes simple using this method, the cost of two lasers and time for
the minute readjustments waste valuable resources for researchers. The dual
laser mechanism also requires that the object is larger than the distance of the
two lasers, meaning fish smaller than the offset cannot be measured.

This system has been used to document the recovery of a population of
Nassau Grouper over 7 years, in order to quantify the effectiveness of recovery
efforts\nbsp\cite{Heppell2012}. While such a system was also demonstrated to be more
effective than visual estimation, it also requires manufacturing a
custom-machined aluminum mount, which is not easily accessible for most divers.
Calibration of the system also involves verifying that the beams are the same
distance apart up to 15m from the source, which can be challenging to do in a
field setting.

*** Acoustic Methods
The primary related technology in this category is sonar, which involves
actively sending sound waves and using the signal as it echoes back to construct
an image of the scene. As we are interested in studying particular fish
populations, it is also important that we are able to identify the species of
fish being imaged, a relatively recent development when it comes to acoustic
methods. Technologies like Dual-frequency Identification Sonar
(DIDSON)\nbsp\cite{Belcher2002} and Adaptive Resolution Imaging Sonar
(ARIS)\nbsp\cite{Jones2021} both take advantage of many individual sonic beams and
high frequencies in order to construct a high-fidelity sonic image of the scene.
Both DIDSON and ARIS technologies have been used to measure fish
lengths\nbsp\cite{Burwen2010,Cook2019} and identify fish
species\nbsp\cite{Langkau2012,Jones2021}. Sonar is far more effective than
camera-based techniques when visibility conditions are poor. However, they are
by far the most expensive option listed here. A system in 2006 was said to have
cost around 20,000USD\nbsp\cite{Mueller2006}. We were unable to find specific
prices for modern imaging sonar units, though prices have ranged from tens of
thousands to even hundreds of thousands of US dollars.

Passive techniques -- monitoring sounds that come from the fish themselves --
have also more recently been used as a way to study fish
populations\nbsp\cite{Fornshell2013}. These have shown to be an effective way to
study species distribution\nbsp\cite{VanHoeck2021}, or for recording sightings of a
targeted species\nbsp\cite{Bolgan2023}, though we are unaware of any existing
literature that attempts to estimate size using this technology.

*** Laser Rangefinding
Our system uses only a single laser to measure distance, which removes the need
to calibrate two lasers simultaneously and keep them in parallel. This technique
is similar to a light projection-based triangulation rangefinder
system\nbsp\cite{Parthasarathy1982}, as it uses spatial information about the laser
dot to determine the depth of the subject. This method can be extremely accurate
with the right combination of laser and image sensor - up to 10
micrometers\nbsp\cite{Ebrahim2015, Cavedo2016}. Such sensors have been experimented
with as a low-cost solution for robot localization\nbsp\cite{Nguyen1995}, quality
assurance in manufacturing\nbsp\cite{Cavedo2016}, and 3D scanning\nbsp\cite{Baba2001}.

Single laser range finding also has precedence for use in animal size
studies\nbsp\cite{Jaquet2006,Monkman2019,Breuer2007}. The primary benefit of this
approach is that it is more inexpensive than other solutions and requires less
training to operate\nbsp\cite{Monkman2019}. Jaquet and Breuer et al. utilize a range
finder as a separate module from a regular digital camera\nbsp\cite{Jaquet2006,Breuer2007}.
Data from both modules must be combined and processed manually to
obtain lengths\nbsp\cite{Jaquet2006, Monkman2019}, which our platform improves upon.

** Outline
The rest of this work will be focused on the algorithms behind the length
extraction, specifically how the laser parameters are determined, how these are
use to calculate the coordinates of the laser dot in physical space, and how
that in turn is used to get the fish length. Chapter\nbsp[[sec:system-overview]] will
give a brief overview of FishSense Lite as a whole. Chapter [[sec:algorithms]] will
provide an in-depth description of the laser algorithms. Chapter [[sec:testing]]
will show how we have tested our system and where sources of error come from.
Finally, Chapter [[sec:conclusion]] will summarize the content of this work and
future work that needs to be done on the system.
* System Overview
<<sec:system-overview>>
** Preamble :ignore:
This chapter contains an overview of FishSense Lite. Our primary goals with this
system are to reduce the cost to gather fish length data, and to make the
practice more accessible to recreational divers. To accomplish this, we attach
an underwater camera (in our case, an Olympus TG6) to an off-the-shelf
underwater laser pointer. Both of these tools are commonly owned by recreational
divers, which is why we target this low cost hardware setup.
Figure\nbsp\ref{fig:fsl} shows an example of one such camera system.

#+ATTR_LATEX: :scale 0.2  :options angle=-90
#+ATTR_ORG: :width 200px
#+CAPTION: A fully assembled FishSense Lite hardware unit.
#+label: fig:fsl
[[file:images/fishsense-lite-system.jpg]]

Our system relies on the diver taking images with the laser beam pointed on the
fish. With just the image, as well as a priori knowledge of the laser beam
parameters, we can then use this to obtain a length of the fish within the
image. A long-term goal we have is that this data can be mapped to region, a
specific species and even a specific individual fish, which would allow us to
create a map of global fish population data.
** Hardware Setup
*** Overview :ignore:
This is the physical device that divers would use to take measurements. It is
intended to be composed of materials that many divers already own, which is why
we use off the shelf parts. The only new component that we include is a 3D
printed mount.
*** Camera
For our own testing, we use an Olympus TG6, a common camera for divers to own
for underwater photography. This camera itself is waterproof, though we use
another protective housing around it.

This is just one example of a camera that can be used for this system --
theoretically any camera would work.
*** Wide Angle Lens
<<sec:backscatter>>

With just the waterproof housing, Snell's law distorts images in such a way that
a relationship between lengths in the image and lengths in the real world are
difficult to obtain\nbsp\cite{Agrawal2012}. To mitigate the effects of this, we use
a corrective optic. Developed by Backscatter, this corrective optic is designed
to attach to the Olympus's underwater housing, and is typically used to widen
the field of view for underwater photographers. Future work is required for us
to understand how to model these Snell's law distortions better so we can
ultimately remove it. This allows us to use a simple camera model described in
Section\nbsp[[sec:pinhole]].

In order to verify that this model was sufficient for this
camera system, one metric we used was the mean squared error given by OpenCV's
=calibrateCamera= function. This value was significantly lower when calibrating
with the dome port on, and the images taken without the dome port showed a
significantly higher amount of distortion.

**** Notes :noexport:
include ana's charts here? that might be sufficient justification

*** Laser and Mount
The laser pointer is another off the shelf component. Many divers own laser
pointers in order to gesture to others, which further reduces the cost of the
overall hardware. This laser is mounted to the camera's housing with a
3D-printed polylactic acid (PLA) mount. Due to manufacturing defects and small
perturbations, that may occur in transit, the laser moves enough to cause
significant error in the measurements. See Section\nbsp[[sec:laser-calibration]] for how
this issue is solved.

The color of the laser also has an effect, discussed in Section\nbsp[[sec:laser-comparison]].
** Operation
*** Overview :ignore:
This section details how the device would be used in the field.
*** Calibration Procedures
**** Camera Calibration
<<sec:camera-calibration>>
In order to obtain parameters like the focal length and location of the
principal point, we must perform calibration of the camera. This is done using
Zhang's procedure\nbsp\cite{Zhang2000}, where many images of a checkerboard are
taken, and parameters are found such that known dimensions and regularity of the
checkerboard are satisfied.

Currently, this procedure must be done underwater. We do the described procedure
in a swimming pool, with the corrective optic attached. OpenCV's
=cameraCalibrate= function provides a straightforward way to obtain camera
intrinsics and lens distortion parameters. We have experimentally observed that
the camera parameters in salt water differ from the parameters in fresh water,
which is discussed in Section\nbsp[[sec:fw-vs-sw]].
**** Laser Calibration
Since our laser mount is not stable enough to guarantee that laser parameters
stay consistent between dives, we require a laser calibration at the beginning
of every dive. More detail about this procedure can be found in
Section\nbsp[[sec:slate-calibration]].

*** Diver Operation
In the field, after calibrating the laser, operating the camera is fairly
straightforward. The diver will take a picture of nearby fish as normal, with
the restriction that the laser dot must be trained on the fish. Note that we
expect the diver to ensure that two conditions are met:
1. The head and tail points of the fish are visible in the image.
2. The laser is both present and visible on the fish.
Excerpts from the field manual are provided in Appendix [[sec:user-manual]].
*** Data Offloading
After the data has been collected, the operator must offload the data to be
processed externally. We are currently in the process of developing software in
order to perform this processing, though details are out of scope of this
document.

** Software
Once the data has been collected, it must be processed to extract the lengths of
the captured fishes. A summary of the process is as follows: locations of both
the fish and the laser dot in the image are identified using machine learning
techniques. The laser's location in 3D space is determined using the location of
the laser dot in the image, along with prior knowledge of the laser's location
with respect to the camera. The fish segmentation mask is used to create a
polygon of the fish's outline, after which PCA is performed to identify the axis
of symmetry of the fish. The intersection of this line is used to find the head
and tail of the fish, and the depth obtained from the laser dot is used to
determine the locations of the head/tail points in 3D space. The distance
between these points is what is used to calculate the length of the fish. We
currently have plans to create a web service that handles all of this
processing, and intent for citizen scientists to upload their data.

A flowchart of the processing pipeline is included in
Figure\nbsp\ref{fig:software-flowchart}.

#+ATTR_LATEX: :scale 0.6
#+ATTR_ORG: :width 200px
#+CAPTION: Flowchart of software processing pipeline.
#+label: fig:software-flowchart
[[file:images/software_flowchart.pdf]]

** Accuracy Bounds
The primary method that our system aims to improve upon is human-made visual
estimates. While humans can collectively estimate the mean length of a
population quite accurately, the precision with which this is done can vary
wildly between divers\nbsp\cite{Harvey2002}. Within the study conducted by Harvey
et. al., we see that divers can be incorrect about the length of a fish by up to
20%. As our device is meant to be a supplement for a similar experiment setup,
we aim to perform as well or even better than this.

** Conclusion :ignore:
\\
Thus far we have described the system setup. The next chapter will describe the
main contribution of this work -- the algorithms underlying distane estimation
that utilize the laser.
* Algorithm Details
<<sec:algorithms>>
** Preamble :ignore:
The specific contribution detailed in this work are the algorithms relating to
the laser pointer -- both how the parameters of the laser with respect to the
camera are determined, and, given those parameters are known, how the location
of the laser dot can be found in the image.

** Pinhole Camera Model
<<sec:pinhole>>
All of the calculations shown assume that our camera follows the pinhole camera
assumption. In air, this assumption is extremely common and is used in stereo
cameras for object triangulation.

Figure\nbsp\ref{}
The pinhole camera model assumes that all light that comes into the camera
passes through a single point known as the /focal point/, or /optical center/, of
the camera. All incoming rays fall onto the image sensor, which is a plane
perpendicular to the camera's axis, at a fixed distance from the focal point
known as the /focal length/. The image projected onto the sensor is a flipped
version of the real life scene, as the light rays are inverted after passing
through the optical center.

Instead of looking at the image on the image sensor, it is typically the
convention to mirror the image sensor plane in front of the optical sensor, such
that the distance from the focal point to this new plane is also the focal
length. This plane is designated as the /image plane/. This convention allows us
to more easily translate from pixel coordinates to coordinates in the image
plane, as we need only know how large each pixel is to map to physical units
without needing to invert.

Underwater it is more difficult to make the assumption that imaging systems
follow the pinhole camera model, especially with our current hardware setup. Our
camera's transparent port separating the camera lens from the water is flat,
meaning that Snell's law causes incoming light to refract. Measures to mitigate
this problem are detailed in Section\nbsp[[sec:backscatter]].
** Quantities and Conventions
Our axis conventions in this chapter are as follows: the $x$ axis points to the
right in the image, the $y$ axis points downward in the image, and the $z$ axis
points forward, away from the camera. This system has the added bonus that the
$x$ and $y$ axes coincide with the directions of pixel coordinates. The origin
will be defined at the camera's optical center. This is commonly known as the
``optical frame''. We also designate a point $O$ to be the orgin of this
coordinate system.

Given a particular fish image, we assume that we are able to trivially determine
the size of the fish in pixels. Figure\nbsp\ref{fig:pinhole-fish} shows a diagram of both the live
fish and its projection onto the image plane.
#+ATTR_LATEX: :scale 0.4
#+ATTR_ORG: :width 200px
#+CAPTION: Pinhole diagram of the fish with it's image projection.
#+label: fig:pinhole-fish
[[file:images/pinhole-fish.png]]

Here $x$ represents the size of the fish in pixels, $w$ represents the ``pixel
pitch'' (distance between two pixels on the image sensor), and $f$ represents
the focal length of the camera. Note that the product $xw$ can be interpreted as
the size of the fish on the image plane. If the apparent size of the fish is
known, using similar triangles we are able to obtain the following relationship:
\begin{equation}
L = \frac{Dxw}{f} \label{eq:length-from-depth}
\end{equation}

Our only unknown on the right hand side is $D$, the distance of the fish from
the camera. Most of the challenge in this system comes from obtaining this
measurement.

Note that the above equation makes the following assumptions:
1. The fish is parallel with the image plane.
2. The laser dot is at the same depth as head and tail points on the fish.

In practice, neither of these are always, if ever, true, and the implications
of this are discussed more in Chapter [[sec:testing]].

We assume that we can describe precisely how the laser is positioned relative to
the camera. More precisely, we define parameters $\ell$ and $\alpha$ , where $\ell$ is the
3D vector from the origin to the laser beam in the $xy$ plane, and $\alpha$ is a 3D
vector of unit length that points in the direction of the laser beam. The laser
beam can therefore be described by a ray with scale factor $\lambda$:
\[
\vec{\ell p} = \ell + \lambda\alpha
\]

Note that our restriction that $\ell$ lies purely in the $xy$ plane means that the
$z$ component of $\ell$ is always zero.

We also assume that we are able to accurately find the laser dot in the image.
In practice, these are hand-labeled, but automatic algorithms are outside the
scope of this document.

Figure\nbsp\ref{fig:diag} illustrates all the aforementioned quantities in one place.

#+CAPTION: Diagram demonstrating known quantities. Laser shown in green, fish plane shown in red, image plane shown in gray.
#+LABEL: fig:diag
#+begin_figure
    \centering
    \begin{tikzpicture}[
        cube/.style={very thick,black},
			grid/.style={very thin,gray},
			axis/.style={->,black,thick}];

   \filldraw[
        draw=red,%
        fill=red!20,%
    ]          (1,1,-5)
            -- (1,-1,-5)
            -- (-1,-1,-5)
            -- (-1,1,-5)
            -- cycle;
    \filldraw[
        draw=black,%
        fill=black!20,%
    ]          (0.2,0.15,-1)
            -- (0.2,-0.15,-1)
            -- (-0.2,-0.15,-1)
            -- (-0.2,0.15,-1)
            -- cycle;

    \draw[green,thick]  (1.5,0.5,0) node[anchor=west,black]{$\alpha \mid \lVert\alpha\rVert = 1$} --(0.3,0.1,-5) node[anchor=west,black]{$p$};

    \draw[->, black,thin] (1.5,0.5,0)--(1.26732998, 0.42244333, -0.96945842) node[anchor=east]{$\ell$};
    \draw[black,dashed] (0.3,0.1,-5)--(0,0,0);
    \draw[axis] (0,0,0)--(1,0,0) node[anchor=west]{$x$};
    \draw[axis] (0,0,0)--(0,-1,0) node[anchor=west]{$y$};
    \draw[axis] (0,0,0)--(0,0,-1) node[anchor=west]{$z$};
    \end{tikzpicture}
#+end_figure

** Finding the Distance to the Fish
<<sec:distance-finding>>

We assume that this laser beam intersects the fish at an unknown point $p$. As
imaged by the camera, this laser dot will have known pixel coordinates
$\mathfrak{p}$. This corresponds to a point $p_\text{image}$ on the image plane,
defined as

\[
p_\text{image} = \begin{bmatrix}
\mathfrak{p}_x \\
\mathfrak{p}_y \\
f
\end{bmatrix}
\]

We arbitrarily decide to scale this vector to be of unit
length, defining
\[
v = \frac{p_\text{image}}{\lVert p_\text{image}\rVert}
\]

This creates two rays that converge toward $p$: the laser beam, and the ray from
the camera that passes through $\mathfrak{p}$ on the image plane. By projecting
both $\vec{\ell p}$ and $\vec{v}$ out until they intersect, we can identify where
$p$ is.

More specifically, we can identify parameters $\lambda_1$ and $\lambda_2$ such that
\[
\ell + \lambda_1\alpha = \lambda_2v
\]

We can refactor the above relationship to be the form

\begin{equation}
\begin{bmatrix}
\alpha & -v
\end{bmatrix}
\begin{bmatrix}
\lambda_1 \\
\lambda_2
\end{bmatrix}
= -\ell
\label{eq:find-laser}
\end{equation}

In practice, however, this relationship does not necessarily hold, because the
laser dot as observed in the image may be in a particular pixel that does not
match up perfectly with where $\lambda_2 \vec{v}$ intersects with the image plane.
This causes the rays to come very close to each other, but not touch, meaning
Equation\nbsp\ref{eq:find-laser} has no solution.


We can, however, obtain $\lambda_1$ and $\lambda_2$ values that minimize the difference
between the left and right hand sides -- this is the least squares solution,
i.e. the solution to the minimization problem
\[
\text{argmin}_{x} \lVert Ax - b \rVert^2,
\]

where $A \in \mathbb{R}^{m\times n}$, $m > n$, and $x,b\in \mathbb{R}^{n}$. In our case, we
define the following:
\begin{align*}
A &= \begin{bmatrix}
\alpha \ -v
\end{bmatrix} \\
x &= \begin{bmatrix}
\lambda_1 \\
\lambda_2
\end{bmatrix} \\
b &= -\ell
\end{align*}

In general, we can solve a least squares problem with the following formula:
\[
x = (A^TA)^{-1}A^T b
\]

Using this, we can obtain a closed form solution for both $\lambda_1$ and $\lambda_2$. We
only require one of them, so we use $\lambda_2$:
\[
 \lambda_2 = \frac{-\alpha^T \ell \alpha^T v + v^T \ell}{1 - (\alpha^T v)^2}
\]

Obtaining $\lambda_1$ or $\lambda_2$ allows us to obtain $p$, and the z-component of $p$,
along with Equation\nbsp\ref{eq:length-from-depth}, gives us the corresponding fish
length.

** Finding Laser Parameters
<<sec:laser-calibration>>
So far, we have assumed that the parameters of the laser $\alpha$ and $\ell$ are known.
These cannot be measured directly for two reasons:
1. Precise measurements relative to the optical center of the camera are hard to
   obtain, since in reality the location of the optical center of the camera is
   not known.
2. Since the laser mount is not perfectly stable, and can change in between
   dives and over time, the parameters need to be recalculated.
Thus, we must use a calibration procedure.

We have $n$ images from which we obtain laser points $p_i$. Here we describe two
possible algorithms to leverage this information to determine $\alpha$ and $\ell$. A
comparison of the two methods is presented in Section\nbsp[[sec:math-testing]].
**** Method 1: Gauss Newton Optimization
We leverage the fact that the parameterized laser beam must intersect with
the laser dot point to give us the following series of equations:
\begin{equation}
p_i = \lVert p_i - \ell \rVert\alpha + \ell
\label{eq:problem}
\end{equation}
The above equation states that the point $p_i$ should be able to be retrieved
from scaling the laser ray out by a factor determined by the difference between
$p_i$ and $\ell$.

We stack these points into a single vector, defining the following;
\[
\mathbf{p} = \begin{bmatrix} p_1 \\ p_2 \\ \vdots \\ p_n \end{bmatrix}
\]




First we define a parameter vector $x$ that contains all of our parameters:
\[
x = \begin{bmatrix}
\alpha \\
\ell_x \\
\ell_y
\end{bmatrix}
\]

We also define a function in terms of our parameters for the right hand side:
\begin{align*}
        g_i(x) &= \lVert p_i - \ell \rVert \alpha + \ell \\
        \mathbf{g}(x) &= \begin{bmatrix}
        g_1(x) \\
        g_2(x) \\
        \vdots \\
        g_n(x)
        \end{bmatrix}
\end{align*}

We then formulate this in terms of the following optimization problem:
\begin{align}
\text{argmin}_{x}\lVert \mathbf{r}(x)\rVert, \nonumber \\
\mathbf{r}(x) = \mathbf{p} - \mathbf{g}(x) \label{eq:residual}
\end{align}

We have a relationship that relates our known quantities $p_i$ and unknown
quantities $\alpha$ and $\ell$, though in this case the relationship is non-linear. We
must therefore choose a non-linear optimization method to find the best
candidates for $\alpha$ and $\ell$ that satisfy this.

The method we currently choose is the Gauss-Newton method, which
involves the following steps:
1. Find the Jacobian $J_r$ of the minimizing function w.r.t to $x$.
2. Take iterative steps of the following form:
\[
x^{(k+1)} = x^{(k)} - (J_r^TJ_r)^{-1}J_r \mathbf{r}(x^{(k)})
\]

According to Equation \ref{eq:residual}, only the second term depends on our
parameters, so we can rework our iterative step into
\[
x^{(k+1)} = x^{(k)} + (J_g^{T}J_g)^{-1}J_{g}\mathbf{g}(x^{(k)})
\]

Here we set $x^{(0)}$ to a rough estimate of where the laser is with respect to
the camera, taken with a ruler. For our particular test system, the laser's
starting point is assumed to be -4cm in the x direction and -11cm in the $y$
direction, with the laser parallel to the camera axis.

There are many other methods of this kind that we could have used, such as the
Levenberg-Marquardt algorithm, though from our experiments this calibration
method has been sufficient.

The Jacobian of $\mathbf{g}(x)$ is given by the following, and the full
derivation is detailed in Section\nbsp[[sec:jacobian-derivation]]:

\begin{align}
J_g &= \begin{bmatrix}
J_{g\alpha}^1 & J_{g\ell}^1 \\
J_{g\alpha}^2 & J_{g\ell}^2 \\
\vdots & \vdots \\
J_{g\alpha}^n & J_{g\ell}^n
\end{bmatrix} \in \mathbb{R}^{3n\times5}\nonumber \\
J_{g\alpha}^i &= \lVert p_i - \ell\rVert I \in \mathbb{R}^{3\times3}\\
J_{g\ell}^i &= \begin{bmatrix}
1 & 0 \\
0 & 1 \\
0 & 0
\end{bmatrix}
\left(I_{3\times3} - \alpha\frac{(p_i - \ell)^T}{\lVert p_i - \ell\rVert }\right) \in \mathbb{R}^{3\times2}
\end{align}

***** Derivation of Jacobian
<<sec:jacobian-derivation>>
Recall the function $\mathbf{g}(x)$:
\begin{equation*}
    \mathbf{g}(x) = \begin{bmatrix}
        \lVert p_1 - \ell\rVert\alpha + \ell \\
        \lVert p_2 - \ell\rVert\alpha + \ell \\
        \vdots \\
        \lVert p_n - \ell\rVert\alpha + \ell
   \end{bmatrix},
\end{equation*}
where $p_i$ is the laser dot in optical coordinates from calibration image $i$.

We can split $J_g$ into two block columns representing the Jacobians with respect to different vectors:
\begin{equation*}
    J_g = \begin{bmatrix}
        J_{g\alpha} & J_{g\ell}
    \end{bmatrix}
\end{equation*}
Calculating $J_{g\alpha}$ is fairly trivial, as the function is linear in $\alpha$:
\begin{equation*}
    J_{g\alpha} = \begin{bmatrix}
        \lVert p_1 - \ell\rVert I_{3\times3} \\
        \lVert p_2 - \ell\rVert I_{3\times3}\\
        \vdots \\
        \lVert p_n - \ell\rVert I_{3\times3}
    \end{bmatrix}
\end{equation*}

We can take the Jacobian with respect to $\ell$ by taking the Jacobians of each row individually. For a particular image $i$ we have:
\begin{align*}
    \mathbf{g}_i(\alpha,\ell) &= \lVert p_i - \ell \rVert \alpha + \ell \\
    \frac{\partial}{\partial\ell}\mathbf{g}_i(\alpha,\ell) &= -\frac{1}{\lVert p_i - \ell\rVert}\left[\begin{smallmatrix}
        (p_x - \ell_x)\alpha_x+1 & (p_y - \ell_y)\alpha_x & (p_z - \ell_z)\alpha_x \\(p_x - \ell_x)\alpha_y & (p_y - \ell_y)\alpha_y + 1 & (p_z - \ell_z)\alpha_y \\p_x - \ell_x)\alpha_z & (p_y - \ell_y)\alpha_z & (p_z - \ell_z)\alpha_z + 1
    \end{smallmatrix}\right] \\
        &= I - \frac{1}{\lVert p_i - \ell \rVert}\alpha(p_i - \ell)^T
\end{align*}

We only want the first two columns of this, so we get
\begin{equation*}
    J^i_{g\ell} =  \begin{bmatrix} I_{2\times 2} \\ 0\end{bmatrix}(I_{3\times3} - \frac{1}{\lVert p_i - \ell \rVert}\alpha(p_i - \ell)^T) \in \mathbb{R}^{3\times 2}
\end{equation*}

**** Method 2: Averaging
Once again assuming we have $n$ images with image $i$ corresponding with laser
dot point $p_i$, a straightforward method to get our laser trajectory $\alpha$ can be
obtained from a normed average over the differences between all points:
\[
        \alpha = \frac{1}{n(n-1)}\sum_{i=1}^n\sum_{j\neq i}\frac{p_i - p_j}{\lVert p_i - p_j \rVert}
\]

This can be flipped as necessary to face the direction in which the laser beam
travels. Once $\alpha$ is obtained, we can choose a point $p_k$ such that $p_k$ is on
our ray, and pick the intersection of the ray defined by $p_k$ and $\alpha$ with the
$xy$ plane to be $\ell$.

This method can be performed much faster than the former method, though while
developing our device we have stuck to using the Hu method since the constraints
and minimizing function are more explicit. Experiments comparing between the two
methods can be found in Section [[sec:math-testing]].
** Calibration Procedure
<<sec:slate-calibration>>

As mentioned in Section\nbsp[[sec:laser-calibration]], to calibrate the laser, we assume
that we know the optical frame coordinates of each laser dot $p_i$. While the
laser parameters $\alpha$ and $\ell$ are still unknown, the vector $v$ is always known,
since this only depends on $\mathfrak{p}$, determined solely from the camera
image. By intersecting $v$ with a known plane, we can precisely determine the
location of each $p_i$.

The same checkerboard pattern mentioned in Section\nbsp[[sec:camera-calibration]] is one
example plane that is ideal for this. The main reasoning for this method was
that there are two parts to this method for which OpenCV provides functionality
for - detecting corners of specifically the checkerboard pattern, and
calculating a 3D transformation given object points and corresponding points in
an image. However, in the field, requiring that divers carry a large and heavy
checkerboard for every dive is a large ask.

We have developed a procedure that does not utilize a checkerboard, and can
instead be done with a dive slate -- something that most divers will carry. An
example of one of these is shown in Figure\nbsp\ref{fig:slate-calibration}. We add
pieces of duct tape in an arbitrary pattern on one side to make the slate more
featureful. We also make the assumption that the slate remains close to parallel
with the image plane.

#+ATTR_LATEX: :scale 0.4
#+ATTR_ORG: :width 200px
#+CAPTION: An example of a slate calibration image.
#+label: fig:slate-calibration
[[file:images/slate-calibration.JPG]]

We assume that we have a scanned copy of the dive slate, and hence the physical
measurements of the duct tape pattern. Assuming that a correspondence can be
drawn between the corners of the scan representation and the corners within the
image, a transformation between the two can be found. This is known as a
Perspective-N-Point (PnP) problem, and OpenCV has a =solvePnP= function that
makes this relatively straightforward. The same problem exists using the
checkerboard, but OpenCV's =findChessboardCorners= function abstracts this away
from the developer by taking advantage of the specific structure of the
checkerboard (number of squares per side, size of a square).

Results demonstrating this method compared to the checkerboard are shown in
Section\nbsp[[sec:field-calibration-testing]].
** Conclusion :ignore:
\\
In this chapter, we have discussed the assumptions we make for this system to
work, and outlined the algorithm for obtaining laser parameters, and using these
to obtain the length of a fish. In the next chapter, we will discuss
experiements done to test the device.
* Experiments
<<sec:testing>>
So far we have mentioned several stages in the processing pipeline that need to be tested:
1. The laser calibration procedure from Section\nbsp[[sec:slate-calibration]].
2. The laser calibration algorithm from Section [[sec:laser-calibration]].
3. Distance measurement.
4. Object length measurement.

In this section we describe the testing that we have done so far to quantify how
well the system works. A total of 24 pool tests and 2 field deployments were
conducted in the process of developing FishSense Lite. A full list of pool tests
is included in Appendix\nbsp[[sec:pool-test-list]].
** Primary Methodology
In order to test the system, the vast majority of our testing involves
variations on the following procedure:
1. Calibrate the laser by taking pictures of one or both the planar objects mentioned in Section\nbsp[[sec:slate-calibration]].
2. Take images of objects with a known length.
3. Analyze how the estimated lengths compare with the true length of the object.

The experiments described in this section involve one of three measurement objects, described below.
*** Checkerboard
The checkerboard itself is useful for the same reason that makes it a good
object to calibrate the laser with -- it is relatively straightforward to
identify the corners of a checkerboard in an image. In our case, we use the
checkerboard as a convenient way to measure the orientation of the object when
we measure lengths from it, while also using it to get many different
measurements from a single image.

*** Box
Historically we have also used an aluminum box, with a 15cm section of tape
being used as the reference length that we measure. The box is shown in
Figure\nbsp\ref{fig:box}.
#+ATTR_LATEX: :scale 0.05
#+ATTR_ORG: :width 200px
#+CAPTION: The aluminum box. The top edge of the black tape across the surface is what we typically measure.
#+LABEL: fig:box
[[file:images/box.jpg]]

*** Fake Fish
In order to fully simulate the purpose for which the device was intended, we use
a dummy rainbow trout as a reference object. Each dummy is made from the same
model with the same dimensions, though different techniques have been used to
make them negatively buoyant. Three generations of dummy fish, named Fred,
George, and Ginny, have been used for these tests\footnote{If you would like to purchase a fish of your own, go to https://www.loftus.com/items/LF-0167.}.
Fred is shown in Figure \ref{fig:fred}.
#+ATTR_LATEX: :scale 0.05
#+ATTR_ORG: :width 200px
#+CAPTION: Fred, our first fake fish. He, and all the others, are 31cm long.
#+LABEL: fig:fred
[[file:images/fred.jpg]]

** Laser Calibration Algorithm
<<sec:math-testing>>

We again test the accuracy of the calibration by comparing reference length
measurements with different laser calibrations.
Figure\nbsp\ref{fig:laser-calibration-comparison} shows the results from obtained
from both calibrations -- as can be seen, they are virtually identical.

#+ATTR_LATEX: :scale 0.8
#+ATTR_ORG: :width 200px
#+CAPTION: Comparison of two calibration methods, graphing estimated distance against estimated length.
#+LABEL: fig:laser-calibration-comparison
[[file:images/laser-calibration-comparison.png]]

This led me to investigate whether the two methods may lead to equivalent
solutions. It is possible that the Atanasov method may happen to yield the laser
parameter solution that minimizes the function defined in the Hu method. My
current hypothesis is that these methods are provably identical, though work to
confirm this is still being done.

** Laser Calibration Procedure
<<sec:field-calibration-testing>>

We have been using the checkerboard method as the standard of calibration, as
this can be easily automated. However, we want to assess how effective slate
calibration is in theory, as it would be a huge asset to people using the device
in the field. Luckily for us, the divers at REEF were able to test the slate
calibration with a slate of their own construction. The methodology was as
follows: each of their seven camera units were calibrated using the same slate,
and the length of the slate, show in
Figure\nbsp\ref{fig:slate-calibration-measurement}, was measured using the newly
obtained laser parameters. Several measurements were taken per image to create a
box and whisker plot, shown in Figure\nbsp\ref{fig:slate-calibration}. We can see
that the measurements are fairly consistent, which demonstrates that the
calibration method is indeed viable.

#+ATTR_LATEX: :scale 0.8
#+ATTR_ORG: :width 200px
#+CAPTION: The length measured in the slate calibration experiment.
#+LABEL: fig:slate-calibration-measurement
[[file:images/slate-calibration-measurement.png]]


#+ATTR_LATEX: :scale 0.8
#+ATTR_ORG: :width 200px
#+CAPTION: Comparisons of slate measurements across different devices.
#+LABEL: fig:slate-calibration
[[file:images/slate-calibration.png]]

** Distance Measurements
<<sec:distance-test>>
To evaluate purely the distance estimates we can obtain from the system, we take
images of objects of known size (in this case, a checkerboard square), at known
distances, and comparing this known distance with the distance we get from the
algorithm described in Section\nbsp[[sec:distance-finding]]. These data were collected by taking
pictures at fixed increments -- in our case, we taped 20cm marks to a long
fishing rod, and took images at each mark. There were originally 19 marks along
the rod, though many of the pieces of tape came off underwater. Results are
shown in Figure\nbsp\ref{fig:length-reference}, which demonstrates that the actual
distance and the measured distance are well correlated.

#+ATTR_LATEX: :scale 0.8
#+ATTR_ORG: :width 200px
#+CAPTION: Length estimates compared to a length reference, measured with two different laser parameter sets.
#+LABEL: fig:length-reference
[[file:images/length-reference.png]]

Note that this is heavily dependent on the laser calibration being accurate --
the data in Figure\nbsp\ref{fig:length-reference} follow two different trends, each
of which comes from a different set of laser parameters. Both of these parameters
were obtained from images in the same test -- one calibration run was performed
before and after the length measurements. If the mount is sturdy, this will be
less likely to occur.

** Length Measurements
Figure\nbsp\ref{fig:measurements} demonstrates measurements of our fake fish, in
this case, Fred, and the box during a pool test. We can see that with varying
distances away from Fred, the estimate remains relatively consistent, and we
always underestimate the lengths of both Fred and our box.

#+ATTR_LATEX: :scale 0.8
#+ATTR_ORG: :width 200px
#+CAPTION: Measurements of Fred and the box, plotted over estimated distances away from the camera.
#+LABEL: fig:measurements
[[file:images/measurements.png]]

Based on Equation\nbsp\ref{eq:length-from-depth}, there are several factors that
cause error here, listed below.
*** Object Thickness
If the object is not flat, as is the case with most fish, then the laser dot may
be at a different distance from the camera than the head and tail points. This
causes closeup measurements of the object to be shorter than in images taken
further away -- this is due to the fact that the difference in distance between
the laser dot and the head/tail points makes up a smaller percentage of the
total distance as the camera becomes further away. This effect can be seen in
Figure\nbsp\ref{fig:measurements}, as Fred's measurements trend upward until around
1m away. It is likely that in field use, there will not be many measurements
taken that close to the fish anyway, as that would likely scare the fish away.
*** Object Tilt
If the object is tilted in the image toward or away from the camera, this causes
the length estimates to be lower than their true length. An experiment on this
is discussed in Section [[sec:tilt-experiment]].

*** Deviations in Laser Calibration
Section\nbsp[[sec:distance-test]] showed that an error in the laser calibration
parameters can cause deviation in measured distance. This creates a deviation in
measured length that increases the further away the object is from the camera.
In practice, care must be taken such that laser points are labeled accurately,
so that the risk of obtaining incorrect laser parameters is minimized.
Generally, however, this seems to fit within our 15% error margin.
*** Deviations in Camera Parameters
An incorrect calibration of the camera's internal parameters can also cause a
deviation in length. Specifically, if the focal length $f$ is off by a certain
percentage, then the measured length of an object will be off by the inverse of
that percentage. This effect can be seen in both
Figures\nbsp\ref{fig:laser-calibration-comparison} and \ref{fig:measurements}. So
far, we have not yet encountered a situation where the contributions from this
exceed our 15% error margin, but more thorough experiments in this area are
currently underway.

** Contributions of Tilt to Error
<<sec:tilt-experiment>>
To evaluate the system's accuracy with respect to different tilt angles, we had
the following experiment setup. We captured a total of 183 images of a
checkerboard pattern with the laser dot on it, with varying tilt angles and
depths. A subset of these images were used to obtain the laser position and
orientation. Using these data, from each image we extracted 13 lengths, as shown
in Figure\nbsp\ref{fig:checkerboard-experiment-setup}.
Figures\nbsp\ref{fig:checkerboard-errors-by-length} and
\ref{fig:checkerboard-errors-by-distance} shows the results of this experiment.

#+ATTR_LATEX: :scale 0.5
#+ATTR_ORG: :width 200px
#+CAPTION: Illustration of lengths measured on checkerboard. Each square pictured here is 41.3mm wide.
#+LABEL: fig:checkerboard-experiment-setup
[[file:images/checkerboard-experiment-setup.png]]

#+ATTR_LATEX: :scale 0.8
#+ATTR_ORG: :width 200px
#+CAPTION: Percent error of length measurements over all distances and angles, grouped by true object length.
#+LABEL: fig:checkerboard-errors-by-length
[[file:images/checkerboard_errors_by_length.png]]

#+ATTR_LATEX: :scale 0.8
#+ATTR_ORG: :width 200px
#+CAPTION: Percent error of length measurements over all distances and angles, plotted over the distance from the camera each image was taken.
#+LABEL: fig:checkerboard-errors-by-distance
[[file:images/checkerboard_errors_by_distance.png]]

Both figures show that the relative error in measurement decreases dramatically
when the angle becomes lower than 15 degrees, which supports previous studies
\cite{Heppell2012}. Since our system is diver operated, we expect the vast
majority of images taken will fit this criteria.

Figure\nbsp\ref{fig:variances} shows the variances of the percentage measurement
errors in all images, where each data point represents a single image. We see
that the variance in measurements is much higher when the tilt angle is higher
and the camera is close to the image. My current hypothesis for this is that
when the calibration board is closer to the camera, the distance covered by the
tilting checkerboard is a more significant percentage of the overall distance,
which causes the error of the distance measured to be larger.

#+ATTR_LATEX: :scale 0.8
#+ATTR_ORG: :width 200px
#+CAPTION: Variances of percentage length error, with each data point being a single image.
#+LABEL: fig:variances
[[file:images/checkerboard_error_variances.png]]

** Field Testing Observations
There are two main locations in which Fishsense Lite modules have been field
tested. The first was conducted off the La Jolla coast. Water conditions in La
Jolla are turbid, so this was an opportunity to test how our system would fare
in low visibility conditions. The main goal of this test was to evaluate the red
and green lasers, looking both at attenuation and how fish would react.

Secondly, in August 2023, we deployed six of our FishSense Lite units (FSL-01 to
FSL-06) to be tested by REEF staff in the Florida Keys. The water conditions in
this region were much clearer than in the La Jolla Kelp forests, so despite the
shorter attenuation of red lasers, the data from this test are predominantly
generated using red lasers. These deployments allowed us to look at how the
system would fare during more long term use, and in the hands of real
recreational divers.

I would like to acknowledge and thank Nathan Hui and Jack Elstner for
collecting data off the coast of La Jolla, and to REEF for helping us test the
many camera units we sent off to them in Florida.

*** Salt Water Measurements
<<sec:fw-vs-sw>>

Much like in our pool tests, the laser was first calibrated, then Fred was
measured and lengths were calculated in post-processing.

Length measurements obtained in salt water were generally higher than length
measurements obtained in pool water, as shown in
Figure\nbsp\ref{fig:freshwater-vs-saltwater}. Since the cameras are calibrated in
fresh water, the change in refractive index in salt water is likely what causes
the discrepancy in measurements. However, this is still within our accepted
error margin of 15%, as we defined earlier.

#+ATTR_LATEX: :scale 0.8
#+ATTR_ORG: :width 200px
#+CAPTION: A freshwater vs saltwater comparison of measured Fred lengths.
#+LABEL: fig:freshwater-vs-saltwater
[[file:images/freshwater-vs-saltwater.png]]

*** Laser Study and Comparison
<<sec:laser-comparison>>

The bulk of our experiments have been with two main laser pointer models: the
Innovative Scuba Concepts laser
pointer\footnote{https://dealer.innovativescuba.com/tc-101-aluminum-underwater-laser-pointer.html},
and the Shark
Laser\footnote{https://waterprooflaser.com/contents/en-us/d22\_TO-ORDER.html}.
Both lasers are rated for less than 5mW of power. The former model emits light
at 700nm, and the latter at 532nm. We discovered two significant factors that
influenced our decision to use one laser over the other.

The first was attenuation - within the visible spectrum, water attenuates light
with longer wavelengths significantly more than shorter wavelengths, and so red
light is more difficult to see at greater distances. From our testing, we found
that the red laser became incredibly difficult to spot at around 5m.
Figure\nbsp\ref{fig:laser-test} shows an example of a comparative range test with
the Shark laser and three different red lasers, of which the innovative Scuba
laser is second from the left.
#+ATTR_LATEX: :scale 0.3
#+ATTR_ORG: :width 200px
#+CAPTION: Lasers from left to right: Shark laser (green), Innovation Scuba, Orca, Gold
#+LABEL: fig:laser-test
[[file:images/lasers-close.JPG]]

The second was fish behavior - from tests with real fish, divers have observed
that fish tended to avoid the green laser. Our main hypothesis for why this
occurs is that fish are able to see green light far better than red. In
especially turbid water, added scattering meant that the beam was visible even
when the beam was not pointed directly at the fish's eyes. This meant that in La
Jolla, fish were especially skittish around the green laser. We realized this
because after the divers made a safety stop further off the bottom, when
pointing the green laser at the fish they seemed less afraid.

In particularly turbid conditions, particles in the water would cause the laser
beam to scatter, causing it to be visible within the image. An example of this
is shown in Figure\nbsp\ref{fig:sheephead}. There is potential for this to be used
to determine the laser's calibration parameters without the need for a planar
object, though work on this is yet to be done.

#+ATTR_LATEX: :scale 0.08
#+ATTR_ORG: :width 200px
#+CAPTION: A male California Sheephead from the La Jolla Kelp Beds. Note the ``lightsaber'' effect caused by particulates in the water.
#+LABEL: fig:sheephead
[[file:images/sheephead.JPG]]

*** Points of Mechanical Failure
<<sec:mechanical-failure>>
During field testing, we identified two main points where the device would tend to mechanically fail, listed below.
**** Laser Mount
<<sec:mount-failure>>

The laser mounts are made out of PLA, and therefore absorb water. This causes
the laser mounts to become brittle, and they have been shown to break after a
month or two of operation. This could be due to a combination of absorption of
water, degradation from exposure to ultraviolet light, or the large amount of
stress being put on certain parts by the screws. Current work is being done to
create these mounts out of a different material which does not absorb water and
resists corrosion.

Figure\nbsp\ref{fig:mount-crack} shows the most common stress point which causes the
mount to fail. This cracking is caused by the two screws used to keep the laser
secured to the mount.

#+ATTR_LATEX: :scale 0.5
#+ATTR_ORG: :width 200px
#+CAPTION: Image of one example of mount failure.
#+LABEL: fig:mount-crack
[[file:images/mount-failure.png]]

**** Laser Pointer
The laser pointers themselves have also been known to fail. There are two main
reasons for this:
1. The O-rings fail due to lack of proper maintenance or debris getting in the way, flooding the battery enclosure
2. A mechanical failure from the mechanism which activates the laser. Wear from turning them on so many times causes them to chip.

Discussions with Backscatter are in the works to create a custom solution that
does not need to be unscrewed, and also includes the possibility of having
multiple lasers. Multiple lasers also allow us to estimate the angle of the fish
relative to the camera, which further increases the accuracy of our estimations.

** Conclusion :ignore:
\\
While this system still has many issues with durability and reliability, we have
still shown that it is capable of obtaining fish lengths to within our desired
error margins.
* Conclusion
<<sec:conclusion>>
** Summary
In this work we have discussed the construction and inner workings of FishSense
Lite. We described the main assumption -- the pinhole camera assumption, which
allows us to draw simple relationships between apparent and actual size. We
established that distance to the fish is necessary to calculate it's actual
length. We discussed both procedures to obtain the laser parameters, and to
obtain the distance to the laser dot from the camera once those parameters are
known. We presented results that verify the accuracy of this system is within
15% of a fish's true length.

** Future Work
There is much work to be done to improve the system. We are working towards
removing some of the current requirements of the subject being imaged. In order
to remove the requirement that the fish be parallel to the image plane, we are
looking into either using multiple lasers, or relying on a machine learning
method to estimate the fish's orientation. We also need to refine some of the
automatic detection algorithms for both the fish and the calibration object, and
continue to build the infrastructure to support fully automatic data processing.

One area which we are especially interested in advancing is removing the
corrective optic, which will be possible once we are able to properly model the
distortions caused by the port of the underwater housing. This will reduce the
cost of our current device significantly, and will also be a significant step in
allowing us to use stereo cameras with Snell's law distortions.


#+LATEX: \appendix

* Appendix
** Pool Test List
<<sec:pool-test-list>>
Table\nbsp\ref{tab:comparison} shows the full list of pool tests conducted while testing FishSense Lite.
#+ATTR_LATEX:  :align |c|p{0.8\linewidth}|
#+CAPTION: List of pool tests conducted, with their purpose
#+LABEL: tab:comparison
|------------+----------------------------------------------------------------------------------------------------------------------------|
|       Date | Purpose                                                                                                                    |
|------------+----------------------------------------------------------------------------------------------------------------------------|
| 2023-02-24 | Lens calibration of first TG6                                                                                              |
| 2023-03-03 | Evaluate accuracy of OpenCV's calibration for different camera configurations (Olympus vs. Go Pro, housing vs. no housing) |
| 2023-03-10 | Test attenuation of red vs. green laser                                                                                    |
| 2023-03-20 | First attempt of laser calibration                                                                                         |
| 2023-04-05 | Repeat of both camera calibration and laser calibration                                                                    |
| 2023-04-12 | Quantify errors of length measurement from both object tilt and thickness                                                  |
| 2023-04-19 | Collect length-referenced data for ground-truth comparison                                                                 |
| 2023-05-03 | Evaluate deviation in laser calibration parameters after travel (Nathan's house)                                           |
| 2023-05-08 | Obtain laser parameters with new mount (Nathan's house)                                                                    |
| 2023-07-13 | Calibrate FSL-01 camera and laser  (Nathan's house)                                                                        |
| 2023-07-29 | Recalibrate and determine whether FSL-01 has maintained calibration after ocean test (Nathan's house)                      |
| 2023-08-07 | Camera calibrations for FSL-02, 03, 04, 05, 06 and 07 (Nathan's house)                                                     |
| 2023-08-14 | Laser calibrations for FSL-02, 03, 04, 05, 06 and 07 (Nathan's house)                                                      |
| 2023-08-18 | Recalibration of laser mounts for all camera systems due to faulty mechanical design. First testing of slate calibration   |
| 2023-10-28 | Flat port data                                                                                                             |
| 2023-11-10 | Trial for obtaining more fine-grain length data for different tilt angles                                                  |
| 2023-11-17 | Second trial for obtaining more fine-grain length data for different tilt angles                                           |
| 2024-02-12 | Data for different dive slates, i.e. do rougher edges and corners affect the quality of slate calibration?                 |
| 2024-02-26 | Laser calibration and length measurements with flat port                                                                   |
| 2024-03-04 | Data for testing automatic slate calibration                                                                               |
| 2024-04-25 | Determine accuracy of range measurements                                                                                   |
| 2024-05-02 | Better determine accuracy of range measurements with more data points                                                      |
| 2024-05-09 | Redo the range accuracy test from previous week, as well as attempt new camera calibration                                 |
|------------+----------------------------------------------------------------------------------------------------------------------------|

** FishSense Lite User Manual
<<sec:user-manual>>
*** Setup
1. Take the camera underwater and burp the lens. When reattaching the lens, ensure that it is tightly attached.  The lens needs to remain in the same position it originally calibrated.
2. Turn on the laser by turning the back end clockwise.
3. Take a validation photo using the method specified in Section

*** Use
1. When the laser is in use, aim it towards the center of the body mass of the fish. Ensure that the laser is not pointed directly toward a person.
2. In order to image fish, it is not necessary to look through the camera's viewport.  If the laser is visible on the fish, it should also be visible in the camera.
3. Refer to the appropriate camera manual for photo and video collection procedures. Take photos with fish as close to flat in the image as possible; otherwise, the length measurement may be inaccurate.

*** Teardown
1. When data collection is complete, turn off the laser.
2. Offload data to a hard disk if necessary.

*** Guidelines
1. Do not use the zoom function on the camera. This changes the focal length, rendering our calibration useless.
2. Avoid touching or moving the laser directly. Do not use the laser as a handle. The laser must be rigidly attached and unmoved for calibration values to hold true.
3. The laser must be pointed directly at the fish, as close to its center of body mass as possible.
4. The camera's optical axis should establish approximately a right angle with the vertical axis of the fish.
5. Ensure the fish is located within the effective range of 0.5 to 5 meters from the camera. With 3 meters being the ideal distance.
6. The fish must be illuminated.

*** Camera Settings
The following settings are the ones for which the camera was tested, and to keep the data consistent, we recommend that you use them as well. Unless specified, all these settings can be found by pressing the "OK" button.
- Program auto (set the wheel to P)
- Flash off
- ISO-A 1600
- Exposure bias 0
- Burst mode
- Underwater shallow white balance
- ESP metering
- Face priority off
- Accessory off
- JPEG mode underwater
- Autofocus
- Aspect ratio 4000 x 3000
- Output format LF + raw
- Still image stabilization on

** Light Attenuation In Water
The optical properties of water have long posed a challenge

Figure\nbsp\ref{} shows the level of light attenuation in water.

* Backmatter                                                         :ignore:
\bibliographystyle{plain}
\bibliography{fishsense}
* Testing :noexport:
** Locations
The data in this section were gathered from four locations, listed below.
*** Swimming Pools
The vast majority of our data comes from pool testing. These were relatively
controlled environment -- we were able to visit during times when lighting
conditions were best, and the lack of particles meant that we were not concerned
with turbidity. We mainly tested in two different pools: Canyonview Pool at
UCSD, and Nathan Hui's private pool. I would like to thank Nathan for allowing
us to use his pool, especially when situations were dire and we needed a venue
on short notice.
*** La Jolla Kelp Forests
In order to gain some insight into how this system would behave in real world
conditions, we tested this system on fish off the La Jolla coast. The author would
like to acknowledge and thank Nathan Hui and Jack Elstner for collecting these
data.

Water conditions in La Jolla are turbid, so this was an opportunity to test how
our system would fare in low visibility conditions. The main goal of this test
was to evaluate the red and green lasers, looking both at attenuation and how
fish would react.

Two devices, FSL-N and FSL-01, were used during this test. FSL-N was fitted with
the green laser, and FSL-01 with the red.

We used Fred as a reference object and took measurement photos from very close
(about 1m) to as far as visibility allowed. This was done with both FSL-N and
FSL-01. After this, divers conducted a roving survey, taking fish measurement
images in the same area. Divers did not take images of the same individuals.

*** Fred's Final Test :noexport:
This was Fred's final test. According to our recorded logs, we know the following:
#+BEGIN_QUOTE
``Anchor stuck, Fred was secured to anchor before ascent. Fred lost when anchor was retrieved.''
#+END_QUOTE
We thank Fred for his service.

*** REEF Florida Keys Deployment
In August 2023, we deployed six of our FishSense Lite units (FSL-01 to FSL-06)
to be tested by REEF staff in the Florida Keys. The water conditions in this
region were much clearer than in the La Jolla Kelp forests, so despite the
shorter attenuation of red lasers, the data from this test are predominantly
generated using red lasers.

Standard roving diver surveys were conducted in four different areas. On all
dives, a FishSense Lite camera unit, along with a stereo Go Pro pair, were used
to measure fish lengths. In this chapter we show comparitive tests between the
two systems.
** Results
*** Overall Accuracy
<<sec:accuracy>>

In order to fully simulate the purpose for which the device was intended, we use
a dummy rainbow trout as a reference object. Each dummy is made from the same
model, though different techniques have been used to make them negatively
buoyant. Three generations of dummy fish, named Fred, George, and Ginny, have
been used for these tests
\footnote{If you would like to purchase a Weasley fish of your own, go to https://www.loftus.com/items/LF-0167}.
Fred is shown in Figure \ref{fig:fred}.

#+ATTR_LATEX: :scale 0.05
#+ATTR_ORG: :width 200px
#+CAPTION: Fred, our first fake fish. He, and all the others, are 31cm long.
#+LABEL: fig:fred
[[file:images/fred.jpg]]


To measure the accuracy of this system, we rely on the accuracy of length
measurements obtained, since we are unable to rely on the true position and
orientation of the laser. In particular, we use relative length, as our sensor
is image-based, and errors would therefore occur in relation to the object's
apparent size, which is depth dependent. The key assumption which we rely on is
that the size of the object is irrelevant, as we only care about the apparent
size in the image.

In addition to Fred et al, another length reference we use is the flat surface
of an aluminum box, across which we mark a distance of 15cm.



**** Object Thickness :noexport:
Fred, and other fishes we measure, have some thickness. Therefore, if the laser
dot is in the middle of the fish, the laser dot point is at a different depth
than the snout and fork points. We know that Fred is approximately 8cm thick at
his thickest point, so we can adjust for this. Figure\nbsp\ref{} shows the adjusted
length measurements, and we can see that they line up almost perfectly.

**** Laser Detection Errors :noexport:
Small errors in the location of the detected laser point can contribute to
errors in the distance to fish, and therefore the length of the overall fish. We
have performed some modelling to determine exactly how large this error is, and
the results are shown in Figure\nbsp\ref{}.

*** Comparison with Stereo Video :noexport:
During the REEF deployment, the FishSense Lite camera modules were tested
simultaneously with a stereo Go Pro rig with a baseline of 1m. Figure\nbsp\ref{fig:smile-vs-stereo}
shows examples of fish length distributions from specific individual fish. To
test if the data collected with the two different systems could be considered to
be significantly different, we used a Kruskal-Wallis test. This was done using
R's built in =kruskal.test= function, which assumes a null hypothesis that the
distributions from the two groups are the same. From this, we get a p-value of
0.453, which indicates that given the two distributions are the same, the
current data is fairly likely.

#+ATTR_LATEX: :scale 0.5
#+ATTR_ORG: :width 200px
#+CAPTION: Bar chart with error bars showing estimated lengths of individual fish over a single day in the Florida Keys.
#+LABEL: fig:smile-vs-stereo
[[file:images/smile_080323_summary.tiff]]

The author would like to acknowledge and thank Jen Loch for analyzing the stereo
video data, and for providing the statistical background needed to write this
section.
*** Field Calibration Accuracy :noexport:
<<sec:field-calibration-testing>>

We compare our checkerboard laser calibration procedure to our automated field
laser calibration procedure. Figure\nbsp\ref{} demonstrates that the measurement
accuracy obtained between these two methods is similar, though the field
calibration procedure performs slightly worse. Our current hypothesis is that the
largest source of error mainly comes from having significantly fewer data points
with the dive slate as compared to the checkerboard, and that if as many images
were taken with the dive slate as with the checkerboard, the measurement error
would decrease.

*** Different Calibration Procedures
<<sec:math-testing>>

We again test the accuracy of the calibration by comparing reference length
measurements with different laser calibrations.
Figure\nbsp\ref{fig:laser-calibration-comparison} shows the results from obtained
from both calibrations -- as can be seen, they are virtually identical.

#+ATTR_LATEX: :scale 0.8
#+ATTR_ORG: :width 200px
#+CAPTION: Comparison of two calibration methods, graphing estimated distance against estimated length.
#+LABEL: fig:laser-calibration-comparison
[[file:images/laser-calibration-comparison.png]]

This led me to investigate whether the two methods may lead to equivalent
solutions. It is possible that the Atanasov method may happen to yield the laser
parameter solution that minimizes the function defined in the Hu method. My
current hypothesis is that these methods are provably identical, though work to
confirm this is still being done.

*** Effects of Freshwater Versus Saltwater

*** Different Lasers

*** Robustness to Tilt
** Conclusion :ignore:
\\
While this system still has many issues with durability and reliability, we have
still shown that it is capable of obtaining fish lengths to within our desired
error margins.
* Problem Motivation                                               :noexport:
Scrapping this, it's too wordy

I will begin describing the problem with some background on the work that
Engineers for Exploration (E4E for short) does, as I think it provides important
context for where the work came from.

** Who are E4E?
Engineers for Exploration are a research lab that specializes in developing
instrumentation for studying wildlife conservation and archaeology. Founded by
Professors Ryan Kastner and Curt Schurgers, along with Albert Lin, it combines
engineering with conservation efforts to create a completely unique research
area that is highly interdisciplinary. We primarily collaborate with scientists
with the goal of aiding them in their research, by designing systems that suit
their specific needs. Some examples of projects that we currently do include:
Acoustic Species ID, a collaboration with the San Diego Zoo with the goal of
studying bird populations in the Amazon Rainforests by classifying bird calls
from audio moths.

** Our Collaborators and Their Goals
FishSense's primary collaborators are the Semmens Lab in the Scripps Institute
of Oceanography, and the Reef Environmental Education Foundation (REEF). Much of
my current understanding of how fish lengths are recorded is based on work done
by Professor Brice Semmens\nbsp\cite{Heppell2012}.

** My Role
I began attending UCSD as an undergraduate in September of 2018, and had been
intermittently involved with several of E4E's projects. In September of 2022, I
was moved from the Radio Telemetry Tracking project onto FishSense to
investigate issues with a previous prototype. The device consisted of a
waterproof enclosure containing an Intel Realsense D455 and an NVIDIA Jetson
TX2, which was designed to process depth maps and convert these to fish lengths
of all fishes in a particular scene in real time. The issues with this device
were two-fold: the Realsense lacked sufficient resolution for species
identification, and distortions stemming from Snell's law prevented us from
obtaining accurate depth maps.

After several unsuccessful attempts at underwater calibration of the Realsense,
we decided to pivot to a much more rudimentary solution - rigidly attach a laser
beam to a normal underwater camera, have a diver take images of fish with this
laser beam on the fish, and perform post-processing to extract fish length. This
work will primarily be focused on this device. rigidly attach a laser beam to a
normal underwater camera, have a diver take images of fish with this laser beam
on the fish, and perform post-processing to extract fish length. This work will
primarily be focused on this device.

** Applications of the System
The problem that this system aims to solve is slightly different from the
original intent of the project - since our system is much more accessible than
our previous prototype, it can be easily deployed in the hands of recreational
divers, who can collect and upload data that will become publicly available for
worldwide usage. We henceforth refer to these recreational divers as "citizen
scientists". Citizen science efforts allow for data collection over a much wider
area than is possible with specialized research deployments.

Our desired accuracy goal with this system is only within 20% of the true fish
length - for this type of data, quantity is desired over quality, especially if
the error is consistent, as methods exist to compensate for those types of
errors. In addition, the current primary method of collecting these data is to
visually estimate the length of a fish, which humans are capable of doing to
within 20% of true length, but requires retraining as frequent as once every 6
months.
* Implementation Details :noexport:
** Preamble :ignore:
The following chapter contains material from a previously published work.
** System Description
The precise components that constitute the overall ``FishSense'' system have
changed drastically over the past few years. The contention comes from the fact
that there are two main components to the system. The first component is all the
hardware -- the physical camera, corrective optic, laser mount, and laser
pointer. The second is the software pipeline that is used to process the data.
Both are required to accomplish the goal of gathering fish length data.
*** Usage Guide
Prior to deploying this device in the field, calibration procedures must be
performed to maximize the accuracy of the obtained measurements.
**** Offline Procedures
***** Camera Calibration
The internal parameters of the camera must be calibrated in water. These
parameters include the camera's calibration matrix $K$, as well as lens
distortion coefficients $d$. This is currently achieved using Zhang's method
\nbsp\cite{Zhang2000}, which is to take many images of a checkerboard pattern, and
optimize the aforementioned parameters such that the checkerboard corners
reflect the structure of the checkerboard (lines remain straight).

This is performed in a swimming pool prior to any field deployments, and only
needs to be performed once in the device's lifetime.

***** Laser Calibration
The precise position and orientation of the laser beam must also be estimated.
Currently we do this by using a flat object with known features -- a dive slate
with duct tape on it, in our case. Multiple images of this flat object must be
taken with the laser dot on it in order to estimate the laser beam's parameters.

The procedure to obtain these parameters is described in [[sec:laser-calibration]].
**** Online Procedures
When in use, the diver points the laser pointer towards a fish and takes a
picture. The laser dot is generally in parallel with the camera, so it acts a
guide for the diver to ensure the image will meet our requirements, even in
cases where the viewfinder is too dim to see the image being taken.
**** Post Processing
After a dive or a series of dives, the diver can then upload the data to an
automated software pipeline that calculates the fish length. The mechanism of
transfer is still a work in progress -- currently it exists as a series of
Python scripts that must be run sequentially, though current work is being done
to both transfer it to a Rust codebase for speed, and construct a web
application in order for this software pipeline to become more easily
accessible.
** Hardware
*** Overview :ignore:
The device used to take the fish images must fit the following assumptions:
1. The system must have an imager that images can be extracted from.
2. The imager must be able to be modeled using the pinhole camera model.
3. The laser beam must be mounted rigidly to the imager.
4. The laser dot must be visible within the image for all desired ranges.

One example of our implementation of the FishSense Lite unit is shown in Figure\nbsp\ref{fig:fsl}.

We currently have seven FishSense Lite camera units, codenamed FSL-01 through
FSL-07. These are all comprised of exactly the same hardware components listed
below.
**** Annotations :noexport:
resolution requirements
rectified image assumption
currently experiements with backscatter demonstrate a decrease in error.
you may want to rerun the checkerboard multiple measurement experiment
*** Laser Mount
To secure the laser rigidly to the camera, we have designed a 3D-printed mount
made of polylactic acid. Figure\nbsp\ref{fig:laser-mount} shows a render of this
mount. Currently, the mount contains three screws: two to secure the laser, and
one to secure the base to the cold shoe on the camera's outer casing.

#+ATTR_LATEX: :scale 0.08
#+ATTR_ORG: :width 200px
#+CAPTION: A CAD render of an early version of the FishSense laser mount.
#+label: fig:laser-mount
[[file:images/laser_mount_render.png]]

These mounts are not water resistant, and break with prolonged use. More
information is included in Section\nbsp[[sec:mount-failure]]. Current work is being done
to design and manufacture a mount made out of other materials that are able to
better withstand saltwater and remain rigid.
**** Annotations :noexport:
Ask Adrian if it's ok to include his work here.
*** Laser Pointer
 Section\nbsp[[sec:laser-comparison]] discusses factors where one model may perform better than another.

** Laser Triangulation
*** Overview :ignore:
Knowing the laser's exact position and orientation is paramount to keeping
measured lengths as accurate as possible. However, due to manufacturing defects
or perturbation during transit, the position of the laser will shift by small
amounts both after the first initial measurement, as well as between dives.

Here we cover the forward problem - calculating laser dot locations using a
known laser position and orientation, and the inverse problem - calculating the
laser position and orientation based on known laser dot locations.
*** Quantity Definitions
We define the following quantities:
 - $\ell \in \mathbb{R}^3$: the location of the laser origin. To further constrain the problem ,we fix $\ell_z = 0$.
 - $\alpha \in \mathbb{R}^3$: a unit vector that determines the direction of the laser from the laser origin.
 - $p \in \mathbb{R}^3$: the 3D point at which the laser beam intersects with the fish.
 - $\mathfrak{p} \in \mathbb{R}^2$: the image coordinates of the laser in pixels, assuming the center of the image is $(0,0)$.
 - $f \in \mathbb{R}$: the focal length of the camera.
 - $w \in \mathbb{R}$: the distance between the center of two pixels on the camera sensor, otherwise known as pixel pitch.
 - $\mathfrak{p}_p \in \mathbb{R}^3$: the principal point. This is the true ``center'' of the imager found using camera calibration.
 - $o \in \mathbb{R}^3$: the origin, which we define using the principal point. Specifically, the origin will be located at $\mathfrak{p}_p + \begin{bmatrix}0 & 0 & f\end{bmatrix}^T$.

Figures\nbsp\ref{fig:diag} and \ref{fig:image_sensor} are used to illustrate these.

#+CAPTION: Close-up view of the image sensor. The distance between the plane and the origin is the focal length. $\mathfrak{p}$ is the image coordinate of the laser dot.
#+LABEL: fig:image_sensor
#+begin_figure
    \centering
    \begin{tikzpicture}
    [cube/.style={very thick,black},
			grid/.style={very thin,gray},
			axis/.style={->,black,thick}];

    \filldraw[
        draw=black,%
        fill=black!20,%
    ]          (1,0.75,5)
            -- (1,-0.75,5)
            -- (-1,-0.75,5)
            -- (-1,0.75,5)
            -- cycle;


    \draw[axis] (0,0,0)--(0.5,0,0) node[anchor=west]{$x$};
    \draw[axis] (0,0,0)--(0,-0.5,0) node[anchor=west]{$y$};
    \draw[axis] (0,0,0)--(0,0,-0.5) node[anchor=west]{$z$};

    \draw[black,thin] (0,0,0)-- node[above]{$f$} ++(0,0,5) ;
    \draw[black,dashed] (0,0,0) -- (0.25,0.1*5/6, 5) node[anchor=west]{$\mathfrak{p}$};

    \end{tikzpicture}
#+end_figure

*** Forward Problem (Point Triangulation)
Here we assume that laser beam origin $\alpha$ and laser beam orientation $\ell$ are
known. $\mathfrak{p}$ is also known from the captured image. Our goal is to use
this information to calculate $p$.

Firstly, we can calculate the vector in the camera's coordinate frame of the
location of the laser dot on the image sensor, which we call $p_s$:

\[
p_s = \begin{pmatrix}-\mathfrak{p}_xw \\ -\mathfrak{p}_yw \\ -f\end{pmatrix}
\]

We can then get the unit vector that points from the origin toward the laser
dot, which we call $v$:

\[
v = -\frac{p_s}{\lVert p_s \rVert}
\]

An observation that we leverage is that the laser beam, parameterized by $\ell +
\lambda_1\alpha$, and $\lambda_2v$, must intersect at $p$. This gives us the equation

\begin{equation}
\ell + \lambda_1\alpha = \lambda_2v
\end{equation}

We can restructure this into a matrix multiplication where $\lambda_1$ and $\lambda_2$ are
being transformed:

\begin{equation}
\begin{bmatrix}
\alpha_x & -v_x \\
\alpha_y & -v_y \\
\alpha_z & -v_z
\end{bmatrix}
\begin{bmatrix} \lambda_1 \\ \lambda_2 \end{bmatrix} =
\begin{bmatrix} -\ell_x \\ -\ell_y \\ 0 \end{bmatrix}
\end{equation}

This is an overconstrained problem for which, in general, no solution for $\lambda_1$
and $\lambda_2$ exists. We can, however, pose this as a least squares problem where we
minimize the norm of the difference between the left and right hand sides:

\begin{equation}
\text{argmin}_{\lambda_1,\lambda_2} \left\lVert\begin{bmatrix}
        \alpha_x & -v_x \\
        \alpha_y & -v_y \\
        \alpha_z & -v_z
    \end{bmatrix}\begin{bmatrix}
        \lambda_1 \\ \lambda_2
    \end{bmatrix} - \begin{bmatrix}
        -\ell_x \\ -\ell_y \\ 0
    \end{bmatrix}\right\rVert_2,
\end{equation}

Note that only one of the two scale parameters needs to be known in order for us
to get the laser point. We choose $\lambda_2$, for which the following closed-form
solution exists:

\begin{equation}
\lambda_2 = \frac{-\alpha^T \ell \alpha^T v + v^T \ell}{1 - (\alpha^T v)^2}
\end{equation}

We then trivially obtain $p$ from $p = \lambda_2v$.
*** Inverse Problem (Calibration)
Here we assume that $\alpha$ and $\ell$ are unknown, but
** Software
*** Overview :ignore:
In order to obtain length measurements from the captured images, we use a custom
software pipeline to extract the necessary features to measure length. Algorithm
\ref{alg:pipeline} shows a summary of the pipeline.

This algorithm is currently written in Python and run manually, though in future
we plan to move this to a Rust codebase.

\begin{algorithm}
\caption{Image processing pipeline.}
\label{alg:pipeline}
\begin{algorithmic}
\Require Image $I$, camera matrix $K$, laser parameters $\alpha$ and $\ell$
\State $I_R \gets \text{rectify}(I, K)$
\If{$\text{not laserInImage}(I_R)$} \\
    Reject image.
\EndIf
\State $_{}P_{\text{laser} }\gets \text{getLaserDot}(I_R)$
\If{$\text{not fishInImage}(I_R)$} \\
    Reject image.
\EndIf
\State $P_{\text{head}},P_{\text{tail}} \gets \text{getHeadTail}(I_R)$
\State $L \gets \text{getFishLength}(I_R, P_{\text{head}}, P_{\text{tail}}, P_{\text{laser}})$ \\
\Return $L$
\end{algorithmic}
\end{algorithm}
*** Image Rectification
All calculations relating to image projection rely on pinhole camera model
assumptions. In order to ensure that images fit this assumption, we rely on the
camera matrix $K$ obtained in the calibration process, as well as lens
distortion parameters. This is currently accomplished using OpenCV's standard
camera model, which contains a calibration camera matrix and a six-degree
polynomial to account for lens distortions.
*** Raw Processing
Figure\nbsp\ref{fig:jpg-vs-png} shows a raw image compared with a JPEG from the same
camera -- we can observe that the bottom of the pool in the raw image is slightly
curved whereas it is straight in the JPEG, implying that the camera does its own
distortion correction internally. To ensure that we have full control over the
entire image processing pipeline, we begin our operation on purely the raw
images. This also gives us control over color correction, which is advantageous
for us, as we can color correct the image in such a way that makes the laser
easier to detect.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\textwidth]{images/camera-jpg}
  \caption{Example JPEG image processed by the camera's internal software. }
  \label{fig:jpg-example}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\textwidth]{images/camera-png-unrectified}
  \caption{The same image }
  \label{fig:png-example}
\end{subfigure}
\caption{Comparison of the same image with onboard camera processing versus our custom processing.}
\label{fig:jpg-vs-png}
\end{figure}
**** Annotations :noexport:
Ask Hamish about whether it's ok to include his work in here and credit him
*** Laser Calibration

*** Automatic Laser Detection
<<sec:laser-detection>>
We chose to use a neural network to find the location of the laser, as over the
course of many pool tests and deployments, we gathered a large enough number of
hand-labeled laser dot images to be able to train such a model.

We chose a binary classification model which classified 20x20 tiles in a given
image as either containing the laser or not. Training data was generated on 326
images and testing data on 24 unique images. 225 tiles with lasers at varying
points within each tile were created per image, resulting in 73350 tiles. 439300
tiles without the laser were also created by extracting tiles from images in
steps of 85 pixels horizontally and vertically, while avoiding areas of said
images that contained a laser dot. Our network consists of a single
convolutional layer, a single average pooling layer, and two linear layers,
trained for 469 epochs. We are able to achieve 99\% accuracy on test data.

To reduce the number of potential tiles, we looked for all the local maximums
within an image, then select a 20x20 region around it. Due to the limited amount
of training images given to the model, many non-laser tiles were classified as
laser. However, we are able to constrain possible laser locations to a line in
the image using a mask derived from the laser parameters. The point closest to
the vanishing point was determined to be the laser.

Performance metrics of the model are shown in
Table\nbsp\ref{tab:laser-model-metrics}. Because most training data was taken from
pool images, the model trained on pool data performed very well. The one false
positive detected in the pool data was another point on the fish, so length
detection would work just fine. The ocean data had many false negatives, which
was deliberate - it is much better for our model to have high precision than it
is to have high recall because a falsely detected laser generates a wildly
incorrect fish length. On the other hand, because we are taking multiple images
of each fish, if a few of those images have lasers that are not detected, these
can simply be discarded.

#+ATTR_LATEX:  :align |c||c|c|c| :width 70%
#+CAPTION: Performance metrics for our laser detection model.
#+LABEL: tab:laser-model-metrics
|---------------+-------------+----------+-------|
| *Data Source* | *Precision* | *Recall* |  *F1* |
|---------------+-------------+----------+-------|
|---------------+-------------+----------+-------|
| Pool          |       0.996 |    0.981 | 0.989 |
|---------------+-------------+----------+-------|
| Ocean         |       0.986 |    0.861 | 0.919 |
|---------------+-------------+----------+-------|


**** Annotations :noexport:
Ask Viva for permission to include his work in here and make sure to credit him
*** Fish Segmentation
To locate the fish in the image, we use an existing technology called
Fishial\nbsp\cite{Fishial2023}, which uses a convolutional neural network to perform
segmentation of fishes within the frame. From this segmentation we get the
outline of the fish. We then employ the "line of best symmetry" to find the head
and tail points.

To find the line of symmetry of the fish, we employ Principal Component Analysis
(PCA) on the extracted contours. PCA is a dimensionality reduction technique
that works by identifying the directions in which the data exhibits the most
variance. This is particularly valuable for our purposes, as in most cases, the
axis of greatest variance corresponds closely to the symmetry axis of the fish,
which, in turn, aligns with the axis along which we intend to measure the fish
length. An illustration of this process is depicted in
Figure\nbsp\ref{fig:fishsymmetry}, showcasing the contours detected by Fishial on a
fish image.

\begin{figure}
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{images/fish_symmetryline}
        \caption{Fish with more typical body plan.}
        \label{fig:fishsymmetry}
    \end{subfigure}%
    \qquad
    \begin{subfigure}{.5\textwidth}
        \includegraphics[width=0.9\textwidth]{images/asymmetricalfish}
        \caption{Fish with large offset features (\textit{Mola mola}).}
        \label{fig:asymmetrical_fish}
    \end{subfigure}
    \caption{Example fish with detected lines of symmetry.}
\end{figure}

By applying PCA to these contours, we discern the principal axis that captures
the maximal variance. This axis, which aligns closely with the desired
measurement axis, provides a robust reference for accurately measuring the
fish's length. Following the identification of the symmetry axis, we intersect
this axis with the contours of the fish. and label the intersection of the two
points with two dots to show the head and tail location of the fish.
**** Annotations :noexport:
Ask Kyle Tran about whether it's ok to include his work in here and credit him

** Acknowledgements
The author would like to thank Christopher Crutchfield, Ana Perez, Hamish Grant,
Viva Suresh, and Kyle Tran for contributions described in this section.
